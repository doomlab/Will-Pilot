---
title             : "The title"
shorttitle        : "Title"

author: 
  - name          : "Kayla N. Jordan"
    affiliation   : "1"
    corresponding : yes    # Define only one corresponding author
    address       : "326 Market St., Harrisburg, PA, 17101"
    email         : "kjordan@harrisburgu.edu"
    role:         # Contributorship roles (e.g., CRediT, https://casrai.org/credit/)
      - Conceptualization
      - Writing - Original Draft Preparation
      - Writing - Review & Editing
      - Formal analysis
      - Methodology
      - Software
  - name          : "William E. Padfield"
    affiliation   : "2"
    role:
      - Writing - Original Draft Preparation
      - Data curation
      - Formal analysis
      - Methodology
      - Investigation
  - name          : "Erin M. Buchanan"
    affiliation   : "1"
    role:
      - Writing - Original Draft Preparation
      - Writing - Review & Editing 
      - Data curation
      - Formal analysis
      - Methodology
      - Software
    
affiliation:
  - id            : "1"
    institution   : "Harrisburg University of Science and Technology"
  - id            : "2"
    institution   : "Missouri State University"

authornote: |
  Kayla N. Jordan is an Assistant Professor of Social Analytics at Harrisburg University of Science and Technology, and Erin M. Buchanan is a Professor of Cognitive Analytics. William E. Padfield completed his Masters Degree at Missouri State University, and this study is a follow up analysis to his previous contribution. 

abstract: |
  One or two sentences providing a **basic introduction** to the field,  comprehensible to a scientist in any discipline.
  
  Two to three sentences of **more detailed background**, comprehensible  to scientists in related disciplines.
  
  One sentence clearly stating the **general problem** being addressed by  this particular study.
  
  One sentence summarizing the main result (with the words "**here we show**" or their equivalent).
  
  Two or three sentences explaining what the **main result** reveals in direct comparison to what was thought to be the case previously, or how the  main result adds to previous knowledge.
  
  One or two sentences to put the results into a more **general context**.
  
  Two or three sentences to provide a **broader perspective**, readily comprehensible to a scientist in any discipline.
  
  <!-- https://tinyurl.com/ybremelq -->
  
keywords          : "keywords"
wordcount         : "X"

bibliography      : ["r-references.bib"]

floatsintext      : no
figurelist        : no
tablelist         : no
footnotelist      : no
linenumbers       : yes
mask              : no
draft             : no

documentclass     : "apa6"
classoption       : "man"
output            : papaja::apa6_pdf
---

TO-DO LIST:
1. Introduction/Lit Review for new direction using LIWC instead of MFD (KJ)
2. Write Methods by combine data from Will's thesis and bootstrap method from LangofWar (EB)


```{r setup, include = FALSE}
#install.packages(c("devtools", "rio", "forestplot", "nlme", "MOTE", "tools"))
#devtools::install_github("crsh/papaja")
library(papaja) 
library(rio)
library(forestplot)
library(nlme)
library(MOTE)
library(tools)

nsim = 100

####make the table for study 1####
table1 = matrix(NA, ncol = 12, nrow = 23)
colnames(table1) = c("DV", "Mconservative", 
                     "SDconservative", "Mliberal", "SDliberal", 
                     "d", "dlow", "dhigh", "bcilowC", "bcihighC", 
                     "bcilowL", "bcihighL")
table1 = as.data.frame(table1)

####make the table for study 2: government shutdown####
table2 = matrix(NA, ncol = 12, nrow = 23)
colnames(table2) = c("DV", "Mconservative", 
                     "SDconservative", "Mliberal", "SDliberal", 
                     "d", "dlow", "dhigh", "bcilowC", "bcihighC", 
                     "bcilowL", "bcihighL")
table2 = as.data.frame(table2)

####make the table for study 2: Kavanaugh Hearings####
table3 = matrix(NA, ncol = 12, nrow = 23)
colnames(table3) = c("DV", "Mconservative", 
                     "SDconservative", "Mliberal", "SDliberal", 
                     "d", "dlow", "dhigh", "bcilowC", "bcihighC", 
                     "bcilowL", "bcihighL")
table3 = as.data.frame(table3)
```

```{r bootstrap-function, include = FALSE}
####bootstrap function #####
bootstrap_values <- function(formula, dataset, nsim){
  
  store_mean <- rep(NA, nsim)
  store_sd <- rep(NA, nsim)  
  attempts <- 0
  #loop until you have enough
  while(attempts < nsim){
    #create a dataset
    d <- dataset[sample(1:nrow(dataset), 
                        size = nrow(dataset),
                        replace = TRUE), ]
    #test the model
    tryCatch({
    
      model1 = lme(formula, 
             data = d, 
             method = "ML", 
             na.action = "na.omit",
             random = list(~1|Source),
             control=lmeControl(opt = "nlminb")) 
      meanvalue = summary(model1)$tTable[1]
      sdvalue = summary(model1)$tTable[2] * sqrt(nrow(d))
      attempts <- attempts + 1
      store_mean[attempts] <- meanvalue
      store_sd[attempts] <- sdvalue
      return(store_mean, store_sd, attempts)
    }, error = function(x){})
  }
  
  return(list("mean" = store_mean, "sd" = store_sd))
}

```

# Experiment 1

# Method

-----EB WILL NEED TO REWORK PART OF THIS AFTER I SEE THE INTRODUCTION-----

For Experiment 1, the researchers approached the study with the intention to answer a method question. That is, this portion of the current research was conducted in order to solidify the best method by which to analyze political news text under the Moral Foundations Theory framework while also alleviating some of the aforementioned valence problem observed in the Moral Foundations Dictionary. The researchers hypothesized the news sources generally perceived as liberal leaning (*NPR* and *The New York Times*) would contain MFD words and valences indicating endorsements of the individualizing moral foundations (*harm/care* and *fairness/reciprocity*). Additionally, the researchers hypothesized the two sources generally perceived to be conservative leaning (*Fox News* and *Breitbart*) would feature MFD words and valences indicating equal endorsement of all five foundations. Owing to the lack of a need for human participants, the researchers did not petition Missouri State University's Institutional Review Board, as no such approval was needed to conduct this study.

## Sources

Political articles were collected from the websites of four notable U.S. news sources, a process known as web scraping. The sources were *The New York Times*, *National Public Radio (NPR)*, *Fox News*, and *Breitbart*. They were selected for their widespread recognition and the fact political partisans have strong preferences for some sources over others. The researchers determined the political lean of each source by referencing @Mitchell2014's article demonstrating the self-reported ideological consistency represented by the consumers of several news sources. In general, *The New York Times* and *NPR* are preferred by consumers reporting a liberal bias or lean. In contrast, *Fox News* and *Breitbart* are believed to have a conservative bias or lean. @Mitchell2014's article presented political ideology as a scale ranging from "consistently liberal" to "consistently conservative." In between these extremes lie more moderate positions, including "mostly liberal," "mixed," and "mostly conservative." Owing to the lower number of sources analyzed herein, the researchers elected to categorize the sources as either "liberal" and "conservative" in order to form a basis for comparison.   

Political articles in particular were identified and subsequently scraped by including the specific URL directing to each source's political content in the *R* script. For example, rather than scrape from nytimes.com, which would return undesired results (non-political features, reviews, etc.), we instead included nytimes.com/section/politics so that more or less exclusively political content was obtained. All code for this manuscript can be found at https://osf.io/5kpj7/, and the scripts are provided inline with this manuscript written with the *papaja* library [@Aust2017]. 

Identification of the sources' political URLs presented a problem for two of the sources owing to complications with how their particular sites were structured. While in the multi-week process of scraping articles, we noticed word counts for *NPR* and *Fox News* were not growing at a similar pace as those from *The New York Times* and *Breitbart*. Upon investigation, we found another, more robust URL for political content from NPR: their politics content "archive." The page structure on NPR's website was such that only a limited selection of articles is displayed to the user at a given time. Scraping both the archive and the normal politics page ensured we were obtaining most (if not all) new articles as they were published. We later ran a process in order to exclude any duplicate articles. *Fox News* presented a similar issue. We discovered *Fox News* utilized six URLs in addition to the regular politics page. These URLs led to pages containing content pertaining the U.S. Executive Branch, Senate, House of Representatives, Judicial Branch, foreign policy, and elections. Once again, duplicates were subsequently eliminated from any analyses.  

## Materials

Using the *rvest* library in the statistical package *R*, we pulled body text for individual articles from each of the aforementioned sources (identified using CSS language) and compiled them into a dataset [@Wickham2016]. Using this dataset, we identified word count and average word count per source. This process was completed once daily starting in February 2018 until March 2018. Starting in mid-March 2018, the process was completed twice daily - once in the morning and again in the evening. Data collection was terminated once 250,000 words per source was collected in April 2018.  

```{r scraping, eval=FALSE, include=FALSE}

library(rvest)
####NY Times####
#Specifying the url for desired website to be scrapped
url <- 'https://www.nytimes.com/section/politics'

#Reading the HTML code from the website - headlines
webpage <- read_html(url)
headline_data = html_nodes(webpage,'.story-link a, .story-body a')

attr_data = html_attrs(headline_data) 
attr_data

urlslist = unlist(attr_data)
urlslist = urlslist[grep("http", urlslist)]
urlslist = unique(urlslist)
urlslist

##start a data frame
NYtimesDF = matrix(NA, nrow = length(urlslist), ncol = 3)
colnames(NYtimesDF) = c("Source", "Url", "Text")
NYtimesDF = as.data.frame(NYtimesDF)

##for loops
for (i in 1:length(urlslist)){
  
  ##read in the URL
  webpage <- read_html(urlslist[i])
  
  ##pull the specific nodes
  headline_data = html_nodes(webpage,'.story-content') 
  
  ##pull the text
  text_data = html_text(headline_data)
  
  ##save the data
  NYtimesDF$Source[i] = "NY Times"
  NYtimesDF$Url[i] = urlslist[i]
  NYtimesDF$Text[i] = paste(text_data, collapse = "")
    } ##end for loop

####NPR original front page####
url2 = 'https://www.npr.org/sections/politics/'
webpage2 = read_html(url2)
headline_data2 = html_nodes(webpage2,'.title a')

#URLs
attr_data2 = html_attrs(headline_data2) 
attr_data2

urlslist2 = unlist(attr_data2)
urlslist2 = urlslist2[grep("http", urlslist2)]
urlslist2

##start a data frame
NPRDF = matrix(NA, nrow = length(urlslist2), ncol = 3)
colnames(NPRDF) = c("Source", "Url", "Text")
NPRDF = as.data.frame(NPRDF)

##for loops
for (i in 1:length(urlslist2)){
  
  ##read in the URL
  webpage2 <- read_html(urlslist2[i])
  
  ##pull the specific nodes
  headline_data2 = html_nodes(webpage2,'#storytext > p') 
  
  ##pull the text
  text_data2 = html_text(headline_data2)
  
  ##save the data
  NPRDF$Source[i] = "NPR"
  NPRDF$Url[i] = urlslist2[i]
  NPRDF$Text[i] = paste(text_data2, collapse = "")
} ##end for loop

####Fox News####
url3 = 'http://www.foxnews.com/politics.html'
url3.1 = 'http://www.foxnews.com/category/politics/executive.html'
url3.2 = 'http://www.foxnews.com/category/politics/senate.htm.html'
url3.3 = 'http://www.foxnews.com/category/politics/house-representatives.html'
url3.4 = 'http://www.foxnews.com/category/politics/judiciary.html'
url3.5 = 'http://www.foxnews.com/category/politics/foreign-policy.html'
url3.6 = 'http://www.foxnews.com/category/politics/elections.html'
webpage3 = read_html(url3)
webpage3.1 = read_html(url3.1)
webpage3.2 = read_html(url3.2)
webpage3.3 = read_html(url3.3)
webpage3.4 = read_html(url3.4)
webpage3.5 = read_html(url3.5)
webpage3.6 = read_html(url3.6)

headline_data3 = html_nodes(webpage3, '.story- a , .article-list .title a')
headline_data3.1 = html_nodes(webpage3.1, '.story- a , .article-list .title a')
headline_data3.2 = html_nodes(webpage3.2, '.story- a , .article-list .title a')
headline_data3.3 = html_nodes(webpage3.3, '.story- a , .article-list .title a')
headline_data3.4 = html_nodes(webpage3.4, '.story- a , .article-list .title a')
headline_data3.5 = html_nodes(webpage3.5, '.story- a , .article-list .title a')
headline_data3.6 = html_nodes(webpage3.6, '.story- a , .article-list .title a')

#headline_data = html_text(headline_data)
head(headline_data3) 

attr_data3 = html_attrs(headline_data3) 
attr_data3.1 = html_attrs(headline_data3.1) 
attr_data3.2 = html_attrs(headline_data3.2) 
attr_data3.3 = html_attrs(headline_data3.3) 
attr_data3.4 = html_attrs(headline_data3.4) 
attr_data3.5 = html_attrs(headline_data3.5) 
attr_data3.6 = html_attrs(headline_data3.6) 

attr_data3

urlslist3 = c(unlist(attr_data3), unlist(attr_data3.1), 
              unlist(attr_data3.2), unlist(attr_data3.3), 
              unlist(attr_data3.4), unlist(attr_data3.5, attr_data3.6))
##find all that are href
urlslist3 = urlslist3[grep("http|.html", urlslist3)]
##fix the ones without the leading foxnews.com
urlslist3F = paste("http://www.foxnews.com", urlslist3[grep("^http", urlslist3, invert = T)], sep = "")
urlslist3N = urlslist3[grep("^http", urlslist3)]
urlslist3 = c(urlslist3N, urlslist3F)
urlslist3 = unique(urlslist3)
urlslist3

##start a data frame
FoxDF = matrix(NA, nrow = length(urlslist3), ncol = 3)
colnames(FoxDF) = c("Source", "Url", "Text")
FoxDF = as.data.frame(FoxDF)

##for loops
for (i in 1:length(urlslist3)){
  
  ##read in the URL
  webpage3 <- read_html(urlslist3[i])
  
  ##pull the specific nodes
  headline_data3 = html_nodes(webpage3,'p+ p , .fn-video+ p , .twitter+ p , .speakable') 
  
  ##pull the text
  text_data3 = html_text(headline_data3)
  
  ##save the data
  FoxDF$Source[i] = "Fox News"
  FoxDF$Url[i] = urlslist3[i]
  FoxDF$Text[i] = paste(text_data3, collapse = "")
} ##end for loop

####Breitbart####
url4 = 'http://www.breitbart.com/big-government/'
webpage4 = read_html(url4)
headline_data4 = html_nodes(webpage4, '.title a , #grid-block-0 span , #BBTrendUL a , #disqus-popularUL a , font')
#headline_data4 = html_text(headline_data4)
head(headline_data4) 

attr_data4 = html_attrs(headline_data4) 
attr_data4

urlslist4 = unlist(attr_data4)
##find all that are href
urlslist4 = urlslist4[grep("http|/big-government", urlslist4)]
##fix the ones without the leading bb
urlslist4F = paste("http://www.breitbart.com", urlslist4[grep("^http", urlslist4, invert = T)], sep = "")
urlslist4N = urlslist4[grep("^http", urlslist4)]
urlslist4 = c(urlslist4N, urlslist4F)
urlslist4 = unique(urlslist4)
urlslist4

##start a data frame
BreitbartDF = matrix(NA, nrow = length(urlslist4), ncol = 3)
colnames(BreitbartDF) = c("Source", "Url", "Text")
BreitbartDF = as.data.frame(BreitbartDF)

##for loops
for (i in 1:length(urlslist4)){
  
  ##read in the URL
  webpage4 <- read_html(urlslist4[i])
  
  ##pull the specific nodes
  headline_data4 = html_nodes(webpage4,'.entry-content p , h2') 
  
  ##pull the text
  text_data4 = html_text(headline_data4)
  
  ##save the data
  BreitbartDF$Source[i] = "Breitbart"
  BreitbartDF$Url[i] = urlslist4[i]
  BreitbartDF$Text[i] = paste(text_data4, collapse = "")
} ##end for loop

####NPR Archive####
url5 = 'https://www.npr.org/sections/politics/archive'
webpage5 = read_html(url5)
headline_data5 = html_nodes(webpage5,'.title a')
#headline_data = html_text(headline_data)
#head(headline_data)

#URLs
attr_data5 = html_attrs(headline_data5) 
attr_data5

urlslist5 = unlist(attr_data5)
urlslist5 = urlslist5[grep("http", urlslist5)]
urlslist5

##start a data frame
NPRArchiveDF = matrix(NA, nrow = length(urlslist5), ncol = 3)
colnames(NPRArchiveDF) = c("Source", "Url", "Text")
NPRArchiveDF = as.data.frame(NPRArchiveDF)

##for loops
for (i in 1:length(urlslist5)){
  
  ##read in the URL
  webpage5 <- read_html(urlslist5[i])
  
  ##pull the specific nodes
  headline_data5 = html_nodes(webpage5,'#storytext > p') 
  
  ##pull the text
  text_data5 = html_text(headline_data5)
  
  ##save the data
  NPRArchiveDF$Source[i] = "NPR"
  NPRArchiveDF$Url[i] = urlslist5[i]
  NPRArchiveDF$Text[i] = paste(text_data5, collapse = "")
} ##end for loop

####put together####

##import the overalldata 
overalldata = read.csv("../exp1_data/overalldata.csv")

##combine with new data (add the other DF names in here...)
newdata = rbind(overalldata, NYtimesDF, NPRDF, FoxDF, BreitbartDF, NPRArchiveDF)

##make the newdata unique in case of overlap across days
newdata = unique(newdata)

##write it back out
write.csv(newdata, "overalldata.csv", row.names = F)
```

## Data analysis

Once data collection ended, the text was scanned using the *ngram* package in *R* [@Schmidt2017]. This package includes a word count function, which was used to remove articles that came through as blank text, as well as to eliminate text picked up from the Disqus commenting system used by certain websites. At this point, duplicate articles were discarded.

```{r data-cleanup, eval=FALSE, include=FALSE}
##pull in the data
master = read.csv("../exp1_data/overalldata.csv", stringsAsFactors = F)

##get rid of the blank stuff
library(ngram)

for (i in 1:nrow(master)) {
  master$wordcount[i] = wordcount(master$Text[i])
}

nozero = subset(master, wordcount > 0)
tapply(nozero$wordcount, nozero$Source, mean)
tapply(nozero$wordcount, nozero$Source, sum)

##take out the disqus ones
nodis = nozero[ -c(grep("disqus", nozero$Url)), ]

##figure out the duplicates
nodis$URLS = duplicated(nodis$Url)

##all data
final = subset(nodis, URLS == FALSE)

tapply(final$wordcount, final$Source, mean)
tapply(final$wordcount, final$Source, sum)

write.csv(final, "finaldata.csv")
```

```{r kayla-cleanup, eval = F, echo = F}
#Code Partisan Lean for Exp1
dfO = read.csv("finaldata.csv", stringsAsFactors = FALSE)
df = read.csv("finaldata_LIWC.csv", stringsAsFactors = FALSE)
colnames(df)[1:6] = colnames(dfO)[1:6]
df$PartisanLean = ifelse(df$Source=="Breitbart"|df$Source=="Fox News", "Conservative", "Liberal")
write.csv(df, 'finaldata_LIWC.csv', row.names = FALSE)
```

----KJ ENTER A PARAGRAPH ABOUT THE LIWC HERE?-----

----NOTE this was rewritten since we published it before, Will's thesis sections were not because it's only a preprint----

This dataset includes multiple instances of each news source across time, thus, repeated measures data within the source. Therefore, we analyzed this data with multilevel modeling to control for the correlated error due to source category. The *nlme* package was used to calculate the mean and standard deviation of each variable by partisan lean of the source [@Pinheiro2017]. The source was included as a random intercept factor [@Gelman2006] using the intercept to predict the dependent variable category from the LIWC. The standard error of each estimate was multiplied by the square root of sample size (*n*) to create the standard deviation. We used bootstrapping to complete this analysis 1000 times, and the normal confidence interval from the *boot* library was calculated [CITE BOOT HERE] separately for each partisan lean. The bootstrapped mean and standard deviation are shown in FIGURE ??, and $d_s$ was calculated using the *MOTE* library using pooled standard deviation as the denominator [@Lakens2013; @Buchanan2017]. $d_s$ represents the standardized mean difference between Conservative and Liberal leaning sources. We examined the bootstrapped confidence interval of $d_s$, rather than *p*-values, for determining differences between groups. Confidence intervals that included zero, suggested no support for differences in the LIWC category between partisan lean, while confidence intervals that did not include zero indicate support for differences in the dependent variable.  

```{r bootstrap-study-1, include = F}
# Import the data
exp1 <- import("finaldata_LIWC.csv")

# Eliminate Missing Data
exp1 <- na.omit(exp1)

# Create group labels
exp1$PartisanLean = ifelse(exp1$PartisanLean=='Conservative', 0, 1)

# Create IVs and DVs
groups = c(0, 1)
DVs = c('Analytic','Clout','Authentic','Tone','WPS','Sixltr','anger','social','family','female','male','cogproc','affiliation','achieve','power','reward','risk','focuspast','focuspresent','focusfuture','money','relig','death')

# Loop over the DVs and create values 
# This saves the difference between groups
r = 1
for(DV in DVs){
  for(group in groups){
    data = subset(exp1, PartisanLean==group) #data name
    f = as.formula(paste(DV,'~1', sep=''))
    bs = bootstrap_values(f,data,nsim)
    table1[r, 1] = DV
    if(group==0){
      table1[r, 2] = mean(bs$mean) #table name all the way down 
      table1[r, 3] = mean(bs$sd)
      ncon = length(na.omit(data[[DV]]))
      table1[r, 9] = quantile(bs$mean, 0.025)
      table1[r, 10] = quantile(bs$mean, 0.975)
    }
    if(group==1){
      table1[r, 4] = mean(bs$mean)
      table1[r, 5] = mean(bs$sd)
      nlib = length(na.omit(data[[DV]]))
      table1[r, 11] = quantile(bs$mean, 0.025)
      table1[r, 12] = quantile(bs$mean, 0.975)
    }
  }
  mdiff = d.ind.t(m1 = table1[r,2], m2 = table1[r,4], 
                sd1 = table1[r,3], sd2 = table1[r,5],
                n1 = ncon, 
                n2 = nlib, 
                a = .05)
  table1[r, 6] = mdiff$d
  table1[r, 7] = mdiff$dlow
  table1[r, 8] = mdiff$dhigh
  r = r + 1
}
```

Doing the bootstrapping thing from the language of war paper
- analytic, cognitive processing, authenticity, tone, clout

- put notes here, I fixed the code 

# Experiment 2

-----QUESTION HERE IF WE WANT THIS HERE OR AT THE FRONT, WILL NEED TO TEXT UPDATE----

## Kavanaugh Supreme Court Hearing

In the wake of Justice Anthony Kennedy's retirement from the Supreme Court of the United States, President Donald Trump nominated Brett Kavanaugh as the new Associate Justice. Kavanaugh was previously on the U.S. Court of Appeals for the District of Columbia. The Senate Judiciary Committee began his confirmation hearing on September 4, 2018 [@USGovernment2018b]. Following allegations of sexual assault by high school classmate Dr. Christine Blasey Ford, the committee postponed its vote on whether or not to open the confirmation to the entire Senate. 

On September 27, the committee questioned Dr. Ford before commencing a second round of questioning for Judge Kavanaugh [@USGovernment2018]. During the intervening weeks between hearings, two more women came forward with two separate allegations of sexual assault on the part of Kavanaugh. According to Nielsen reports, more than 20 million people watched the September 27 proceedings on television [@OConnell2018]. This figure does not take into account viewers who watched online, nor does it account for viewers outside the United States. On September 28, the Senate Judiciary Committee voted to send the nomination to the Senate floor. Senator Jeff Flake of Arizona, however, lobbied for a week-long FBI investigation on Kavanaugh and the allegations facing him, which the committee, and later the President, approved. The investigation concluded with no significant findings. The Senate voted 50-48 to approve Kavanaugh's appointment on October 6, 2018 [@USGovernment2018a].

The Kavanaugh nomination, confirmation hearing, and eventual swearing-in, as well as the news media's coverage of all three events, feature many dimensions that likely differ depending on one's morals. The issue might be exacerbated given the presence of questionable sexual behaviors at the center of many concerns. On one side of the debate, Kavanaugh's Supreme Court tenure presents a prime opportunity to bring morality back into interpretation of the Constitution. Kavanaugh's confirmation creates a conservative stronghold among the justices on the court. Commentators have noted this might help advance a judicial agenda that backpedals certain rights previously upheld by the Supreme Court, including abortion and gay marriage - social issues challenged by their opponents at least partially on moral grounds. Concerns around abortion might be related either to *harm/care* or *purity/sanctity*. On the other side of the debate, the assault allegations have energized Kavanaugh's opponents to advocate for his rejection from the court owing to misdeeds resulting from Kavanaugh's own alleged lack of morals. Likewise, arguments could be made that relate concerns regarding sexual violence to *harm/care* or *fairness/reciprocity*. Additionally, the moral duty of the Senate as the upper chamber in the U.S. legislature has been scrutinized in public discourse with respect to its handling of the assault allegations vis-a-vis Kavanaugh's confirmation. 

## U.S. Government Shutdown of 2018-2019

The U.S. Federal Government partially shut down on December 22, 2018. The government reopened on January 25, 2019 upon the passage of an appropriations bill by both houses of Congress [@Axelrod2019]. The shutdown stemmed from a disagreement between President Donald Trump and Congress over funding for the president's proposed U.S.-Mexico border wall. President Trump demanded $5.7 billion dollars in the new budget to be appropriated for the wall, which Congress did not provide in its budget [@Peoples2019]. Owing to the lack of funding, the government began a partial shutdown, causing "around 800,000" federal employees to be either furloughed or compelled to work without pay [@Axelrod2019].

The government shutdown ended 35 days later when Congress passed a continuing resolution to fund the government for three weeks while further negotiations regarding the budget would take place. President Trump signed the bill, ending the longest government shutdown in U.S. history [@Axelrod2019]. Many political commentators cite the mounting pressure on Trump and Congress stemming from significant costs both to unpaid American workers and the economy as a whole [@Kheel2019]. Perhaps owing to the financial pressures experienced by thousands of federal employees and their families as a result of a political quarrel, President Trump saw his approval rating fall as the shutdown progressed [@Peoples2019]. While the shutdown and its associated burdens were unpopular, the issues discussed concurrent to the shutdown offer the potential for more divergent opinions.

As mentioned before, the shutdown started as a result of an impasse regarding funding for the border wall. Since the wall was proposed during the 2016 presidential campaign, the issue has taken on a moral dimension. As an example, Shaun Casey, the director of Georgetown University's Berkely Center for Religion, Peace and World Affairs, cites diverging opinions stemming from theological concerns [@Martin2019]. Some supporters, citing certain passages from the Bible, believe construction of the wall is part of a divinely ordained process handed down by God. Opponents, on the other hand, have cited other parts of the Bible extolling the significance of exiled peoples, especially the Hebrew people [@Martin2019]. This example potentially ties into the five moral foundations. Theological concerns regarding the border wall might invoke *authority/respect* as well as *purity/sanctity*. In addition to these two foundations, the involvement of foreign countries and their citizens' migration into the U.S. could also elicit moral concerns related to *ingroup/outgroup*.

Owing to the wealth of moral opinions regarding the border wall and its association with the 2018-2019 government shutdown, the researchers decided to add the government shutdown as a news event to analyze under the auspices of Moral Foundations Theory. In addition to articles related to the Kavanaugh confirmation hearing, the researchers scraped articles related to the government shutdown in order to analyze their content for valence and moral alignment.

# Method

In contrast to Experiment 1, the researchers approached Experiment 2 with the intention to confirm the method employed was valid for the analysis of the scraped text as well as for any inferences drawn from the analyses. For Experiment 2, the researchers hypothesized that news sources perceived as liberal will exhibit positive endorsements of the individualizing moral foundations (*harm/care* and *fairness/reciprocity*) in their articles reporting on both the Kavanaugh confirmation hearing as well as the 2018-2019 government shutdown. News sources perceived as conservative are hypothesized to positively endorse all five foundations equally in their coverage of the Kavanaugh hearing and the government shutdown. The researchers tested the hypothesis by analyzing the content scraped from news sources' web pages spanning the two weeks before (September 13, 2018) and two weeks after (October 11, 2018) Kavanaugh's confirmation hearing, owing to its prominence in the news. Likewise, the researchers analyzed content spanning two weeks before the start of the government shutdown (December 8, 2018) to two weeks following the end of the shutdown (February 8, 2019). The content will be analyzed for valence and moral alignment under Moral Foundations Theory. Once again, no human participants were needed for this study, so no Institutional Review Board approval was necessary.

```{r NYTimes_kav, eval = F, include = F}

library(rvest)
library(RSelenium)

rD <- rsDriver(browser=c("chrome"), chromever="73.0.3683.68")
remDr <- rD[["client"]]

#page for NYTIMES
url <- 'https://www.nytimes.com/search?endDate=20181011&query=kavanaugh&sort=best&startDate=20180913'

#navigate to it
remDr$navigate(url)

#click the button 
#don't change class name
#right click --> inspect element 
webElem <- remDr$findElement(using = 'class name',"css-1t62hi8")

#with 752 results at 10 per page we need
#752 - 10 original = 742 / 10 per click = 75 clicks

for (i in 1:75) { #change this to 75 later
  webElem$clickElement()
  Sys.sleep(runif(1, 1, 5))
}

webpage <-remDr$getPageSource()

#Reading the HTML code from the website - headlines
webpage2 <- read_html(unlist(webpage))

headline_data = html_nodes(webpage2,'a')

attr_data = html_attrs(headline_data) 
attr_data

urlslist = unlist(attr_data)
urlslist = urlslist[grep(".html", urlslist)]
urlslist = urlslist[grep("/2018", urlslist)]
urlslist = unique(urlslist)
urlslist

#paste NY on front
urlslist = paste("https://www.nytimes.com", urlslist, sep = "")

##start a data frame
NYtimesDF = matrix(NA, nrow = length(urlslist), ncol = 3)
colnames(NYtimesDF) = c("Source", "Url", "Text")
NYtimesDF = as.data.frame(NYtimesDF)

##for loops
for (i in 1:length(urlslist)){
  
  ##read in the URL
  webpage <- read_html(urlslist[i])
  
  ##pull the specific nodes
  headline_data = html_nodes(webpage,'.StoryBodyCompanionColumn') 
  
  ##pull the text
  text_data = html_text(headline_data)
  
  ##save the data
  NYtimesDF$Source[i] = "NY Times"
  NYtimesDF$Url[i] = urlslist[i]
  NYtimesDF$Text[i] = paste(text_data, collapse = "")
  Sys.sleep(runif(1, 1, 10)) #don't make them angry
} ##end for loop

write.csv(NYtimesDF, "NYtimes_kav.csv", row.names = F)

remDr$close()
# stop the selenium server
rD[["server"]]$stop()

```

```{r NYTimes_shut, eval = F, include = F}
library(rvest)
library(RSelenium)
library(beepr)

rD <- rsDriver(browser=c("chrome"), chromever="73.0.3683.68")
remDr <- rD[["client"]]

#page for NYTIMES
url <- 'https://www.nytimes.com/search?endDate=20190208&query=government%20shutdown&sort=best&startDate=20181208'

#navigate to it
remDr$navigate(url)

#click the button 
#don't change class name
#right click --> inspect element 
webElem <- remDr$findElement(using = 'class name',"css-1t62hi8")

#with 803 results at 10 per page we need
#803 - 10 original / 10 per click is 80 clicks

for (i in 1:80) {
  webElem$clickElement()
  Sys.sleep(runif(1, 1, 5))
}

webpage <-remDr$getPageSource()

#Reading the HTML code from the website - headlines
webpage2 <- read_html(unlist(webpage))

headline_data = html_nodes(webpage2,'a')

attr_data = html_attrs(headline_data) 
#attr_data

urlslist = unlist(attr_data)
urlslist = urlslist[grep(".html", urlslist)]
urlslist = urlslist[grep("/201", urlslist)]
urlslist = unique(urlslist)
urlslist

#paste NY on front
urlslist = paste("https://www.nytimes.com", urlslist, sep = "")

##start a data frame
NYtimesDF = matrix(NA, nrow = length(urlslist), ncol = 3)
colnames(NYtimesDF) = c("Source", "Url", "Text")
NYtimesDF = as.data.frame(NYtimesDF)

##for loops
for (i in 1:length(urlslist)){
  
  ##read in the URL
  webpage <- read_html(urlslist[i])
  
  ##pull the specific nodes
  headline_data = html_nodes(webpage,'.StoryBodyCompanionColumn') 
  
  ##pull the text
  text_data = html_text(headline_data)
  
  ##save the data
  NYtimesDF$Source[i] = "NY Times"
  NYtimesDF$Url[i] = urlslist[i]
  NYtimesDF$Text[i] = paste(text_data, collapse = "")
  Sys.sleep(runif(1, 1, 10)) #don't make them angry
} ##end for loop

write.csv(NYtimesDF, "NYtimes_gs.csv", row.names = F)
beep(sound = 5)

remDr$close()
# stop the selenium server
rD[["server"]]$stop()
```

````{r NPR_kav, eval = F, include = F}
library(rvest)
library(RSelenium)
library(beepr)

#open the browser
rD <- rsDriver(browser=c("chrome"), chromever="73.0.3683.68")
remDr <- rD[["client"]]

#create a blank space to put the links
urlslist_final = list()

##loop through the page number at the end until done with ~1000 / 20 = 50
for (i in 1:50) { ##change this to 50
  
  url = paste0('https://www.npr.org/search?query=kavanaugh&page=', i)
  
  #navigate to it
  remDr$navigate(url)
  
  #scroll on the page
  webscroll <- remDr$findElement("css", "body")
  webscroll$sendKeysToElement(list(key = "end"))
  
  #get the links
  webElems <- remDr$findElements(using = "css", "[href]")
  urlslist_final[[i]] = unlist(sapply(webElems, function(x) {x$getElementAttribute("href")}))
  
  #don't go too fast
  Sys.sleep(runif(1, 1, 5))
  
} #close the loop

remDr$close()
# stop the selenium server
rD[["server"]]$stop()

beep(sound = 3)

urlslist = unlist(urlslist_final)
#find all the ones with /2018/
urlslist2 = urlslist[grep("/2018", urlslist)]
urlslist2 = urlslist2[grep("npr.org", urlslist2)] #get rid of wbur
urlslist2 = unique(urlslist2)
temp = strsplit(urlslist2, split = "/")
temp2 = plyr::ldply(temp, rbind)
temp2 = as.data.frame(apply(temp2, 2, as.character), stringsAsFactors = F)

#deal with the ones with extra parts
temp2[temp2$`4` == "sections" , 4:8] = temp2[temp2$`4` == "sections" , 6:10]

#pull only the right dates
rows = rownames(temp2[ temp2$`4` == 2018 & #must be 2018 and 
                         (temp2$`5` == "09" & as.numeric(temp2$`6`)>=13 | 
                            temp2$`5` == "10" & as.numeric(temp2$`6`)<=11), #Must be 9 or 10 
                ])

  
urlslist3 = urlslist2[as.numeric(rows)]

##start a data frame
NPRDF = matrix(NA, nrow = length(urlslist3), ncol = 3)
colnames(NPRDF) = c("Source", "Url", "Text")
NPRDF = as.data.frame(NPRDF)


##for loops
for (i in 1:length(urlslist3)){
  
  ##read in the URL
  webpage2 <- read_html(urlslist3[i])
  
  ##pull the specific nodes
  headline_data2 = html_nodes(webpage2,'#storytext > p') 
  
  ##pull the text
  text_data2 = html_text(headline_data2)
  
  ##save the data
  NPRDF$Source[i] = "NPR"
  NPRDF$Url[i] = urlslist3[i]
  NPRDF$Text[i] = paste(text_data2, collapse = "")
  Sys.sleep(runif(1, 1, 10))
} ##end for loop

beep(sound = 3)

write.csv(NPRDF, "NPR_kav.csv", row.names = F)

```

```{r NPR_shut, eval = F, include = F}

library(rvest)
library(RSelenium)
library(beepr)

#open the browser
rD <- rsDriver(browser=c("chrome"), chromever="73.0.3683.68")
remDr <- rD[["client"]]

#create a blank space to put the links
urlslist_final = list()

##loop through the page number at the end until done with ~3316 / 20 = 166
for (i in 1:166) { 
  
  url = paste0('https://www.npr.org/search?query=government%20shutodwn&page=', i)
  
  #navigate to it
  remDr$navigate(url)
  
  #scroll on the page
  webscroll <- remDr$findElement("css", "body")
  webscroll$sendKeysToElement(list(key = "end"))
  
  #get the links
  webElems <- remDr$findElements(using = "css", "[href]")
  urlslist_final[[i]] = unlist(sapply(webElems, function(x) {x$getElementAttribute("href")}))
  
  #don't go too fast
  Sys.sleep(runif(1, 1, 5))
  
} #close the loop

remDr$close()
# stop the selenium server
rD[["server"]]$stop()

beep(sound = 3)

urlslist = unlist(urlslist_final)
#find all the ones with /2018/
urlslist2 = urlslist[grep("/201", urlslist)]
urlslist2 = urlslist2[grep("npr.org", urlslist2)] #get rid of wbur
urlslist2 = unique(urlslist2)
temp = strsplit(urlslist2, split = "/")
temp2 = plyr::ldply(temp, rbind)
temp2 = as.data.frame(apply(temp2, 2, as.character), stringsAsFactors = F)

#deal with the ones with extra parts
temp2[temp2$`4` == "sections" , 4:8] = temp2[temp2$`4` == "sections" , 6:10]

#pull only the right dates
rows = rownames(temp2[ (temp2$`4` == 2018 | temp2$`4` == 2019) & #must be 2018/19 and  
                         (
                           (temp2$`5` == "12" & as.numeric(temp2$`6`)>8 & temp2$`4` == 2018) | #after dec 12
                            (temp2$`5`== "01" & temp2$`4` == 2019) | #all of jan
                            (temp2$`5` == "02" & as.numeric(temp2$`6`)<8 & temp2$`4` == 2019)
                           ), #before feb 8
                ])

  
urlslist3 = urlslist2[as.numeric(rows)]

##start a data frame
NPRDF = matrix(NA, nrow = length(urlslist3), ncol = 3)
colnames(NPRDF) = c("Source", "Url", "Text")
NPRDF = as.data.frame(NPRDF)


##for loops
for (i in 315:length(urlslist3)){
  
  ##read in the URL
  webpage2 <- read_html(urlslist3[i])
  
  ##pull the specific nodes
  headline_data2 = html_nodes(webpage2,'#storytext > p') 
  
  ##pull the text
  text_data2 = html_text(headline_data2)
  
  ##save the data
  NPRDF$Source[i] = "NPR"
  NPRDF$Url[i] = urlslist3[i]
  NPRDF$Text[i] = paste(text_data2, collapse = "")
  Sys.sleep(runif(1, 1, 10))
} ##end for loop

beep(sound = 3)

write.csv(NPRDF, "NPR_gs.csv", row.names = F)
```

````{r Slate_kav, eval = F, include = F}
library(rvest)
library(RSelenium)
library(beepr)

#open the browser
rD <- rsDriver(browser=c("chrome"), chromever="73.0.3683.68")
remDr <- rD[["client"]]

url = "https://slate.com/search?q=kavanaugh&via=homepage_nav_search"

remDr$navigate(url)

#create a blank space to put the links
urlslist_final = list()

#loop over the results and go through them
#appear to be 10 pages
for (i in 1:10){
  
  #find the button to click on 
  webClick <- remDr$findElements(using = "css", ".gsc-cursor-page")
  
  #don't go too fast for the page to load
  Sys.sleep(runif(1, 1, 5))
  
  #click on the page number
  webClick[[i]]$clickElement()
  
  #page reload
  Sys.sleep(runif(1, 1, 5))
  
  #find the urls for page X
  webElems <- remDr$findElements(using = "css", "[href]")
  
  #add the urls to a list
  urlslist_final[[i]] = unlist(sapply(webElems, function(x) {x$getElementAttribute("href")}))
  
  #don't go too fast
  Sys.sleep(runif(1, 1, 5))
}

remDr$close()
# stop the selenium server
rD[["server"]]$stop()

beep(sound = 3)

##deal with the URLS
urlslist = unlist(urlslist_final)

#find all the ones with /2018/
urlslist2 = urlslist[grep("/2018/10|/2018/09", urlslist)]
urlslist2 = unique(urlslist2)

#71 articles seems suspiciously low, so also scraped this page
#https://slate.com/tag/brett-kavanaugh/

#open the browser
rD <- rsDriver(browser=c("chrome"), chromever="73.0.3683.68")
remDr <- rD[["client"]]

#create a blank space to put the links
urlslist_final = list()

#loop over the results and go through them
#337 / 20 = 17 pages
for (i in 1:17){
  
  url = paste0("https://slate.com/tag/brett-kavanaugh/", i)
  
  remDr$navigate(url)
  
  #don't go too fast
  Sys.sleep(runif(1, 1, 5))
  
  #find the urls for page X
  webElems <- remDr$findElements(using = "css", "[href]")
  
  #add the urls to a list
  urlslist_final[[i]] = unlist(sapply(webElems, function(x) {x$getElementAttribute("href")}))
  
  #don't go too fast
  Sys.sleep(runif(1, 1, 5))
}

remDr$close()
# stop the selenium server
rD[["server"]]$stop()

beep(sound = 3)

##deal with the URLS
urlslist = unlist(urlslist_final)

#find all the ones with /2018/
urlslist2.1 = urlslist[grep("/2018/10|/2018/09", urlslist)]
urlslist2.1 = unique(urlslist2.1)

urlslist3 = unique(c(urlslist2, urlslist2.1))

##start a data frame
SLATEDF = matrix(NA, nrow = length(urlslist3), ncol = 3)
colnames(SLATEDF) = c("Source", "Url", "Text")
SLATEDF = as.data.frame(SLATEDF)


##for loops
for (i in 1:length(urlslist3)){
  
  ##read in the URL
  webpage2 <- read_html(urlslist3[i])
  
  ##pull the specific nodes
  headline_data2 = html_nodes(webpage2,'.slate-paragraph') 
  
  ##pull the text
  text_data2 = html_text(headline_data2)
  
  ##save the data
  SLATEDF$Source[i] = "SLATE"
  SLATEDF$Url[i] = urlslist3[i]
  SLATEDF$Text[i] = paste(text_data2, collapse = "")
  Sys.sleep(runif(1, 1, 10))
} ##end for loop

beep(sound = 3)

write.csv(SLATEDF, "SLATE_kav.csv", row.names = F)

```

```{r SLATE_shut, eval = F, include = F}
library(rvest)
library(RSelenium)
library(beepr)

#open the browser
rD <- rsDriver(browser=c("chrome"), chromever="73.0.3683.68")
remDr <- rD[["client"]]

url = "https://slate.com/search?q=government+shutdown"

remDr$navigate(url)

#create a blank space to put the links
urlslist_final = list()

#loop over the results and go through them
#appear to be 10 pages
for (i in 1:10){
  
  #find the button to click on 
  webClick <- remDr$findElements(using = "css", ".gsc-cursor-page")
  
  #don't go too fast for the page to load
  Sys.sleep(runif(1, 1, 5))
  
  #click on the page number
  webClick[[i]]$clickElement()
  
  #page reload
  Sys.sleep(runif(1, 1, 5))
  
  #find the urls for page X
  webElems <- remDr$findElements(using = "css", "[href]")
  
  #add the urls to a list
  urlslist_final[[i]] = unlist(sapply(webElems, function(x) {x$getElementAttribute("href")}))
  
  #don't go too fast
  Sys.sleep(runif(1, 1, 5))
}

remDr$close()
# stop the selenium server
rD[["server"]]$stop()

beep(sound = 3)

##deal with the URLS
urlslist = unlist(urlslist_final)

#find all the ones from 12 2018 to 02 2019
urlslist2 = urlslist[grep("/2018/12|/2019/01|/2019/02", urlslist)]
urlslist2 = unique(urlslist2)

#60 something articles seems suspiciously low, so also scraped this page
#https://slate.com/tag/governtment-shutdown

#open the browser
rD <- rsDriver(browser=c("chrome"), chromever="73.0.3683.68")
remDr <- rD[["client"]]

#create a blank space to put the links
urlslist_final = list()

#loop over the results and go through them
#354 / 20 = 18 pages
for (i in 1:18){
  
  url = paste0("https://slate.com/tag/government-shutdown/", i)
  
  remDr$navigate(url)
  
  #don't go too fast
  Sys.sleep(runif(1, 1, 5))
  
  #find the urls for page X
  webElems <- remDr$findElements(using = "css", "[href]")
  
  #add the urls to a list
  urlslist_final[[i]] = unlist(sapply(webElems, function(x) {x$getElementAttribute("href")}))
  
  #don't go too fast
  Sys.sleep(runif(1, 1, 5))
}

remDr$close()
# stop the selenium server
rD[["server"]]$stop()

beep(sound = 3)

##deal with the URLS
urlslist = unlist(urlslist_final)

#find all the ones with /2018/
urlslist2.1 = urlslist[grep("/2018/12|/2019/01|/2019/02", urlslist)]
urlslist2.1 = unique(urlslist2.1)

urlslist3 = unique(c(urlslist2, urlslist2.1))

##start a data frame
SLATEDF = matrix(NA, nrow = length(urlslist3), ncol = 3)
colnames(SLATEDF) = c("Source", "Url", "Text")
SLATEDF = as.data.frame(SLATEDF)


##for loops
for (i in 1:length(urlslist3)){
  
  ##read in the URL
  webpage2 <- read_html(urlslist3[i])
  
  ##pull the specific nodes
  headline_data2 = html_nodes(webpage2,'.slate-paragraph') 
  
  ##pull the text
  text_data2 = html_text(headline_data2)
  
  ##save the data
  SLATEDF$Source[i] = "SLATE"
  SLATEDF$Url[i] = urlslist3[i]
  SLATEDF$Text[i] = paste(text_data2, collapse = "")
  Sys.sleep(runif(1, 1, 10))
} ##end for loop

#beep(sound = 3)

write.csv(SLATEDF, "SLATE_gs.csv", row.names = F)
```

````{r HuffPo_kav, eval = F, include = F}
library(rvest)
library(RSelenium)
library(beepr)
library(lubridate)

#open the browser
rD <- rsDriver(browser=c("chrome"), chromever="73.0.3683.68")
remDr <- rD[["client"]]

url = "https://search.huffpost.com/search?utf8=%E2%9C%93&p=kavanaugh&fr=huffpost_desktop-s"

remDr$navigate(url)

#create a blank space to put the links
urlslist_final = list()

#loop over the results and go through them
#943 results with 10 per page 94 clicks
for (i in 1:94){
  
  #find the urls for page X
  webElems <- remDr$findElements(using = "class name", "fz-20")
  
  #add the urls to a list
  urlslist_final[[i]] = unlist(sapply(webElems, function(x) {x$getElementAttribute("href")}))
  
  #find the button to click on 
  webClick <- remDr$findElements(using = "class name", "next")
  
  #click on the page number
  webClick[[1]]$clickElement()
  
  #page reload
  Sys.sleep(runif(1, 1, 5))
  
}

remDr$close()
# stop the selenium server
rD[["server"]]$stop()

beep(sound = 3)

##deal with the URLS
urlslist = unlist(urlslist_final)
urlslist = unique(urlslist)

##start a data frame
HUFFPODF = matrix(NA, nrow = length(urlslist), ncol = 3)
colnames(HUFFPODF) = c("Source", "Url", "Text")
HUFFPODF = as.data.frame(HUFFPODF)
int <- interval(ymd("2018-09-13"), ymd("2018-10-11"))


for (i in 937:length(urlslist)){
  
  #convert the url
  temp = paste("https", unlist(strsplit(unlist(strsplit(urlslist[i], "https"))[3], "/RK=0"))[1], sep = "")
  temp = gsub("%3a", ":", temp)
  temp = gsub("%2f", "/", temp)
  
  webpage = read_html(temp)
  #figure out the date
  headline_data = html_nodes(webpage, '.timestamp__date--published')
  text_data = html_text(headline_data)
  time = unlist(strsplit(text_data, split = "\n| "))[2]
  time = as.Date(time, format = "%m/%d/%Y")
  
  if (time %within% int) {
    ##pull out the text
    headline_data2 = html_nodes(webpage,'p') 
    text_data2 = html_text(headline_data2)
    
    ##save the data
    HUFFPODF$Source[i] = "HUFFPO"
    HUFFPODF$Url[i] = urlslist[i]
    HUFFPODF$Text[i] = paste(text_data2, collapse = "")
  }
  
  Sys.sleep(runif(1,1,10))
  
  }

beep(sound = 3)

write.csv(HUFFPODF, "HUFFPO_kav.csv", row.names = F)
```

```{r HuffPo_shut, eval = F, include = F}
library(rvest)
library(RSelenium)
library(beepr)
library(lubridate)

#open the browser
rD <- rsDriver(browser=c("chrome"), chromever="73.0.3683.68")
remDr <- rD[["client"]]

url = "https://search.huffpost.com/search?utf8=%E2%9C%93&p=government+shutdown&fr=huffpost_desktop"

remDr$navigate(url)

#create a blank space to put the links
urlslist_final = list()

#loop over the results and go through them
#942 results with 10 per page 94 clicks
for (i in 1:94){
  
  #find the urls for page X
  webElems <- remDr$findElements(using = "class name", "fz-20")
  
  #add the urls to a list
  urlslist_final[[i]] = unlist(sapply(webElems, function(x) {x$getElementAttribute("href")}))
  
  #find the button to click on 
  webClick <- remDr$findElements(using = "class name", "next")
  
  #click on the page number
  webClick[[1]]$clickElement()
  
  #page reload
  Sys.sleep(runif(1, 1, 5))
  
}

remDr$close()
# stop the selenium server
rD[["server"]]$stop()

beep(sound = 2)

##deal with the URLS
urlslist = unlist(urlslist_final)
urlslist = unique(urlslist)

##start a data frame
HUFFPODF = matrix(NA, nrow = length(urlslist), ncol = 3)
colnames(HUFFPODF) = c("Source", "Url", "Text")
HUFFPODF = as.data.frame(HUFFPODF)
int <- interval(ymd("2018-12-08"), ymd("2019-02-08"))


for (i in 1:length(urlslist)){
  
  #convert the url
  temp = paste("https", unlist(strsplit(unlist(strsplit(urlslist[i], "https"))[3], "/RK=0"))[1], sep = "")
  temp = gsub("%3a", ":", temp)
  temp = gsub("%2f", "/", temp)
  
  webpage = read_html(temp)
  #figure out the date
  headline_data = html_nodes(webpage, '.timestamp__date--published')
  text_data = html_text(headline_data)
  time = unlist(strsplit(text_data, split = "\n| "))[2]
  time = as.Date(time, format = "%m/%d/%Y")
  
  if (time %within% int) {
    ##pull out the text
    headline_data2 = html_nodes(webpage,'p') 
    text_data2 = html_text(headline_data2)
    
    ##save the data
    HUFFPODF$Source[i] = "HUFFPO"
    HUFFPODF$Url[i] = urlslist[i]
    HUFFPODF$Text[i] = paste(text_data2, collapse = "")
  }
  
  Sys.sleep(runif(1,1,10))
  
  }

beep(sound = 2)

write.csv(HUFFPODF, "HUFFPO_gs.csv", row.names = F)
```

````{r Politico_kav, eval = F, include = F}
library(rvest)
library(RSelenium)
library(beepr)
library(lubridate)

#open the browser
rD <- rsDriver(browser=c("chrome"), chromever="73.0.3683.68")
remDr <- rD[["client"]]

url = "https://www.politico.com/search?adv=true&userInitiated=true&s=newest&q=kavanaugh&pv=&c=&r=&start=09%2F13%2F2018&start_submit=09%2F13%2F2018&end=10%2F11%2F2018&end_submit=10%2F11%2F2018"

remDr$navigate(url)

#create a blank space to put the links
urlslist_final = list()

#loop over the results and go through them
#749 with 20 per page with 37 clicks

for (i in 1:37){
  
  if (i > 1){
    #navigate to the next page
    url = paste0("https://www.politico.com/search/", i, 
                 "?s=newest&q=kavanaugh&adv=true&start=09/13/2018&end=10/11/2018")
    remDr$navigate(url)
  }
  
  #find the urls for page X
  webElems <- remDr$findElements(using = "css", "[href]")
  
  #add the urls to a list
  urlslist_final[[i]] = unlist(sapply(webElems, function(x) {x$getElementAttribute("href")}))

  #page reload
  Sys.sleep(runif(1, 1, 5))
  
}

remDr$close()
# stop the selenium server
rD[["server"]]$stop()

beep(sound = 7)

##deal with the URLS
urlslist = unlist(urlslist_final)
urlslist = unique(urlslist)
urlslist = urlslist[grep("/2018/", urlslist)]
urlslist = urlslist[-c(grep("subscriber.", urlslist))] #take out the stuff we can't see

##start a data frame
POLITICODF = matrix(NA, nrow = length(urlslist), ncol = 3)
colnames(POLITICODF) = c("Source", "Url", "Text")
POLITICODF = as.data.frame(POLITICODF)


for (i in 1:length(urlslist)){
  
  webpage = read_html(urlslist[i])
  
  headline_data2 = html_nodes(webpage,'p') 
  text_data2 = html_text(headline_data2)
  
  ##save the data
  POLITICODF$Source[i] = "POLITICO"
  POLITICODF$Url[i] = urlslist[i]
  POLITICODF$Text[i] = paste(text_data2, collapse = "")
  
  }
  
  Sys.sleep(runif(1,1,10))
  
beep(sound = 7)

write.csv(POLITICODF, "POLITICO_kav.csv", row.names = F)
#easiest one yet
```

```{r Politico_shut, eval = F, include = F}
library(rvest)
library(RSelenium)
library(beepr)
library(lubridate)

#open the browser
rD <- rsDriver(browser=c("chrome"), chromever="73.0.3683.68")
remDr <- rD[["client"]]

url = "https://www.politico.com/search?adv=true&userInitiated=true&s=newest&q=government+shutdown&pv=&c=&r=&start=12%2F08%2F2018&start_submit=12%2F08%2F2018&end=02%2F08%2F2019&end_submit=02%2F08%2F2019"

remDr$navigate(url)

#create a blank space to put the links
urlslist_final = list()

#loop over the results and go through them
#1329 with 20 per page with 66 clicks

for (i in 1:66){
  
  if (i > 1){
    #navigate to the next page
    url = paste0("https://www.politico.com/search/", i, 
                 "?s=newest&q=government shutdown&adv=true&start=12/08/2018&end=02/08/2019")
    remDr$navigate(url)
  }
  
  #find the urls for page X
  webElems <- remDr$findElements(using = "css", "[href]")
  
  #add the urls to a list
  urlslist_final[[i]] = unlist(sapply(webElems, function(x) {x$getElementAttribute("href")}))

  #page reload
  Sys.sleep(runif(1, 1, 5))
  
}

remDr$close()
# stop the selenium server
rD[["server"]]$stop()

beep(sound = 7)

##deal with the URLS
urlslist = unlist(urlslist_final)
urlslist = unique(urlslist)
urlslist = urlslist[grep("/2018/", urlslist)]
urlslist = urlslist[-c(grep("subscriber.", urlslist))] #take out the stuff we can't see

##start a data frame
POLITICODF = matrix(NA, nrow = length(urlslist), ncol = 3)
colnames(POLITICODF) = c("Source", "Url", "Text")
POLITICODF = as.data.frame(POLITICODF)


for (i in 1:length(urlslist)){
  
  webpage = read_html(urlslist[i])
  
  headline_data2 = html_nodes(webpage,'p') 
  text_data2 = html_text(headline_data2)
  
  ##save the data
  POLITICODF$Source[i] = "POLITICO"
  POLITICODF$Url[i] = urlslist[i]
  POLITICODF$Text[i] = paste(text_data2, collapse = "")
  
  
  Sys.sleep(runif(1,1,10))
  
  }
  
  
beep(sound = 7)

write.csv(POLITICODF, "POLITICO_gs.csv", row.names = F)

```

````{r Fox_kav, eval = F, include = F}

##start a data frame
library(rvest)
library(RSelenium)
library(beepr)
library(lubridate)

#open the browser
rD <- rsDriver(browser=c("chrome"), chromever="73.0.3683.68")
remDr <- rD[["client"]]

#create a blank space to put the links
urlslist_final = list()

#loop over the results and go through them
#1651 with 10 per page with 165 clicks

for (i in 1:165){
  
    #navigate to the page
    url = paste0("https://www.foxnews.com/search-results/search?q=kavanaugh&ss=fn&min_date=2018-09-13&max_date=2018-10-11&start=",
                 (i-1)*10)
    remDr$navigate(url)
  
  #find the urls for page X
  webElems <- remDr$findElements(using = "css", ".ng-binding")
  
  #add the urls to a list
  urlslist_final[[i]] = unlist(sapply(webElems, function(x) {x$getElementAttribute("href")}))

  #page reload
  Sys.sleep(runif(1, 1, 5))
  
}

remDr$close()
# stop the selenium server
rD[["server"]]$stop()

beep(sound = 6)

##deal with the URLS
urlslist = unlist(urlslist_final)
urlslist = unique(urlslist)
urlslist = urlslist[-c(grep("video.", urlslist))] #take out videos
urlslist = urlslist[-c(grep("/search-results", urlslist))] #take out search result fake links


##start a data frame
FOXDF = matrix(NA, nrow = length(urlslist), ncol = 3)
colnames(FOXDF) = c("Source", "Url", "Text")
FOXDF = as.data.frame(FOXDF)


for (i in 1:length(urlslist)){
  
  webpage = read_html(urlslist[i])
  
  headline_data2 = html_nodes(webpage,'p') 
  text_data2 = html_text(headline_data2)
  
  ##save the data
  FOXDF$Source[i] = "FOX"
  FOXDF$Url[i] = urlslist[i]
  FOXDF$Text[i] = paste(text_data2, collapse = "")
  
  
  Sys.sleep(runif(1,1,10))
  
  }
  
  
beep(sound = 7)

write.csv(FOXDF, "FOX_kav.csv", row.names = F)
```

````{r Breitbart_kav, eval = F, include = F}
library(rvest)
library(RSelenium)
library(beepr)
library(lubridate)

#open the browser
rD <- rsDriver(browser=c("chrome"), chromever="73.0.3683.68")
remDr <- rD[["client"]]

#create a blank space to put the links
urlslist_final = list()

url = "https://www.breitbart.com/tag/brett-kavanaugh/"

remDr$navigate(url)

#loop over the results and go through them
for (i in 1:42){
  
  #find the urls for page X
  webElems <- remDr$findElements(using = "css", "h2 a")
  
  #add the urls to a list
  urlslist_final[[i]] = unlist(sapply(webElems, function(x) {x$getElementAttribute("href")}))

  if (i > 1)
    {
  url = paste0("https://www.breitbart.com/tag/brett-kavanaugh/page/",
               i+1, "/")
  remDr$navigate(url)
  }
  
  #page reload
  Sys.sleep(runif(1, 1, 5))
  
}

remDr$close()
# stop the selenium server
rD[["server"]]$stop()

beep(sound = 6)

##deal with the URLS
urlslist = unlist(urlslist_final)
urlslist = unique(urlslist)
urlslist = urlslist[-c(grep("store", urlslist))] #take out store

##find the right dates
temp = strsplit(urlslist, split = "/")
temp2 = plyr::ldply(temp, rbind)
temp2 = as.data.frame(apply(temp2, 2, as.character), stringsAsFactors = F)

#pull only the right dates
rows = rownames(temp2[ temp2$`5` == 2018 & #must be 2018 and 
                         (temp2$`6` == "09" & as.numeric(temp2$`7`)>=13 | 
                            temp2$`6` == "10" & as.numeric(temp2$`7`)<=11), #Must be 9 or 10 
                       ])

urlslist3 = urlslist[as.numeric(rows)]

##start a data frame
BREITBARTDF = matrix(NA, nrow = length(urlslist3), ncol = 3)
colnames(BREITBARTDF) = c("Source", "Url", "Text")
BREITBARTDF = as.data.frame(BREITBARTDF)


for (i in 1:length(urlslist3)){
  
  webpage = read_html(urlslist3[i])
  
  headline_data2 = html_nodes(webpage,'h2, p') 
  text_data2 = html_text(headline_data2)
  
  ##save the data
  BREITBARTDF$Source[i] = "BREITBART"
  BREITBARTDF$Url[i] = urlslist3[i]
  BREITBARTDF$Text[i] = paste(text_data2, collapse = "")
  
  
  Sys.sleep(runif(1,1,10))
  
  }
  
  
beep(sound = 7)

write.csv(BREITBARTDF, "BREITBART_kav.csv", row.names = F)
```

```{r Breitbart_gs, eval = F, include = F}
library(rvest)
library(RSelenium)
library(beepr)
library(lubridate)

#open the browser
rD <- rsDriver(browser=c("chrome"), chromever="73.0.3683.68")
remDr <- rD[["client"]]

#create a blank space to put the links
urlslist_final = list()

url = "https://www.breitbart.com/tag/government-shutdown/"

remDr$navigate(url)

#loop over the results and go through them
for (i in 1:16){
  
  #find the urls for page X
  webElems <- remDr$findElements(using = "css", "h2 a")
  
  #add the urls to a list
  urlslist_final[[i]] = unlist(sapply(webElems, function(x) {x$getElementAttribute("href")}))

  if (i > 1)
    {
  url = paste0("https://www.breitbart.com/tag/government-shutdown/page/",
               i+1, "/")
  remDr$navigate(url)
  }
  
  #page reload
  Sys.sleep(runif(1, 1, 5))
  
}

remDr$close()
# stop the selenium server
rD[["server"]]$stop()

beep(sound = 6)

##deal with the URLS
urlslist = unlist(urlslist_final)
urlslist = unique(urlslist)
urlslist = urlslist[-c(grep("store", urlslist))] #take out store

##find the right dates
temp = strsplit(urlslist, split = "/")
temp2 = plyr::ldply(temp, rbind)
temp2 = as.data.frame(apply(temp2, 2, as.character), stringsAsFactors = F)

#pull only the right dates
rows = rownames(temp2[ (temp2$`5` == 2018 | temp2$`5` == 2019) & #must be 2018/19 and  
                         (
                           (temp2$`6` == "12" & as.numeric(temp2$`7`)>8 & temp2$`5` == 2018) | #after dec 12
                             (temp2$`6`== "01" & temp2$`5` == 2019) | #all of jan
                             (temp2$`6` == "02" & as.numeric(temp2$`7`)<8 & temp2$`5` == 2019)
                         ), #before feb 8
                       ])

urlslist3 = urlslist[as.numeric(rows)]

##start a data frame
BREITBARTDF = matrix(NA, nrow = length(urlslist3), ncol = 3)
colnames(BREITBARTDF) = c("Source", "Url", "Text")
BREITBARTDF = as.data.frame(BREITBARTDF)


for (i in 1:length(urlslist3)){
  
  webpage = read_html(urlslist3[i])
  
  headline_data2 = html_nodes(webpage,'h2, p') 
  text_data2 = html_text(headline_data2)
  
  ##save the data
  BREITBARTDF$Source[i] = "BREITBART"
  BREITBARTDF$Url[i] = urlslist3[i]
  BREITBARTDF$Text[i] = paste(text_data2, collapse = "")
  
  
  Sys.sleep(runif(1,1,10))
  
  }
  
  
beep(sound = 7)

write.csv(BREITBARTDF, "BREITBART_gs.csv", row.names = F)
```

````{r Rush_kav, eval = F, include = F}
library(rvest)
library(RSelenium)
library(beepr)
library(lubridate)

#open the browser
rD <- rsDriver(browser=c("chrome"), chromever="73.0.3683.68")
remDr <- rD[["client"]]

#create a blank space to put the links
urlslist_final = list()

url = "https://www.rushlimbaugh.com/?s=kavanaugh&is_v=1"

remDr$navigate(url)

#loop over the results and go through them
##18 pages
for (i in 1:18){
  
  #find the urls for page X
  webElems <- remDr$findElements(using = "css", "a")
  
  #add the urls to a list
  urlslist_final[[i]] = unlist(sapply(webElems, function(x) {x$getElementAttribute("href")}))

  #find the thing to click on 
  webClick <- remDr$findElements(using = "css", ".ais-pagination--link")
  
  #click on the page number
  webClick[[18]]$clickElement()
  
  #page reload
  Sys.sleep(runif(1, 1, 5))
  
}

remDr$close()
# stop the selenium server
rD[["server"]]$stop()

beep(sound = 6)

##deal with the URLS
urlslist = unlist(urlslist_final)
urlslist = unique(urlslist)
urlslist = urlslist[-c(grep("store", urlslist))] #take out store
urlslist = urlslist[grep("/daily/[0-9]*/[0-9]*/[0-9]*/[a-zA-Z]", urlslist)]

##find the right dates
temp = strsplit(urlslist, split = "/")
temp2 = plyr::ldply(temp, rbind)
temp2 = as.data.frame(apply(temp2, 2, as.character), stringsAsFactors = F)

#pull only the right dates
rows = rownames(temp2[ temp2$`5` == 2018 & #must be 2018 and 
                         (temp2$`6` == "09" & as.numeric(temp2$`7`)>=13 | 
                            temp2$`6` == "10" & as.numeric(temp2$`7`)<=11), #Must be 9 or 10 
                       ])

urlslist3 = urlslist[as.numeric(rows)]

##start a data frame
RUSHDF = matrix(NA, nrow = length(urlslist3), ncol = 3)
colnames(RUSHDF) = c("Source", "Url", "Text")
RUSHDF = as.data.frame(RUSHDF)


for (i in 1:length(urlslist3)){
  
  webpage = read_html(urlslist3[i])
  
  headline_data2 = html_nodes(webpage,'p') 
  text_data2 = html_text(headline_data2)
  
  ##save the data
  RUSHDF$Source[i] = "RUSH"
  RUSHDF$Url[i] = urlslist3[i]
  RUSHDF$Text[i] = paste(text_data2, collapse = "")
  
  
  Sys.sleep(runif(1,1,10))
  
  }
  
  
beep(sound = 7)

write.csv(RUSHDF, "RUSH_kav.csv", row.names = F)
```

```{r Rush_shut, eval = F, include = F}
library(rvest)
library(RSelenium)
library(beepr)
library(lubridate)

#open the browser
rD <- rsDriver(browser=c("chrome"), chromever="73.0.3683.68")
remDr <- rD[["client"]]

#create a blank space to put the links
urlslist_final = list()

url = "https://www.rushlimbaugh.com/?s=government%20shutdown&is_v=1"

remDr$navigate(url)

#loop over the results and go through them
##47 pages
for (i in 1:47){
  
  #find the urls for page X
  webElems <- remDr$findElements(using = "css", "a")
  
  #add the urls to a list
  urlslist_final[[i]] = unlist(sapply(webElems, function(x) {x$getElementAttribute("href")}))

  #find the thing to click on 
  webClick <- remDr$findElements(using = "css", ".ais-pagination--link")
  
  #click on the page number
  webClick[[18]]$clickElement()
  
  #page reload
  Sys.sleep(runif(1, 1, 5))
  
}

remDr$close()
# stop the selenium server
rD[["server"]]$stop()

beep(sound = 6)

##deal with the URLS
urlslist = unlist(urlslist_final)
urlslist = unique(urlslist)
urlslist = urlslist[-c(grep("store", urlslist))] #take out store
urlslist = urlslist[grep("/daily/[0-9]*/[0-9]*/[0-9]*/[a-zA-Z]", urlslist)]

##find the right dates
temp = strsplit(urlslist, split = "/")
temp2 = plyr::ldply(temp, rbind)
temp2 = as.data.frame(apply(temp2, 2, as.character), stringsAsFactors = F)

#pull only the right dates
rows = rownames(temp2[ (temp2$`5` == 2018 | temp2$`5` == 2019) & #must be 2018/19 and  
                         (
                           (temp2$`6` == "12" & as.numeric(temp2$`7`)>8 & temp2$`5` == 2018) | #after dec 12
                             (temp2$`6`== "01" & temp2$`5` == 2019) | #all of jan
                             (temp2$`6` == "02" & as.numeric(temp2$`7`)<8 & temp2$`5` == 2019)
                         ), #before feb 8
                       ])
urlslist3 = urlslist[as.numeric(rows)]

##start a data frame
RUSHDF = matrix(NA, nrow = length(urlslist3), ncol = 3)
colnames(RUSHDF) = c("Source", "Url", "Text")
RUSHDF = as.data.frame(RUSHDF)


for (i in 1:length(urlslist3)){
  
  webpage = read_html(urlslist3[i])
  
  headline_data2 = html_nodes(webpage,'p') 
  text_data2 = html_text(headline_data2)
  
  ##save the data
  RUSHDF$Source[i] = "RUSH"
  RUSHDF$Url[i] = urlslist3[i]
  RUSHDF$Text[i] = paste(text_data2, collapse = "")
  
  
  Sys.sleep(runif(1,1,10))
  
  }
  
  
beep(sound = 7)

write.csv(RUSHDF, "RUSH_gs.csv", row.names = F)
 
```

````{r Blaze_kav, eval = F, include = F}
library(rvest)
library(RSelenium)
library(beepr)
library(lubridate)

#open the browser
rD <- rsDriver(browser=c("chrome"), chromever="73.0.3683.68")
remDr <- rD[["client"]]

url9 = 'https://www.theblaze.com/search/?q=kavanaugh'
webpage9 = read_html(url9)
headline_data9 = html_nodes(webpage9, '.widget__headline-text')

attr_data9 = html_attrs(headline_data9) 
attr_data9

urlslist9 = unlist(attr_data9)
urlslist9 = urlslist9[grep(urlslist9)]

##start a data frame
BlazeDF = matrix(NA, nrow = length(urlslist9), ncol = 3)
colnames(BlazeDF) = c("Source", "Url", "Text")
BlazeDF = as.data.frame(BlazeDF)

##for loops
for (i in 1:length(urlslist9)){
  
  ##read in the URL
  webpage9 <- read_html(urlslist9[i])
  
  ##pull the specific nodes
  headline_data9 = html_nodes(webpage9,'p') 
  
  ##pull the text
  text_data9 = html_text(headline_data9)
  
  ##save the data
  BlazeDF$Source[i] = "The Blaze"
  BlazeDF$Url[i] = urlslist9[i]
  BlazeDF$Text[i] = paste(text_data9, collapse = "")
} ##end for loop
```

```{r Blaze_shut, eval = F, include = F}
library(rvest)
library(RSelenium)
library(beepr)
library(lubridate)

#open the browser
rD <- rsDriver(browser=c("chrome"), chromever="73.0.3683.68")
remDr <- rD[["client"]]

url = "https://www.theblaze.com/search/?q=government+shutdown"

remDr$navigate(url)

webClick <- remDr$findElements(using = "css", ".action-btn")

#click next until done
while (length(webClick) > 0 ) { 
  
  #find the urls for page X
  webClick <- remDr$findElements(using = "css", ".action-btn")
  
  webClick[[1]]$clickElement()
  
  Sys.sleep(runif(1, 1, 5))
}

#find the urls for page X
webElems <- remDr$findElements(using = "css", "a")

#add the urls to a list
urlslist_final = unlist(sapply(webElems, function(x) {x$getElementAttribute("href")}))

remDr$close()
# stop the selenium server
rD[["server"]]$stop()


##deal with the URLS
urlslist = urlslist_final
urlslist = unique(urlslist)
urlslist = urlslist[-c(grep("video/", urlslist))]
urlslist = urlslist[grep("/news/", urlslist)]
urlslist = urlslist[-c(grep("/201[3-7]", urlslist))]

##start a data frame
BLAZEDF = matrix(NA, nrow = length(urlslist), ncol = 3)
colnames(BLAZEDF) = c("Source", "Url", "Text")
BLAZEDF = as.data.frame(BLAZEDF)
int <- interval(ymd("2018-12-08"), ymd("2019-02-08"))


for (i in 1:length(urlslist)){
  
  webpage = read_html(urlslist[i])
  
  headline_data = html_nodes(webpage, '.post-date')
  text_data = html_text(headline_data)
  date = as.Date(text_data, "%B %d, %Y")
  
  if (date %within% int){
    headline_data2 = html_nodes(webpage,'h1, h3, p') 
    text_data2 = html_text(headline_data2)
    
    ##save the data
    BLAZEDF$Source[i] = "BLAZE"
    BLAZEDF$Url[i] = urlslist[i]
    BLAZEDF$Text[i] = paste(text_data2, collapse = "")
  }
  
  Sys.sleep(runif(1,1,10))
  
  }
  
  
beep(sound = 7)

write.csv(BLAZEDF, "BLAZE_gs.csv", row.names = F)

```

```{r Hannity_kav, eval = F, include = F}
library(rvest)
library(RSelenium)
library(beepr)
library(lubridate)

#open the browser
rD <- rsDriver(browser=c("chrome"), chromever="73.0.3683.68")
remDr <- rD[["client"]]

url = "https://www.hannity.com/trending/brett-kavanaugh/"

remDr$navigate(url)

urlslist_final = list()

for (i in 1:4){
  
  #find the urls for page X
  webElems <- remDr$findElements(using = "css", "a")
  
  #add the urls to a list
  urlslist_final[[i]] = unlist(sapply(webElems, function(x) {x$getElementAttribute("href")}))
  
  #move on to the next page
  url = paste0("https://www.hannity.com/trending/brett-kavanaugh/?pg=", i+1)
  
  remDr$navigate(url)
  
  Sys.sleep(runif(1,1,5))
  
}

remDr$close()
# stop the selenium server
rD[["server"]]$stop()


##deal with the URLS
urlslist = unlist(urlslist_final)
urlslist = unique(urlslist)

#also use google search to see if we can find more

#open the browser
rD <- rsDriver(browser=c("chrome"), chromever="73.0.3683.68")
remDr <- rD[["client"]]

url = "https://www.google.com/search?q=kavanaugh+site%3Ahannity.com&source=lnt&tbs=cdr%3A1%2Ccd_min%3A9%2F13%2F2018%2Ccd_max%3A10%2F11%2F2018&tbm="

remDr$navigate(url)

urlslist_final2 = list()

for (i in 1:3){
  
  #find the urls for page X
  webElems <- remDr$findElements(using = "css", "a")
  
  #add the urls to a list
  urlslist_final2[[i]] = unlist(sapply(webElems, function(x) {x$getElementAttribute("href")}))
  
  #find the thing to click on 
  webClick <- remDr$findElements(using = "link text", "Next")
  
  #click on the page number
  webClick[[1]]$clickElement()
  
  Sys.sleep(runif(1,1,5))
  
}

remDr$close()
# stop the selenium server
rD[["server"]]$stop()

urlslist2 = unlist(urlslist_final2)
urlslist2 = c(urlslist, urlslist2)
urlslist2 = unique(urlslist2)
urlslist3 = urlslist2[grep("/media-room/[a-zA-Z]", urlslist2)]
urlslist3 = urlslist3[-c(grep("webcache", urlslist3))]
urlslist3 = urlslist3[-c(grep("radio-show", urlslist3))]


##start a data frame
HANNITYDF = matrix(NA, nrow = length(urlslist), ncol = 3)
colnames(HANNITYDF) = c("Source", "Url", "Text")
HANNITYDF = as.data.frame(HANNITYDF)
int <- interval(ymd("2018-09-13"), ymd("2018-10-11"))

for (i in 1:length(urlslist3)){
  
  webpage = read_html(urlslist3[i])
  
  headline_data = html_nodes(webpage, '.author-info')
  text_data = html_text(headline_data)
  text_data = strsplit(text_data, " ")[[1]][7]
  date = as.Date(text_data, "%m.%d.%y")
  
  if (date %within% int){
    headline_data2 = html_nodes(webpage,'h1, p') 
    text_data2 = html_text(headline_data2)
    
    ##save the data
    HANNITYDF$Source[i] = "HANNITY"
    HANNITYDF$Url[i] = urlslist3[i]
    HANNITYDF$Text[i] = paste(text_data2, collapse = "")
  }
  
  Sys.sleep(runif(1,1,10))
  
  }
  
  
beep(sound = 7)

write.csv(HANNITYDF, "HANNITY_kav.csv", row.names = F)
```

```{r Hannity_shut, eval = F, include = F}
library(rvest)
library(RSelenium)
library(beepr)
library(lubridate)

#open the browser
rD <- rsDriver(browser=c("chrome"), chromever="73.0.3683.68")
remDr <- rD[["client"]]

url = "http://www.hannity.com/trending/shutdown/"

remDr$navigate(url)

urlslist_final = list()

for (i in 1:4){
  
  #find the urls for page X
  webElems <- remDr$findElements(using = "css", "a")
  
  #add the urls to a list
  urlslist_final[[i]] = unlist(sapply(webElems, function(x) {x$getElementAttribute("href")}))
  
  #move on to the next page
  url = paste0("http://www.hannity.com/trending/shutdown/?pg=", i+1)
  
  remDr$navigate(url)
  
  Sys.sleep(runif(1,1,5))
  
}

remDr$close()
# stop the selenium server
rD[["server"]]$stop()


##deal with the URLS
urlslist = unlist(urlslist_final)
urlslist = unique(urlslist)

#also use google search to see if we can find more

#open the browser
rD <- rsDriver(browser=c("chrome"), chromever="73.0.3683.68")
remDr <- rD[["client"]]

url = "https://www.google.com/search?biw=1366&bih=632&tbs=cdr%3A1%2Ccd_min%3A12%2F8%2F2018%2Ccd_max%3A2%2F8%2F2019&ei=gECaXNe8Aaft_Qb9w7G4BA&q=government+shutdown+site%3Ahannity.com&oq=government+shutdown+site%3Ahannity.com&gs_l=psy-ab.3...0.0..1915...0.0..0.0.0.......0......gws-wiz.aFps0nA33mY"

remDr$navigate(url)

urlslist_final2 = list()

for (i in 1:9){
  
  #find the urls for page X
  webElems <- remDr$findElements(using = "css", "a")
  
  #add the urls to a list
  urlslist_final2[[i]] = unlist(sapply(webElems, function(x) {x$getElementAttribute("href")}))
  
  #find the thing to click on 
  webClick <- remDr$findElements(using = "link text", "Next")
  
  #click on the page number
  webClick[[1]]$clickElement()
  
  Sys.sleep(runif(1,1,5))
  
}

remDr$close()
# stop the selenium server
rD[["server"]]$stop()

urlslist2 = unlist(urlslist_final2)
urlslist2 = c(urlslist, urlslist2)
urlslist2 = unique(urlslist2)
urlslist3 = urlslist2[grep("/media-room/[a-zA-Z]", urlslist2)]
urlslist3 = urlslist3[-c(grep("webcache", urlslist3))]
urlslist3 = urlslist3[-c(grep("radio-show", urlslist3))]


##start a data frame
HANNITYDF = matrix(NA, nrow = length(urlslist), ncol = 3)
colnames(HANNITYDF) = c("Source", "Url", "Text")
HANNITYDF = as.data.frame(HANNITYDF)
int <- interval(ymd("2018-12-08"), ymd("2019-02-08"))

for (i in 1:length(urlslist3)){
  
  webpage = read_html(urlslist3[i])
  
  headline_data = html_nodes(webpage, '.author-info')
  text_data = html_text(headline_data)
  text_data = strsplit(text_data, " ")[[1]][7]
  date = as.Date(text_data, "%m.%d.%y")
  
  if (date %within% int){
    headline_data2 = html_nodes(webpage,'h1, p') 
    text_data2 = html_text(headline_data2)
    
    ##save the data
    HANNITYDF$Source[i] = "HANNITY"
    HANNITYDF$Url[i] = urlslist3[i]
    HANNITYDF$Text[i] = paste(text_data2, collapse = "")
  }
  
  Sys.sleep(runif(1,1,10))
  
  }
  
  
beep(sound = 7)

write.csv(HANNITYDF, "HANNITY_gs.csv", row.names = F)

```

## Sources

Articles pertaining to the Brett Kavanaugh Supreme Court confirmation hearing and the 2018-2019 U.S. Government shutdown were scraped from the websites of 10 U.S. news sources. As in Experiment 1, these sources were selected owing to their favorability among political partisans according to @Mitchell2014. The sources favored by the highest proportion of consistent liberals were *The New York Times*, *National Public Radio (NPR)*, *Slate*, *Huffington Post*, and *Politico* [@Mitchell2014]. The sources favored by the highest proportion of consistent conservatives included *Fox News*, *Breitbart*, *The Rush Limbaugh Show*, *The Blaze*, and *Sean Hannity*. Political articles referencing Brett Kavanaugh's nomination process were identified and subsequently scraped by including the URL for each source's coverage of the nomination in the *R* script. All code for this manuscript can be found at https://osf.io/5kpj7/, and the scripts, again written with the *papaja* library in *R*, are provided inline with this manuscript [@Aust2017].

## Materials

Using the *rvest* library in the statistical package *R*, we pulled body text for individual articles from each of the aforementioned 10 news sources (identified using CSS language). We compiled the articles into a dataset [@Wickham2016]. Using this dataset, we identified word count and average word count per source. This process was run for articles pertaining to Kavanaugh's nomination that were published between September 13, 2018 and October 11, 2018 inclusive. This date range was selected in reference to the widely-publicized and viewed nomination hearing on September 27, 2018. We set the start date at September 13 (two weeks before the hearing) and the end date at October 11 (two weeks after the hearing) so that we could capture a large amount of data (roughly one month) during which Kavanaugh's nomination was at its peak saturation in news coverage. 

The same process was followed for scraping articles related to the partial U.S. Government shutdown of 2018-2019. The articles scraped were published between December 8, 2018 and February 8, 2019 inclusive. Once again, the researchers elected to scrape articles published two weeks before and after the event in question in order to capitalize on the shutdown's saturation in American news media.

```{r data_clean_kav, eval = F, echo = F}
#Merge Exp2 text files and code partisan lean
library(stringr)
file.names = dir("C:/Users/kayla/Documents/GitHub/Will-Pilot/exp2_data/", pattern = '.csv')
first = TRUE
for(i in 1:length(file.names))
{
  df = read.csv(file.names[i], stringsAsFactors = FALSE)
  colnames(df)[1] = "Source"
  df$Text = gsub('\n', ' ', df$Text)
  df$Text = gsub('\t', ' ', df$Text)
  event = str_extract(file.names[i], '_[a-z]+')
  event = str_replace(event, "_", '')
  df$Event = rep(event, nrow(df))
  if(df$Source=='BLAZE'|df$Source=='BREITBART'|df$Source=='FOX'|df$Source=='HANNITY'|df$Source=='RUSH')
  {df$PartisanLean = rep('Conservative', nrow(df))}
  if(df$Source=='HUFFPO'|df$Source=='NPR'|df$Source=='NY Times'|df$Source=='POLITICO'|df$Source=='SLATE')
  {df$PartisanLean = rep('Liberal', nrow(df))}
  if(first==TRUE)
  {
    df_full = df
    first = FALSE
  }
  df_full = rbind(df_full, df)
}

write.csv(df_full, "exp2_merged data.csv", row.names = FALSE)
```

move the data collection from the original thesis
both effect sizes and topics analysis 

```{r bootstrap-study-2-goverment-shutdown}
# Import the Data
exp2 <- import("exp2_merged data_LIWC.csv")
colnames(exp2)[1:5] = c('Source', 'URL', 'Text', 'Event', 'PartisanLean')

# Eliminate Missing Data
exp2 <- na.omit(exp2)

#subset with Event=='gs'
exp2_gs <- subset(exp2, Event == "gs")

# Create group labels
exp2_gs$PartisanLean = ifelse(exp2_gs$PartisanLean=='Conservative', 0, 1)

# Create IVs and DVs
groups = c(0, 1)
DVs = c('Analytic','Clout','Authentic','Tone','WPS','Sixltr','anger','social','family','female','male','cogproc','affiliation','achieve','power','reward','risk','focuspast','focuspresent','focusfuture','money','relig','death')

# Loop over the DVs and create values 
# This saves the difference between groups
r = 1
for(DV in DVs){
  for(group in groups){
    data = subset(exp2_gs, PartisanLean==group) #data name
    f = as.formula(paste(DV,'~1', sep=''))
    bs = bootstrap_values(f,data,nsim)
    table2[r, 1] = DV
    if(group==0){
      table2[r, 2] = mean(bs$mean) #table name all the way down 
      table2[r, 3] = mean(bs$sd)
      ncon = length(na.omit(data[[DV]]))
      table2[r, 9] = quantile(bs$mean, 0.025)
      table2[r, 10] = quantile(bs$mean, 0.975)
    }
    if(group==1){
      table2[r, 4] = mean(bs$mean)
      table2[r, 5] = mean(bs$sd)
      nlib = length(na.omit(data[[DV]]))
      table2[r, 11] = quantile(bs$mean, 0.025)
      table2[r, 12] = quantile(bs$mean, 0.975)
    }
  }
  mdiff = d.ind.t(m1 = table2[r,2], m2 = table2[r,4], 
                sd1 = table2[r,3], sd2 = table2[r,5],
                n1 = ncon, 
                n2 = nlib, 
                a = .05)
  table2[r, 6] = mdiff$d
  table2[r, 7] = mdiff$dlow
  table2[r, 8] = mdiff$dhigh
  r = r + 1
}

```

```{r bootstrap-study-2-kavanaugh-hearings}

#subset with Event=='kav'
exp2_kav  <- subset(exp2, Event == "kav")

# Create group labels
exp2_kav$PartisanLean = ifelse(exp2_kav$PartisanLean=='Conservative', 0, 1)

# Create IVs and DVs
groups = c(0, 1)
DVs = c('Analytic','Clout','Authentic','Tone','WPS','Sixltr','anger','social','family','female','male','cogproc','affiliation','achieve','power','reward','risk','focuspast','focuspresent','focusfuture','money','relig','death')

# Loop over the DVs and create values 
# This saves the difference between groups
r = 1
for(DV in DVs){
  for(group in groups){
    data = subset(exp2_kav, PartisanLean==group) #data name
    f = as.formula(paste(DV,'~1', sep=''))
    bs = bootstrap_values(f,data,nsim)
    table3[r, 1] = DV
    if(group==0){
      table3[r, 2] = mean(bs$mean) #table name all the way down 
      table3[r, 3] = mean(bs$sd)
      ncon = length(na.omit(data[[DV]]))
      table3[r, 9] = quantile(bs$mean, 0.025)
      table3[r, 10] = quantile(bs$mean, 0.975)
    }
    if(group==1){
      table3[r, 4] = mean(bs$mean)
      table3[r, 5] = mean(bs$sd)
      nlib = length(na.omit(data[[DV]]))
      table3[r, 11] = quantile(bs$mean, 0.025)
      table3[r, 12] = quantile(bs$mean, 0.975)
    }
  }
  mdiff = d.ind.t(m1 = table3[r,2], m2 = table3[r,4], 
                sd1 = table3[r,3], sd2 = table3[r,5],
                n1 = ncon, 
                n2 = nlib, 
                a = .05)
  table3[r, 6] = mdiff$d
  table3[r, 7] = mdiff$dlow
  table3[r, 8] = mdiff$dhigh
  r = r + 1
}
```


# Discussion


\newpage

# References

\begingroup
\setlength{\parindent}{-0.5in}
\setlength{\leftskip}{0.5in}

<div id="refs" custom-style="Bibliography"></div>
\endgroup
