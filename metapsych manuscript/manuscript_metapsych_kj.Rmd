---
title             : "Moral Foundations of U.S. Political News Organizations"
shorttitle        : "MORAL NEWS"

author: 
  - name          : "William E. Padfield"
    affiliation   : "1"
    corresponding : yes    # Define only one corresponding author
    address       : "901 S. National Ave, Springfield, MO, 65897"
    email         : "Padfield94@live.missouristate.edu"
  - name          : "Erin M. Buchanan, Ph.D."
    affiliation   : "2"
  - name          : "Kayla N. Jordan, Ph.D."
    affiliation   : "2"

affiliation:
  - id            : "1"
    institution   : "Missouri State University"
  - id            : "2"
    institution  : "Harrisburg University of Science and Technology"
  
author_note: |
  William Padfield is a master's degree candidate in Psychology at Missouri State University. This thesis partially fulfills the requirements for the Master of Science degree in Psychology.

abstract: |
  Partisan differences and diviseness have become an increasing hot topic in psychological research. Many theories have been proposed to explain these differences and divisions including Moral Foundations Theory. The current research seeks to use a linguistic measure of Moral Foundations, the Moral Foundations Dictionary (MFD), to test the theory in terms of predicted partisan differences. Through web scraping, the researchers extracted articles from popular partisan news sources' websites, calculated MFD word frequencies, and identified words' respective valences. This process attempts to uncover news outlets' positive or negative endorsements of certain moral dimensions concomitant with a particular ideology. In Experiment 1, the researchers gathered political articles from four sources. We were unable to reveal significant differences in moral endorsements, but we solidified the method to be employed in further research. In Experiment 2, the researchers expanded their number of sources to 10 and analyzed articles that pertain to two specific topics: the 2018 confirmation hearings of U.S. Supreme Court Justice Brett Kavanaugh and the partial U.S. Government Shutdown of 2018-2019. Once again, no significant differences in moral endorsements were found. Together with past work, the results shed doubt on the validity of the MFD as a measure.
  
keywords          : "politics, morality, psycholinguistics"

bibliography      : ["will_thesis.bib"] #turn this off right now so it knits

figsintext        : no
figurelist        : no
tablelist         : no
footnotelist      : no
lineno            : no
lang              : "english"
class             : "man"
mask              : no
output            : papaja::apa6_pdf
replace_ampersands: yes
csl               : apa6.csl
---

```{r load_packages, include = FALSE}
library(papaja) 
library(MuMIn)
library(reshape)
library(nlme)
library(ggplot2)
library(expss)
library(MOTE)

knitr::opts_chunk$set(cache = TRUE)
options(tinytex.verbose = TRUE)
```

The field of moral psychology has a long history with modern moral psychology beginning in the late 1960s with Lawrence Kohlberg's theory of moral development [@Kohlberg1977].Since then, Kohlberg's theory has been highly criticized with numerous theory and hypotheses being proposed to replace it. One of these theories is Moral Foundations Theory (MFT) proposed by Jonathan Haidt and Jesse Graham [@Haidt2007]. While the theory itself has been widely criticized and debated, the goal of the current work is to tackle a related issue, the Moral Foundations Dictionary (MFD) [@Graham2009].@Graham2009 developed a lexicon-based linguistic measure of MFT arguing that moral concerns and the propositions of MFT could be measured and tested with the words people use. Here we first review evidence and criticisms against this assumption. We then provide our own test of the MFD using partisan news articles. 

## Moral Foundations Dictionary

Jonathan Haidt and Jesse Graham formulated Moral Foundations Theory as a method by which to capture the entirety of humans' moral domain [@Haidt2007]. The researchers argued older theories of moral psychology were focused primarily on issues of justice, fairness, and caring - individually focused foundations of morality that align with the beliefs of political liberals [@Haidt2007]. In other words, moral psychology ignored the valid moral foundations of conservatives. Moral Foundations Theory (MFT) holds that people's moral domain can be mapped by quantifying their endorsement of five moral foundations: *harm/care*, *fairness/reciprocity*, *ingroup/loyalty*, *authority/respect*, and *purity/sanctity* [@Haidt2007]. 

In order to capture language's role in moral and political reasoning, @Graham2009 formulated the Moral Foundations Dictionary (MFD) in order to capture moral reasoning and justification as used in speech and text. The MFD is composed of 259 words, with around 50 words assigned to each of the five foundations. @Graham2009 created a preliminary list of words that they believed would be associated with the five foundations. Then, using the Linguistic Inquiry and Word Count [LIWC; @Pennebaker2007] computer program, they analyzed transcripts of liberal and conservative Christian sermons in order to obtain frequencies of the occurrence of words from the researchers' initial list.

Similar to previous research on Moral Foundations Theory, liberal ministers used *harm*, *fairness*, and *ingroup* words more often than conservative ministers. Conversely, conservative ministers used *authority* and *purity* words more often than liberal ministers. However, conservative ministers did not use *ingroup/loyalty* words more than liberals. Rather, liberal ministers used words pertaining to *ingroup/loyalty*, but in contexts that promote rebellion and independence - causes *opposite* to positive endorsements of that foundation [@Graham2009].

## Critiques of Moral Foundations

Moral Foundations Theory has received criticism on the grounds that its assumptions regarding moral intuitions have little empirical basis. @Suhler2011 criticized the content and taxonomy of the five foundations and question whether or not the foundations are sufficiently distinct as to stand as their own foundation. Likewise, @Gray2015 argues that the measurement of the moral foundations confounds morality with other constructs (such as weirdness) and the moral foundations lack statistical and conceptual distinction. @Schien2015 argues that harm-based morality is the most parsimonious and logical explanation for MFT findings. 

Beyond the critiques of the theory itself, many have also provided strong critiques of the Moral Foundations Dictionary (MFD).Conducting two close conceptual replication and six extension studies, @Frimer2020 found that the liberal-conservative differences found in the original study replicated in only 30% of cases and the effect sizes were over 30 times smaller than reported in @Graham2009.In a meta-analysis, the theorized differences were only found for authority and purity (e.g. conservatives used more authority and purity word). Loyalty was contrary to theoretical predictions (e.g. used more by liberals), and no differences were found for harm and fairness.Other studies have also found conflicting findings which fail to support the basic assertions of MFT. Exploring the political Twitterverse, @Sterling2018 found differences in the MFD to depend on political sophistication. For example, harm words were used more by liberals only if they had low levels of political sophistication; at high levels of political sophistication, conservatives used more harm words. While the MFD has not been extensive used (at least in published research), studies which have use the MFD to test partisan differences have found mixed results in terms of supporting MFT [@Clifford2013; @Frimer2015; @Sagi2014].

The most successful attempts to validate a linguistic measure of moral foundations have used more complex methods (compared to the original dictionary-based approach).These attempts generally fall into two categories: human annotations and semantic representations. @Hopp2021 represents the first approach where instead of relying on 'experts', a large sample of lay people were crowdsourced to manually annotate document for content relevant to each foundation. While resource-intensive, this method better captures how people may encounter and judge moral issues in everyday life leading to a more ecologically valid dictionary. @Garten2016 represents the second categories which instead relies on data-driven semantic analysis. Using shallow learning models like word2vec, moral foundations are measured using semantic similarity vectors rather than simple word counts. 

As a further conceptual replication of @Frimer2020, here we test the liberal-conservative difference proposed by MFT using partisan media content and the Moral Foundations Dictionary. We first test differences in a general news corpus compared liberal sources (NPR and New York Times) to conservative sources (Fox News and Breitbart). Second, we examined news about specific political events: Brett Kavaugh's Supreme Court nomination and the Government Shutdown in 2018-19. Here the news sources were expanded to cover NYT, NPR, Slate, Huffington Post, and Politico on the liberal side and Fox News, Breitbart, The Rush Limbaugh Show, The Blaze, and Sean Hannity on the conservative side. To address potential issues of measurement with the MFD, the MFD was combined with previous research by the current authors (see below) and weighted by valence to create weighted percentages to better specify endorsement. Like @Frimer2020, results were judged based on the direction of the liberal-conservative difference compared MFT predictions and the size of the effect compared to past studies.

# Experiment 1 

# Method

For Experiment 1, the researchers approached the study with the intention to answer a method question. That is, this portion of the current research was conducted in order to solidify the best method by which to analyze political news text under the Moral Foundations Theory framework while also alleviating some of the aforementioned valence problem observed in the Moral Foundations Dictionary. The researchers hypothesized the news sources generally perceived as liberal leaning (*NPR* and *The New York Times*) would contain MFD words and valences indicating endorsements of the individualizing moral foundations (*harm/care* and *fairness/reciprocity*). Additionally, the researchers hypothesized the two sources generally perceived to be conservative leaning (*Fox News* and *Breitbart*) would feature MFD words and valences indicating equal endorsement of all five foundations. Owing to the lack of a need for human participants, the researchers did not petition Missouri State University's Institutional Review Board, as no such approval was needed to conduct this study.

## Sources

Political articles were collected from the websites of four notable U.S. news sources, a process known as web scraping. The sources were *The New York Times*, *National Public Radio (NPR)*, *Fox News*, and *Breitbart*. They were selected for their widespread recognition and the fact political partisans have strong preferences for some sources over others. The researchers determined the political lean of each source by referencing @Mitchell2014's article demonstrating the self-reported ideological consistency represented by the consumers of several news sources. In general, *The New York Times* and *NPR* are preferred by consumers reporting a liberal bias or lean. In contrast, *Fox News* and *Breitbart* are believed to have a conservative bias or lean. @Mitchell2014's article presented political ideology as a scale ranging from "consistently liberal" to "consistently conservative." In between these extremes lie more moderate positions, including "mostly liberal," "mixed," and "mostly conservative." Owing to the lower number of sources analyzed herein, the researchers elected to categorize the sources as either "liberal" and "conservative" in order to form a basis for comparison.   

Political articles in particular were identified and subsequently scraped by including the specific URL directing to each source's political content in the *R* script. For example, rather than scrape from nytimes.com, which would return undesired results (non-political features, reviews, etc.), we instead included nytimes.com/section/politics so that more or less exclusively political content was obtained. All code for this manuscript can be found at https://osf.io/5kpj7/, and the scripts are provided inline with this manuscript written with the *papaja* library [@Aust2017]. 

Identification of the sources' political URLs presented a problem for two of the sources owing to complications with how their particular sites were structured. While in the multi-week process of scraping articles, we noticed word counts for *NPR* and *Fox News* were not growing at a similar pace as those from *The New York Times* and *Breitbart*. Upon investigation, we found another, more robust URL for political content from NPR: their politics content "archive." The page structure on NPR's website was such that only a limited selection of articles is displayed to the user at a given time. Scraping both the archive and the normal politics page ensured we were obtaining most (if not all) new articles as they were published. We later ran a process in order to exclude any duplicate articles. *Fox News* presented a similar issue. We discovered *Fox News* utilized six URLs in addition to the regular politics page. These URLs led to pages containing content pertaining the U.S. Executive Branch, Senate, House of Representatives, Judicial Branch, foreign policy, and elections. Once again, duplicates were subsequently eliminated from any analyses.  

## Materials

Using the *rvest* library in the statistical package *R*, we pulled body text for individual articles from each of the aforementioned sources (identified using CSS language) and compiled them into a dataset [@Wickham2016]. Using this dataset, we identified word count and average word count per source. This process was completed once daily starting in February 2018 until March 2018. Starting in mid-March 2018, the process was completed twice daily - once in the morning and again in the evening. Data collection was terminated once 250,000 words per source was collected in April 2018.  

```{r scraping, eval=FALSE, include=FALSE}

library(rvest)
####NY Times####
#Specifying the url for desired website to be scrapped
url <- 'https://www.nytimes.com/section/politics'

#Reading the HTML code from the website - headlines
webpage <- read_html(url)
headline_data = html_nodes(webpage,'.story-link a, .story-body a')

attr_data = html_attrs(headline_data) 
attr_data

urlslist = unlist(attr_data)
urlslist = urlslist[grep("http", urlslist)]
urlslist = unique(urlslist)
urlslist

##start a data frame
NYtimesDF = matrix(NA, nrow = length(urlslist), ncol = 3)
colnames(NYtimesDF) = c("Source", "Url", "Text")
NYtimesDF = as.data.frame(NYtimesDF)

##for loops
for (i in 1:length(urlslist)){
  
  ##read in the URL
  webpage <- read_html(urlslist[i])
  
  ##pull the specific nodes
  headline_data = html_nodes(webpage,'.story-content') 
  
  ##pull the text
  text_data = html_text(headline_data)
  
  ##save the data
  NYtimesDF$Source[i] = "NY Times"
  NYtimesDF$Url[i] = urlslist[i]
  NYtimesDF$Text[i] = paste(text_data, collapse = "")
    } ##end for loop

####NPR original front page####
url2 = 'https://www.npr.org/sections/politics/'
webpage2 = read_html(url2)
headline_data2 = html_nodes(webpage2,'.title a')

#URLs
attr_data2 = html_attrs(headline_data2) 
attr_data2

urlslist2 = unlist(attr_data2)
urlslist2 = urlslist2[grep("http", urlslist2)]
urlslist2

##start a data frame
NPRDF = matrix(NA, nrow = length(urlslist2), ncol = 3)
colnames(NPRDF) = c("Source", "Url", "Text")
NPRDF = as.data.frame(NPRDF)

##for loops
for (i in 1:length(urlslist2)){
  
  ##read in the URL
  webpage2 <- read_html(urlslist2[i])
  
  ##pull the specific nodes
  headline_data2 = html_nodes(webpage2,'#storytext > p') 
  
  ##pull the text
  text_data2 = html_text(headline_data2)
  
  ##save the data
  NPRDF$Source[i] = "NPR"
  NPRDF$Url[i] = urlslist2[i]
  NPRDF$Text[i] = paste(text_data2, collapse = "")
} ##end for loop

####Fox News####
url3 = 'http://www.foxnews.com/politics.html'
url3.1 = 'http://www.foxnews.com/category/politics/executive.html'
url3.2 = 'http://www.foxnews.com/category/politics/senate.htm.html'
url3.3 = 'http://www.foxnews.com/category/politics/house-representatives.html'
url3.4 = 'http://www.foxnews.com/category/politics/judiciary.html'
url3.5 = 'http://www.foxnews.com/category/politics/foreign-policy.html'
url3.6 = 'http://www.foxnews.com/category/politics/elections.html'
webpage3 = read_html(url3)
webpage3.1 = read_html(url3.1)
webpage3.2 = read_html(url3.2)
webpage3.3 = read_html(url3.3)
webpage3.4 = read_html(url3.4)
webpage3.5 = read_html(url3.5)
webpage3.6 = read_html(url3.6)

headline_data3 = html_nodes(webpage3, '.story- a , .article-list .title a')
headline_data3.1 = html_nodes(webpage3.1, '.story- a , .article-list .title a')
headline_data3.2 = html_nodes(webpage3.2, '.story- a , .article-list .title a')
headline_data3.3 = html_nodes(webpage3.3, '.story- a , .article-list .title a')
headline_data3.4 = html_nodes(webpage3.4, '.story- a , .article-list .title a')
headline_data3.5 = html_nodes(webpage3.5, '.story- a , .article-list .title a')
headline_data3.6 = html_nodes(webpage3.6, '.story- a , .article-list .title a')

#headline_data = html_text(headline_data)
head(headline_data3) 

attr_data3 = html_attrs(headline_data3) 
attr_data3.1 = html_attrs(headline_data3.1) 
attr_data3.2 = html_attrs(headline_data3.2) 
attr_data3.3 = html_attrs(headline_data3.3) 
attr_data3.4 = html_attrs(headline_data3.4) 
attr_data3.5 = html_attrs(headline_data3.5) 
attr_data3.6 = html_attrs(headline_data3.6) 

attr_data3

urlslist3 = c(unlist(attr_data3), unlist(attr_data3.1), 
              unlist(attr_data3.2), unlist(attr_data3.3), 
              unlist(attr_data3.4), unlist(attr_data3.5, attr_data3.6))
##find all that are href
urlslist3 = urlslist3[grep("http|.html", urlslist3)]
##fix the ones without the leading foxnews.com
urlslist3F = paste("http://www.foxnews.com", urlslist3[grep("^http", urlslist3, invert = T)], sep = "")
urlslist3N = urlslist3[grep("^http", urlslist3)]
urlslist3 = c(urlslist3N, urlslist3F)
urlslist3 = unique(urlslist3)
urlslist3

##start a data frame
FoxDF = matrix(NA, nrow = length(urlslist3), ncol = 3)
colnames(FoxDF) = c("Source", "Url", "Text")
FoxDF = as.data.frame(FoxDF)

##for loops
for (i in 1:length(urlslist3)){
  
  ##read in the URL
  webpage3 <- read_html(urlslist3[i])
  
  ##pull the specific nodes
  headline_data3 = html_nodes(webpage3,'p+ p , .fn-video+ p , .twitter+ p , .speakable') 
  
  ##pull the text
  text_data3 = html_text(headline_data3)
  
  ##save the data
  FoxDF$Source[i] = "Fox News"
  FoxDF$Url[i] = urlslist3[i]
  FoxDF$Text[i] = paste(text_data3, collapse = "")
} ##end for loop

####Breitbart####
url4 = 'http://www.breitbart.com/big-government/'
webpage4 = read_html(url4)
headline_data4 = html_nodes(webpage4, '.title a , #grid-block-0 span , #BBTrendUL a , #disqus-popularUL a , font')
#headline_data4 = html_text(headline_data4)
head(headline_data4) 

attr_data4 = html_attrs(headline_data4) 
attr_data4

urlslist4 = unlist(attr_data4)
##find all that are href
urlslist4 = urlslist4[grep("http|/big-government", urlslist4)]
##fix the ones without the leading bb
urlslist4F = paste("http://www.breitbart.com", urlslist4[grep("^http", urlslist4, invert = T)], sep = "")
urlslist4N = urlslist4[grep("^http", urlslist4)]
urlslist4 = c(urlslist4N, urlslist4F)
urlslist4 = unique(urlslist4)
urlslist4

##start a data frame
BreitbartDF = matrix(NA, nrow = length(urlslist4), ncol = 3)
colnames(BreitbartDF) = c("Source", "Url", "Text")
BreitbartDF = as.data.frame(BreitbartDF)

##for loops
for (i in 1:length(urlslist4)){
  
  ##read in the URL
  webpage4 <- read_html(urlslist4[i])
  
  ##pull the specific nodes
  headline_data4 = html_nodes(webpage4,'.entry-content p , h2') 
  
  ##pull the text
  text_data4 = html_text(headline_data4)
  
  ##save the data
  BreitbartDF$Source[i] = "Breitbart"
  BreitbartDF$Url[i] = urlslist4[i]
  BreitbartDF$Text[i] = paste(text_data4, collapse = "")
} ##end for loop

####NPR Archive####
url5 = 'https://www.npr.org/sections/politics/archive'
webpage5 = read_html(url5)
headline_data5 = html_nodes(webpage5,'.title a')
#headline_data = html_text(headline_data)
#head(headline_data)

#URLs
attr_data5 = html_attrs(headline_data5) 
attr_data5

urlslist5 = unlist(attr_data5)
urlslist5 = urlslist5[grep("http", urlslist5)]
urlslist5

##start a data frame
NPRArchiveDF = matrix(NA, nrow = length(urlslist5), ncol = 3)
colnames(NPRArchiveDF) = c("Source", "Url", "Text")
NPRArchiveDF = as.data.frame(NPRArchiveDF)

##for loops
for (i in 1:length(urlslist5)){
  
  ##read in the URL
  webpage5 <- read_html(urlslist5[i])
  
  ##pull the specific nodes
  headline_data5 = html_nodes(webpage5,'#storytext > p') 
  
  ##pull the text
  text_data5 = html_text(headline_data5)
  
  ##save the data
  NPRArchiveDF$Source[i] = "NPR"
  NPRArchiveDF$Url[i] = urlslist5[i]
  NPRArchiveDF$Text[i] = paste(text_data5, collapse = "")
} ##end for loop

####put together####
##set your working directory
setwd("~/OneDrive - Missouri State University/RESEARCH/2 projects/Will-Pilot")

##import the overalldata 
overalldata = read.csv("../exp1_data/overalldata.csv")

##combine with new data (add the other DF names in here...)
newdata = rbind(overalldata, NYtimesDF, NPRDF, FoxDF, BreitbartDF, NPRArchiveDF)

#temp NPR updates
#newdata = newdata[ , -4]
#newdata = rbind(newdata, NPRArchiveDF)

#change politics archive to NPR, so we can eliminate dupes
#newdata$Source[newdata$Source == "NPR Politics Archive"] = "NPR"
#newdata$Source = droplevels(newdata$Source)

##make the newdata unique in case of overlap across days
newdata = unique(newdata)

##write it back out
#write.csv(newdata, "overalldata.csv", row.names = F)

##number of articles
table(newdata$Source)

##number of words
library(ngram)
newdata$Text = as.character(newdata$Text)
for (i in 1:nrow(newdata)) {
  
  #newdata$writing[i] = preprocess(newdata$Text[i], case="lower", remove.punct=TRUE)
  newdata$wordcount[i] = string.summary(newdata$Text[i])$words
  
}

tapply(newdata$wordcount, newdata$Source, mean)
tapply(newdata$wordcount, newdata$Source, sum)

```

## Data analysis

Once data collection ended, the text was scanned using the *ngram* package in *R* [@Schmidt2017]. This package includes a word count function, which was used to remove articles that came through as blank text, as well as to eliminate text picked up from the Disqus commenting system used by certain websites. At this point, duplicate articles were discarded.

```{r data-cleanup, eval=FALSE, include=FALSE}
##pull in the data
master = read.csv("../exp1_data/overalldata.csv", stringsAsFactors = F)

##get rid of the blank stuff
library(ngram)

for (i in 1:nrow(master)) {
  master$wordcount[i] = wordcount(master$Text[i])
}

nozero = subset(master, wordcount > 0)
tapply(nozero$wordcount, nozero$Source, mean)
tapply(nozero$wordcount, nozero$Source, sum)

##take out the disqus ones
nodis = nozero[ -c(grep("disqus", nozero$Url)), ]

##figure out the duplicates
nodis$URLS = duplicated(nodis$Url)

##all data
final = subset(nodis, URLS == FALSE)

tapply(final$wordcount, final$Source, mean)
tapply(final$wordcount, final$Source, sum)

write.csv(final, "finaldata.csv")

```

The article text was processed using the *tm* and *ngram* packages in *R* in order to render the text in lowercase, remove punctuation, and fix spacing issues [@Feinerer2017]. The individual words were then reduced to their stems (i.e., *abused* was stemmed to *abus*). The same procedure was applied to the MFD words and the words in the @Warriner2013 dataset. Using the @Warriner2013 dictionary, the words making up each of the five foundations in the MFD were matched to their respective valence value.   

Concurrent research by @Jordan2019 is assessing the validity of both the Moral Foundations Questionnaire and the Moral Foundations Dictionary through a multi-trait multi-method analysis of the two instruments using multiple samples. The instruments and foundation areas are being analyzed against one another, in order to test reliability, as well as against the Congressional Record in order to test predictive validity for political orientation. The researchers were able to identify a number of potential new words that, if added to the MFD, could comprise a dictionary with greater validity, and less likelihood of zero percent texts, as this often occurs with the current MFD. Those results have informed this analysis, and their updated findings may change the underlying dictionary used in this analysis (albeit, we do not expect any changes in the results presented below). 

The source article words were compiled into a dataset where they were matched up with their counterparts in the MFD along with their valence and a percentage of their occurrence. Therefore, for each article, the percentage of the number of *harm/care* words occurring in the articles were calculated, and this process was repeated for each of the foundations. Words' percent occurrence were multiplied by their *z*-scored valence. Valences were *z*-scored in order to eliminate any ambiguity regarding the direction of the valence. Positive values indicate positive valence, and negative values indicate negative valence. Words were categorized in accordance to their MFD affiliation, creating a weighted sum for each moral foundation.

```{r stem_text, eval=TRUE, include=FALSE}

final = read.csv("../exp1_data/finaldata.csv", stringsAsFactors = F)
final = na.omit(final) #one weird NA line

##load libraries
library(tm)
library(ngram)

#process the Text column (write a loop) - save processed text as a separate column
for(i in 1:nrow(final)) {
  
  final$edited[i] = preprocess(final$Text[i], #one value at a time
           case = "lower", 
           remove.punct = TRUE,
           remove.numbers = FALSE, 
           fix.spacing = TRUE)

##stem the words - do this second in the loop
  final$stemmed[i] = stemDocument(final$edited[i], language = "english")
} 

```

```{r stem_warriner, include = FALSE}
##open the Warriner data for later
warriner = read.csv("../exp1_data/BRM-emot-submit.csv", stringsAsFactors = F) #want v.mean.sum
warriner$V.Mean.Sum = scale(warriner$V.Mean.Sum)

for(i in 1:nrow(warriner)) {
  warriner$Word2[i] = stemDocument(warriner$Word[i], language = "english")
} 
```

```{r create_datasets_harm, echo = F, include=FALSE}
#read in the harm total words from the mtmm project
harm_all = read.csv("../exp1_data/harm_words_total.csv", stringsAsFactors = F)

harm_final_data = data.frame("URL" = NA, "Source" = NA, "Var1" = NA, "Freq" = NA, "wordcount" = NA)

for (i in 1:nrow(final)) {
##first make a table of a response, unlisting everything
temp = as.data.frame(table(unlist(strsplit(final$stemmed[i], " "))))

#only pull the words in the dataset for each foundation
temp2 = temp[temp$Var1 %in% unique(harm_all$words[harm_all$words != "" & harm_all$words != "NA" & !is.na(harm_all$words)]), ]

#control here for times when no words
if (nrow(temp2) == 0){
  temp2 = data.frame("Var1" = 0, "Freq" = 0)
}

#add to the dataset
temp2$URL = final$Url[i]
temp2$Source = final$Source[i]
temp2$wordcount = final$wordcount[i]

#bind with original
harm_final_data = rbind(harm_final_data, temp2)
}

#pull in the data from Warriner 
harm_final_data$valence = vlookup(harm_final_data$Var1, #index in original data
                                  warriner, #where's the stuff you want to put in
                                  "V.Mean.Sum", #what column is the stuff you want
                                  "Word2") #what's the matching index

#figure out stupid words
#table(harm_final_data$Var1[is.na(harm_final_data$valence) & harm_final_data$Freq > 0])
#fought, safer, spurn
harm_final_data$valence[harm_final_data$Var1 == "fought"] = warriner$V.Mean.Sum[warriner$Word2 == "fight"]
harm_final_data$valence[harm_final_data$Var1 == "safer"] = warriner$V.Mean.Sum[warriner$Word2 == "safe"]
#no spurn avaliable
```

```{r create_datasets_fair, include=FALSE}
fair_all = read.csv("../exp1_data/fair_words_total.csv", stringsAsFactors = F)

fair_final_data = data.frame("URL" = NA, "Source" = NA, "Var1" = NA, "Freq" = NA, "wordcount" = NA)

for (i in 1:nrow(final)) {
##first make a table of a response, unlisting everything
temp = as.data.frame(table(unlist(strsplit(final$stemmed[i], " "))))

#only pull the words in the dataset for each foundation
temp2 = temp[temp$Var1 %in% unique(fair_all$words[fair_all$words != "" & fair_all$words != "NA" & !is.na(fair_all$words)]), ]

#control here for times when no words
if (nrow(temp2) == 0){
  temp2 = data.frame("Var1" = 0, "Freq" = 0)
}

#add to the dataset
temp2$URL = final$Url[i]
temp2$Source = final$Source[i]
temp2$wordcount = final$wordcount[i]

#bind with original
fair_final_data = rbind(fair_final_data, temp2)
}

fair_final_data$valence = vlookup(fair_final_data$Var1, #index in original data
                                  warriner, #where's the stuff you want to put in
                                  "V.Mean.Sum", #what column is the stuff you want
                                  "Word2") #what's the matching index

#figure out stupid words
table(fair_final_data$Var1[is.na(fair_final_data$valence) & fair_final_data$Freq > 0])
#just plagiar safer
fair_final_data$valence[fair_final_data$Var1 == "safer"] = warriner$V.Mean.Sum[warriner$Word2 == "safe"]
#nothing for just or plagiarism
```

```{r create_datasets_ingroup, include=FALSE}
ingroup_all = read.csv("../exp1_data/ingroup_words_total.csv", stringsAsFactors = F)

ingroup_final_data = data.frame("URL" = NA, "Source" = NA, "Var1" = NA, "Freq" = NA, "wordcount" = NA)

for (i in 1:nrow(final)) {
##first make a table of a response, unlisting everything
temp = as.data.frame(table(unlist(strsplit(final$stemmed[i], " "))))

#only pull the words in the dataset for each foundation
temp2 = temp[temp$Var1 %in% unique(ingroup_all$words[ingroup_all$words != "" & ingroup_all$words != "NA" & !is.na(ingroup_all$words)]), ]

#control here for times when no words
if (nrow(temp2) == 0){
  temp2 = data.frame("Var1" = 0, "Freq" = 0)
}

#add to the dataset
temp2$URL = final$Url[i]
temp2$Source = final$Source[i]
temp2$wordcount = final$wordcount[i]

#bind with original
ingroup_final_data = rbind(ingroup_final_data, temp2)
}

ingroup_final_data$valence = vlookup(ingroup_final_data$Var1, #index in original data
                                  warriner, #where's the stuff you want to put in
                                  "V.Mean.Sum", #what column is the stuff you want
                                  "Word2") #what's the matching index

#figure out stupid words
table(ingroup_final_data$Var1[is.na(ingroup_final_data$valence) & ingroup_final_data$Freq > 0])
#backstabb  benedict      best      fals   indvidu      juda    knight      true
ingroup_final_data$valence[ingroup_final_data$Var1 == "fals"] = warriner$V.Mean.Sum[warriner$Word2 == "FALSE"]
ingroup_final_data$valence[ingroup_final_data$Var1 == "indvidu"] = mean(warriner$V.Mean.Sum[warriner$Word2 == "individu"])
ingroup_final_data$valence[ingroup_final_data$Var1 == "true"] = mean(warriner$V.Mean.Sum[warriner$Word2 == "TRUE"])
```

```{r create_datasets_authority, include=FALSE}
authority_all = read.csv("../exp1_data/authority_words_total.csv", stringsAsFactors = F)

authority_final_data = data.frame("URL" = NA, "Source" = NA, "Var1" = NA, "Freq" = NA, "wordcount" = NA)

for (i in 1:nrow(final)) {
##first make a table of a response, unlisting everything
temp = as.data.frame(table(unlist(strsplit(final$stemmed[i], " "))))

#only pull the words in the dataset for each foundation
temp2 = temp[temp$Var1 %in% unique(authority_all$words[authority_all$words != "" & authority_all$words != "NA" & !is.na(authority_all$words)]), ]

#control here for times when no words
if (nrow(temp2) == 0){
  temp2 = data.frame("Var1" = 0, "Freq" = 0)
}

#add to the dataset
temp2$URL = final$Url[i]
temp2$Source = final$Source[i]
temp2$wordcount = final$wordcount[i]

#bind with original
authority_final_data = rbind(authority_final_data, temp2)
}

authority_final_data$valence = vlookup(authority_final_data$Var1, #index in original data
                                  warriner, #where's the stuff you want to put in
                                  "V.Mean.Sum", #what column is the stuff you want
                                  "Word2") #what's the matching index

#figure out stupid words
table(authority_final_data$Var1[is.na(authority_final_data$valence) & authority_final_data$Freq > 0])
#  abov behind   gase higher  older  taken 
authority_final_data$valence[authority_final_data$Var1 == "gase"] = warriner$V.Mean.Sum[warriner$Word2 == "gas"]
authority_final_data$valence[authority_final_data$Var1 == "higher"] = mean(warriner$V.Mean.Sum[warriner$Word2 == "high"])
authority_final_data$valence[authority_final_data$Var1 == "older"] = mean(warriner$V.Mean.Sum[warriner$Word2 == "old"])
authority_final_data$valence[authority_final_data$Var1 == "taken"] = mean(warriner$V.Mean.Sum[warriner$Word2 == "take"])
```

```{r create_datasets_purity, include=FALSE}
purity_all = read.csv("../exp1_data/purity_words_total.csv", stringsAsFactors = F)

purity_final_data = data.frame("URL" = NA, "Source" = NA, "Var1" = NA, "Freq" = NA, "wordcount" = NA)

for (i in 1:nrow(final)) {
##first make a table of a response, unlisting everything
temp = as.data.frame(table(unlist(strsplit(final$stemmed[i], " "))))

#only pull the words in the dataset for each foundation
temp2 = temp[temp$Var1 %in% unique(purity_all$words[purity_all$words != "" & purity_all$words != "NA" & !is.na(purity_all$words)]), ]

#control here for times when no words
if (nrow(temp2) == 0){
  temp2 = data.frame("Var1" = 0, "Freq" = 0)
}

#add to the dataset
temp2$URL = final$Url[i]
temp2$Source = final$Source[i]
temp2$wordcount = final$wordcount[i]

#bind with original
purity_final_data = rbind(purity_final_data, temp2)
}

purity_final_data$valence = vlookup(purity_final_data$Var1, #index in original data
                                  warriner, #where's the stuff you want to put in
                                  "V.Mean.Sum", #what column is the stuff you want
                                  "Word2") #what's the matching index

#figure out stupid words
table(purity_final_data$Var1[is.na(purity_final_data$valence) & purity_final_data$Freq > 0])
#befor christian     jesus      mari      none    sacred    someon      true     women 
purity_final_data$valence[purity_final_data$Var1 == "sacred"] = warriner$V.Mean.Sum[warriner$Word2 == "sacr"]
purity_final_data$valence[purity_final_data$Var1 == "true"] = mean(warriner$V.Mean.Sum[warriner$Word2 == "TRUE"])
purity_final_data$valence[purity_final_data$Var1 == "women"] = mean(warriner$V.Mean.Sum[warriner$Word2 == "woman"])
```

```{r descriptives, echo = F, include = FALSE}
#summarize the dataset so that you have the total and percent for each foundation 
# summary(harm_final_data)
# summary(fair_final_data)
# summary(ingroup_final_data)
# summary(authority_final_data)
# summary(purity_final_data)
# #report descriptive statistics of the foundations
# mean(harm_final_data$valence)
# sd(harm_final_data$valence)
# mean(fair_final_data$valence)
# sd(fair_final_data$valence)
# mean(ingroup_final_data$valence)
# sd(ingroup_final_data$valence)
# mean(authority_final_data$valence)
# sd(authority_final_data$valence)
# mean(purity_final_data$valence)
# sd(purity_final_data$valence)
#note that a big problem is the valence of the words 
```

```{r analysis_harm, include = FALSE}
#final_data clean up
#multiply by frequency and create percentage
harm_final_data$weight = harm_final_data$Freq * harm_final_data$valence / harm_final_data$wordcount * 100

#flatten down to sum by article 
harm_final_temp = aggregate(harm_final_data$weight, 
          by = list(harm_final_data$URL),
          FUN = sum, na.rm = T)
colnames(harm_final_temp) = c("URL", "MFDweight")

harm_final_temp$lean[grepl("nytimes.com", harm_final_temp$URL) |
                       grepl("npr.org", harm_final_temp$URL)] = "Liberal"
harm_final_temp$lean[grepl("breitbart.com", harm_final_temp$URL) |
                       grepl("foxnews.com", harm_final_temp$URL)] = "Conservative"

harm_final_temp$Source[grepl("nytimes.com", harm_final_temp$URL)] = "NY Times"
harm_final_temp$Source[grepl("npr.org", harm_final_temp$URL)] = "NPR"
harm_final_temp$Source[grepl("foxnews.com", harm_final_temp$URL)] = "Fox"
harm_final_temp$Source[grepl("breitbart.com", harm_final_temp$URL)] = "Breitbart"

harm_model = lme(MFDweight ~ lean, 
                 data = harm_final_temp, 
                 method = "ML",
                 na.action = "na.omit",
                 random = list(~1|Source))
summary(harm_model)
```

```{r analysis_fair, include = FALSE}
#final_data clean up
#multiply by frequency and create percentage
fair_final_data$weight = fair_final_data$Freq * fair_final_data$valence / fair_final_data$wordcount * 100

#flatten down to sum by article 
fair_final_temp = aggregate(fair_final_data$weight, 
          by = list(fair_final_data$URL),
          FUN = sum, na.rm = T)
colnames(fair_final_temp) = c("URL", "MFDweight")

fair_final_temp$lean[grepl("nytimes.com", fair_final_temp$URL) |
                       grepl("npr.org", fair_final_temp$URL)] = "Liberal"
fair_final_temp$lean[grepl("breitbart.com", fair_final_temp$URL) |
                       grepl("foxnews.com", fair_final_temp$URL)] = "Conservative"

fair_final_temp$Source[grepl("nytimes.com", fair_final_temp$URL)] = "NY Times"
fair_final_temp$Source[grepl("npr.org", fair_final_temp$URL)] = "NPR"
fair_final_temp$Source[grepl("foxnews.com", fair_final_temp$URL)] = "Fox"
fair_final_temp$Source[grepl("breitbart.com", fair_final_temp$URL)] = "Breitbart"

fair_model = lme(MFDweight ~ lean, 
                 data = fair_final_temp, 
                 method = "ML",
                 na.action = "na.omit",
                 random = list(~1|Source))
summary(fair_model)
```

```{r analysis_ingroup, include = FALSE}
#final_data clean up
#multiply by frequency and create percentage
ingroup_final_data$weight = ingroup_final_data$Freq * ingroup_final_data$valence / ingroup_final_data$wordcount * 100

#flatten down to sum by article 
ingroup_final_temp = aggregate(ingroup_final_data$weight, 
          by = list(ingroup_final_data$URL),
          FUN = sum, na.rm = T)
colnames(ingroup_final_temp) = c("URL", "MFDweight")

ingroup_final_temp$lean[grepl("nytimes.com", ingroup_final_temp$URL) |
                       grepl("npr.org", ingroup_final_temp$URL)] = "Liberal"
ingroup_final_temp$lean[grepl("breitbart.com", ingroup_final_temp$URL) |
                       grepl("foxnews.com", ingroup_final_temp$URL)] = "Conservative"

ingroup_final_temp$Source[grepl("nytimes.com", ingroup_final_temp$URL)] = "NY Times"
ingroup_final_temp$Source[grepl("npr.org", ingroup_final_temp$URL)] = "NPR"
ingroup_final_temp$Source[grepl("foxnews.com", ingroup_final_temp$URL)] = "Fox"
ingroup_final_temp$Source[grepl("breitbart.com", ingroup_final_temp$URL)] = "Breitbart"

ingroup_model = lme(MFDweight ~ lean, 
                 data = ingroup_final_temp, 
                 method = "ML",
                 na.action = "na.omit",
                 random = list(~1|Source))
summary(ingroup_model)
```

```{r analysis_authority, include = FALSE}
#final_data clean up
#multiply by frequency and create percentage
authority_final_data$weight = authority_final_data$Freq * authority_final_data$valence / authority_final_data$wordcount * 100

#flatten down to sum by article 
authority_final_temp = aggregate(authority_final_data$weight, 
          by = list(authority_final_data$URL),
          FUN = sum, na.rm = T)
colnames(authority_final_temp) = c("URL", "MFDweight")

authority_final_temp$lean[grepl("nytimes.com", authority_final_temp$URL) |
                       grepl("npr.org", authority_final_temp$URL)] = "Liberal"
authority_final_temp$lean[grepl("breitbart.com", authority_final_temp$URL) |
                       grepl("foxnews.com", authority_final_temp$URL)] = "Conservative"

authority_final_temp$Source[grepl("nytimes.com", authority_final_temp$URL)] = "NY Times"
authority_final_temp$Source[grepl("npr.org", authority_final_temp$URL)] = "NPR"
authority_final_temp$Source[grepl("foxnews.com", authority_final_temp$URL)] = "Fox"
authority_final_temp$Source[grepl("breitbart.com", authority_final_temp$URL)] = "Breitbart"

authority_model = lme(MFDweight ~ lean, 
                 data = authority_final_temp, 
                 method = "ML",
                 na.action = "na.omit",
                 random = list(~1|Source))
summary(authority_model)
```

```{r analysis_purity, include = FALSE}
#final_data clean up
#multiply by frequency and create percentage
purity_final_data$weight = purity_final_data$Freq * purity_final_data$valence / purity_final_data$wordcount * 100

#flatten down to sum by article 
purity_final_temp = aggregate(purity_final_data$weight, 
          by = list(purity_final_data$URL),
          FUN = sum, na.rm = T)
colnames(purity_final_temp) = c("URL", "MFDweight")

purity_final_temp$lean[grepl("nytimes.com", purity_final_temp$URL) |
                       grepl("npr.org", purity_final_temp$URL)] = "Liberal"
purity_final_temp$lean[grepl("breitbart.com", purity_final_temp$URL) |
                       grepl("foxnews.com", purity_final_temp$URL)] = "Conservative"

purity_final_temp$Source[grepl("nytimes.com", purity_final_temp$URL)] = "NY Times"
purity_final_temp$Source[grepl("npr.org", purity_final_temp$URL)] = "NPR"
purity_final_temp$Source[grepl("foxnews.com", purity_final_temp$URL)] = "Fox"
purity_final_temp$Source[grepl("breitbart.com", purity_final_temp$URL)] = "Breitbart"

purity_model = lme(MFDweight ~ lean, 
                 data = purity_final_temp, 
                 method = "ML",
                 na.action = "na.omit",
                 random = list(~1|Source))
summary(purity_model)
```

# Results

## Descriptive Statistics

```{r exp1-source-descriptives, results = 'asis', echo = FALSE, message = F}

final$uniquewords = NA
final$valenceavg = NA

for (i in 1:nrow(final)){
  
  #add unique
  final$uniquewords[i] = length(unique(unlist(strsplit(final$stemmed[i], " "))))
  
  #add valence
  tempwords = as.data.frame(unlist(strsplit(final$stemmed[i], " ")))
  colnames(tempwords) = "theword"
  tempwords$tempval = vlookup(tempwords$theword, #index in original data
                    warriner, #where's the stuff you want to put in
                    "V.Mean.Sum", #what column is the stuff you want
                    "Word2") #what's the matching index
  
  final$valenceavg[i] = mean(tempwords$tempval, na.rm = T)

}


# tapply(final$valenceavg, final$Source, mean)
# tapply(final$valenceavg, final$Source, sd)
# tapply(final$wordcount, final$Source, mean)
# tapply(final$wordcount, final$Source, sd)
# tapply(final$uniquewords, final$Source, mean)
# tapply(final$uniquewords, final$Source, sd)
# tapply(final$Source, final$Source, length)
# tapply(final$wordcount, final$Source, sum)

library(quanteda)

final$FK = NA

for (i in 1:nrow(final)){
  final$FK[i] = unlist(textstat_readability(final$Text[i], 
                     measure = "Flesch.Kincaid")$Flesch.Kincaid)
}

# tapply(final$FK, final$Source, mean)
# tapply(final$FK, final$Source, sd)

####build the table here####
tableprint = matrix(NA, nrow=4, ncol=11)
colnames(tableprint) = c("Source", "Mean_Valence", "SD_Valence", "Number_of_Articles", "Total_Words", "Mean_Tokens", "SD_Tokens", "Mean_Types", "SD_Types", "Mean_Readability", "SD_Readability")

#tableprint[ , 1] = unique(final$Source)
tableprint[ , 1] = c("Breitbart", "Fox News", "NPR", "New York Times")
tableprint[ , 2] = tapply(final$valenceavg, final$Source, mean)
tableprint[ , 3] = tapply(final$valenceavg, final$Source, sd)
tableprint[ , 4] = tapply(final$valenceavg, final$Source, length)
tableprint[ , 5] = tapply(final$wordcount, final$Source, sum)
tableprint[ , 6] = tapply(final$wordcount, final$Source, mean)
tableprint[ , 7] = tapply(final$wordcount, final$Source, sd)
tableprint[ , 8] = tapply(final$uniquewords, final$Source, mean)
tableprint[ , 9] = tapply(final$uniquewords, final$Source, sd)
tableprint[ , 10] = tapply(final$FK, final$Source, mean)
tableprint[ , 11] = tapply(final$FK, final$Source, sd)

tableprint[ , c(2,3,6,7,8,9,10,11)] = printnum(as.numeric(tableprint[ , c(2,3,6,7,8,9,10,11)]))

tableprint = tableprint[c(3, 4, 1, 2) , ]

#change this to match
apa_table(as.data.frame(tableprint),
          note = "Readability statistics were calculated using the Flesch-Kincaid Grade Level readability formula. V = Valence, T = Tokens or total words, Ty = Types or unique words, FK = Flesch-Kincaid",
          caption = "Experiment 1 - Descriptive Statistics by Source", 
          col.names = c("Source", "$M_V$", "$SD_V$", "$N_{Article}$", "$N_{Words}$", "$M_T$", "$SD_T$", "$M_{Ty}$", "$SD_{Ty}$", "$M_{FK}$", "$SD_{FK}$"),
          format = "latex",
          escape = F,
          align = c("l", rep("c", 6))
          )
```

The researchers calculated descriptive statistics for each news source in order to understand any and all fundamental linguistic differences in the sources' use of English. Statistics calculated included average *z*-scored valence of the unique words per article, number of articles per source, total number of words per source, average number of tokens (words) per article in each source, average number of types (unique words) per article in each source, and mean readability level per source. Readability statistics were calculated using the Flesch-Kincaid Grade Level Readability formula [@Kincaid1975]. Readability is calculated using a formula where the total number of syllables, words, and sentences in a given passage are determinants of its difficulty. The obtained value is intended to match up with the grade level at which one should be able to comfortably read the passage [@Kincaid1975]. For example, a text with a readability score of 11 should be easily read by a high school junior.

As seen in Table \@ref(tab:exp1-source-descriptives), the sources are similar in some aspects yet different in others. Valence appears to be slightly positive across all sources. The large standard deviations seem to indicate little to no presence of a difference in valence across sources. 
*The New York Times* published the greatest number of articles as well as total words. *Breitbart* featured the lowest number of articles, and *NPR* the lowest number of total words from all articles. Per individual article, however, *Breitbart* appears to feature the highest average number of words as well as unique words. Once again the standard deviations call into question any apparent differences between sources. Finally, *Fox News* articles had the lowest reading grade level on average, while *The New York Times* had the highest. This result might be attributable to the greater number of tokens in the average *New York Times* article compared to *Fox News*. The standard deviations for readability suggest the presence of a diverse array of articles for each source, ranging from low to high reading level. Large standard deviations suggest the sources feature a lot of overlap between them in their representation of scores. 

## Inferential Statistics

To analyze if news sources adhered to differences in word use based on their target audience, we utilized a multilevel model (MLM) to analyze the data. MLM is a regression technique that allows one to control for the repeated measurement and nested structured of the data, which creates correlated error [@Gelman2006]. Using the *nlme* library in *R* [@Pinheiro2017], each foundation's weighted percentage was predicated here by the political lean of the news source, using the individual news sources as a random intercept to control for the structure of the data.

The multilevel model did not indicate the presence of any significant or practical effect of political lean for any of the five moral foundations. The strongest effect size was observed for the *authority/respect* foundation, but the effect was in the opposite direction from what was originally hypothesized - liberal sources tended to use more *authority/respect* words than did conservative sources. Descriptive and test statistics, *p*-values and effect sizes (Cohen's *d*) can be found in Table \@ref(tab:exp1-table). To interpret the weighted scores, one can examine the mean and standard deviations for each. A zero score for the mean, with a non-zero standard deviation, would indicate a perfect balance of positive and negative words in each category, likely representing a neutral tone when all words are considered. Negative percentages would indicate more representation of the negative words in the MFD area, while positive percentages indicate an endorsement of the positive words in a MFD. Therefore, we suggest using the sign of the mean score to determine the directionality of the endorsement for the MFD (positive, neutral, negative), and the standard deviation to ensure that a zero score is not zero endorsement (i.e., a SD of zero indicates no words were used). Based on the weighted percent values for the five foundations, the researchers observed that MFD words seem to make up a small portion of the article text. Furthermore, the observed percentages and means appear to indicate a generally positive endorsement of all five foundations across both liberal and conservative sources.

```{r exp1-table, results = 'asis', echo = F}
#MFD, mean, sd, mean, sd, t, p, d
tableprint = matrix(NA, ncol = 8, nrow = 5)
colnames(tableprint) = c("Foundation", "M_C", "SD_C", "M_L", "SD_L", "t", "p", "d")

#calculate d
#harm
harm_d = d.ind.t(m1 = mean(harm_final_temp$MFDweight[harm_final_temp$lean == "Conservative"], na.rm = T), 
                 m2 = mean(harm_final_temp$MFDweight[harm_final_temp$lean == "Liberal"], na.rm = T),
                 sd1 = sd(harm_final_temp$MFDweight[harm_final_temp$lean == "Conservative"], na.rm = T),
                 sd2 = sd(harm_final_temp$MFDweight[harm_final_temp$lean == "Liberal"], na.rm = T),
                 n1 = length(harm_final_temp$MFDweight[harm_final_temp$lean == "Conservative"]),
                 n2 = length(harm_final_temp$MFDweight[harm_final_temp$lean == "Liberal"]),
                 a = .05)

#fair
fair_d = d.ind.t(m1 = mean(fair_final_temp$MFDweight[fair_final_temp$lean == "Conservative"], na.rm = T), 
                 m2 = mean(fair_final_temp$MFDweight[fair_final_temp$lean == "Liberal"], na.rm = T),
                 sd1 = sd(fair_final_temp$MFDweight[fair_final_temp$lean == "Conservative"], na.rm = T),
                 sd2 = sd(fair_final_temp$MFDweight[fair_final_temp$lean == "Liberal"], na.rm = T),
                 n1 = length(fair_final_temp$MFDweight[fair_final_temp$lean == "Conservative"]),
                 n2 = length(fair_final_temp$MFDweight[fair_final_temp$lean == "Liberal"]),
                 a = .05)

#ingroup
ingroup_d = d.ind.t(m1 = mean(ingroup_final_temp$MFDweight[ingroup_final_temp$lean == "Conservative"], na.rm = T), 
                 m2 = mean(ingroup_final_temp$MFDweight[ingroup_final_temp$lean == "Liberal"], na.rm = T),
                 sd1 = sd(ingroup_final_temp$MFDweight[ingroup_final_temp$lean == "Conservative"], na.rm = T),
                 sd2 = sd(ingroup_final_temp$MFDweight[ingroup_final_temp$lean == "Liberal"], na.rm = T),
                 n1 = length(ingroup_final_temp$MFDweight[ingroup_final_temp$lean == "Conservative"]),
                 n2 = length(ingroup_final_temp$MFDweight[ingroup_final_temp$lean == "Liberal"]),
                 a = .05)

#authority
authority_d = d.ind.t(m1 = mean(authority_final_temp$MFDweight[authority_final_temp$lean == "Conservative"], na.rm = T), 
                 m2 = mean(authority_final_temp$MFDweight[authority_final_temp$lean == "Liberal"], na.rm = T),
                 sd1 = sd(authority_final_temp$MFDweight[authority_final_temp$lean == "Conservative"], na.rm = T),
                 sd2 = sd(authority_final_temp$MFDweight[authority_final_temp$lean == "Liberal"], na.rm = T),
                 n1 = length(authority_final_temp$MFDweight[authority_final_temp$lean == "Conservative"]),
                 n2 = length(authority_final_temp$MFDweight[authority_final_temp$lean == "Liberal"]),
                 a = .05)

#purity
purity_d = d.ind.t(m1 = mean(purity_final_temp$MFDweight[purity_final_temp$lean == "Conservative"], na.rm = T), 
                 m2 = mean(purity_final_temp$MFDweight[purity_final_temp$lean == "Liberal"], na.rm = T),
                 sd1 = sd(purity_final_temp$MFDweight[purity_final_temp$lean == "Conservative"], na.rm = T),
                 sd2 = sd(purity_final_temp$MFDweight[purity_final_temp$lean == "Liberal"], na.rm = T),
                 n1 = length(purity_final_temp$MFDweight[purity_final_temp$lean == "Conservative"]),
                 n2 = length(purity_final_temp$MFDweight[purity_final_temp$lean == "Liberal"]),
                 a = .05)

#put in the numbers
tableprint[1, ] = c("Harm/Care", harm_d$M1, harm_d$sd1, harm_d$M2, harm_d$sd1, 
                    summary(harm_model)$tTable[2,4], summary(harm_model)$tTable[2,5],
                    harm_d$d)
tableprint[2, ] = c("Fairness/Reciprocity", fair_d$M1, fair_d$sd1, fair_d$M2, fair_d$sd1, 
                    summary(fair_model)$tTable[2,4], summary(fair_model)$tTable[2,5],
                    fair_d$d)
tableprint[3, ] = c("Ingroup/Loyalty", ingroup_d$M1, ingroup_d$sd1, ingroup_d$M2, ingroup_d$sd1, 
                    summary(ingroup_model)$tTable[2,4], summary(ingroup_model)$tTable[2,5],
                    ingroup_d$d)
tableprint[4, ] = c("Authority/Respect", authority_d$M1, authority_d$sd1, authority_d$M2, authority_d$sd1, 
                    summary(authority_model)$tTable[2,4], summary(authority_model)$tTable[2,5],
                    authority_d$d)
tableprint[5, ] = c("Purity/Sanctity", purity_d$M1, purity_d$sd1, purity_d$M2, purity_d$sd1, 
                    summary(purity_model)$tTable[2,4], summary(purity_model)$tTable[2,5],
                    purity_d$d)

tableprint[ , c(2:6, 8)] = printnum(as.numeric(tableprint[ , c(2:6, 8)]))
tableprint[ , 7] = printp(as.numeric(tableprint[ , 7]))

apa_table(tableprint,
          note = "For mean and standard deviation values, 'C' and 'L' refer to 'conservative' and 'liberal,' respectively",
          caption = "Experiment 1 Results - Multilevel Model", 
          col.names = c("Foundation", "$M_C$", "$SD_C$", "$M_L$", "$SD_L$", "$t$", "$p$", "$d$"),
          format = "latex",
          escape = F,
          align = c("l", rep("c", 7))
          )
```

```{r conference-pic, eval = F, include = F}
library(ggplot2)
library(reshape)

cleanup = theme(panel.grid.major = element_blank(), 
                panel.grid.minor = element_blank(), 
                panel.background = element_blank(), 
                axis.line.x = element_line(color = "black"),
                axis.line.y = element_line(color = "black"),
                legend.key = element_rect(fill = "white"),
                text = element_text(size = 10))

graphdata = as.data.frame(tableprint)
graphlong = melt(graphdata[ , c(1,2,4)], 
                 id = "Foundation")
graphlong2 = melt(graphdata[ , c(1, 3,5)],
                  id = "Foundation",
                  measured = c("SD_C", "SD_L"))
graphlong$sd = graphlong2$value
graphlong$lower = as.numeric(as.character(graphlong$value)) - 2*as.numeric(as.character(graphlong$sd))
graphlong$upper = as.numeric(as.character(graphlong$value)) + 2*as.numeric(as.character(graphlong$sd))

graphlong$value = as.numeric(as.character(graphlong$value))

graph_data = ggplot(graphlong, aes(Foundation, value, color = variable)) + 
  geom_point(position = position_dodge(width = 0.90)) +
  geom_errorbar(aes(ymin = lower, ymax = upper), 
                position = position_dodge(width = 0.90),
                width = .2) + 
  cleanup +
  ylab("Mean Weighted Valence") + 
  scale_color_manual(values = c("red", "blue"),
                       labels = c("Conservative", "Liberal"),
                       name = "Party") +
  theme(axis.text.x = element_text(angle = 60, hjust = 1)) + 
  geom_hline(yintercept = 0, linetype = 1, color = "black")
  
tiff(filename = "exp1_graph.tiff", res = 300, width = 4, 
     height = 3, units = 'in', compression = "lzw")
plot(graph_data)
dev.off()

```

# Discussion 

The results obtained in Experiment 1 fail to support the differences predicted by Moral Foundations Theory. First, differences between liberal and conservative news sources failed to reach statistically significance for any foundations. Second, looking purely at the direction of the differences, the results run completely contrary to the differences expected by MFT. Conservative sources scored higher on concern for harm and fairness while liberal sources scored higher on concern for loyalty, authority, and purity. Overall, the results lend strong support to the case against the Moral Foundations Dictionary and weakens the case generally for Moral Foundations Theory. 

However,one possible limitation of the current data is the general-ness of the corpus. The selection of the broad and amorphous topic of "political news" may have led to the scraping of large numbers of articles with little to no moral-centric content. To address this limitation, two changes that were subsequently employed in Experiment 2. First, we elected to include more news sources for web scraping and analysis in addition to the four used in Experiment 1. Second, we chose to focus  data collection efforts on two heavily moralized events in the Trump administration: (1) the nomination and confirmation of Justice Brett Kavanaugh to the U.S. Supreme Court and the government shutdown in December 2018 over disagreements about funding a U.S.-Mexico border wall. Hence, Experiment 2 is the best test of the Moral Foundations theory and dictionary in the context of partisan news.

# Experiment 2

# Method

In contrast to Experiment 1, the researchers approached Experiment 2 with the intention to confirm the method employed was valid for the analysis of the scraped text as well as for any inferences drawn from the analyses. For Experiment 2, the researchers hypothesized that news sources perceived as liberal will exhibit positive endorsements of the individualizing moral foundations (*harm/care* and *fairness/reciprocity*) in their articles reporting on both the Kavanaugh confirmation hearing as well as the 2018-2019 government shutdown. News sources perceived as conservative are hypothesized to positively endorse all five foundations equally in their coverage of the Kavanaugh hearing and the government shutdown. The researchers tested the hypothesis by analyzing the content scraped from news sources' web pages spanning the two weeks before (September 13, 2018) and two weeks after (October 11, 2018) Kavanaugh's confirmation hearing, owing to its prominence in the news. Likewise, the researchers analyzed content spanning two weeks before the start of the government shutdown (December 8, 2018) to two weeks following the end of the shutdown (February 8, 2019). The content will be analyzed for valence and moral alignment under Moral Foundations Theory. Once again, no human participants were needed for this study, so no Institutional Review Board approval was necessary.

```{r NYTimes_kav, eval = F, include = F}

library(rvest)
library(RSelenium)

rD <- rsDriver(browser=c("chrome"), chromever="73.0.3683.68")
remDr <- rD[["client"]]

#page for NYTIMES
url <- 'https://www.nytimes.com/search?endDate=20181011&query=kavanaugh&sort=best&startDate=20180913'

#navigate to it
remDr$navigate(url)

#click the button 
#don't change class name
#right click --> inspect element 
webElem <- remDr$findElement(using = 'class name',"css-1t62hi8")

#with 752 results at 10 per page we need
#752 - 10 original = 742 / 10 per click = 75 clicks

for (i in 1:75) { #change this to 75 later
  webElem$clickElement()
  Sys.sleep(runif(1, 1, 5))
}

webpage <-remDr$getPageSource()

#Reading the HTML code from the website - headlines
webpage2 <- read_html(unlist(webpage))

headline_data = html_nodes(webpage2,'a')

attr_data = html_attrs(headline_data) 
attr_data

urlslist = unlist(attr_data)
urlslist = urlslist[grep(".html", urlslist)]
urlslist = urlslist[grep("/2018", urlslist)]
urlslist = unique(urlslist)
urlslist

#paste NY on front
urlslist = paste("https://www.nytimes.com", urlslist, sep = "")

##start a data frame
NYtimesDF = matrix(NA, nrow = length(urlslist), ncol = 3)
colnames(NYtimesDF) = c("Source", "Url", "Text")
NYtimesDF = as.data.frame(NYtimesDF)

##for loops
for (i in 1:length(urlslist)){
  
  ##read in the URL
  webpage <- read_html(urlslist[i])
  
  ##pull the specific nodes
  headline_data = html_nodes(webpage,'.StoryBodyCompanionColumn') 
  
  ##pull the text
  text_data = html_text(headline_data)
  
  ##save the data
  NYtimesDF$Source[i] = "NY Times"
  NYtimesDF$Url[i] = urlslist[i]
  NYtimesDF$Text[i] = paste(text_data, collapse = "")
  Sys.sleep(runif(1, 1, 10)) #don't make them angry
} ##end for loop

write.csv(NYtimesDF, "NYtimes_kav.csv", row.names = F)

remDr$close()
# stop the selenium server
rD[["server"]]$stop()

```

```{r NYTimes_shut, eval = F, include = F}
library(rvest)
library(RSelenium)
library(beepr)

rD <- rsDriver(browser=c("chrome"), chromever="73.0.3683.68")
remDr <- rD[["client"]]

#page for NYTIMES
url <- 'https://www.nytimes.com/search?endDate=20190208&query=government%20shutdown&sort=best&startDate=20181208'

#navigate to it
remDr$navigate(url)

#click the button 
#don't change class name
#right click --> inspect element 
webElem <- remDr$findElement(using = 'class name',"css-1t62hi8")

#with 803 results at 10 per page we need
#803 - 10 original / 10 per click is 80 clicks

for (i in 1:80) {
  webElem$clickElement()
  Sys.sleep(runif(1, 1, 5))
}

webpage <-remDr$getPageSource()

#Reading the HTML code from the website - headlines
webpage2 <- read_html(unlist(webpage))

headline_data = html_nodes(webpage2,'a')

attr_data = html_attrs(headline_data) 
#attr_data

urlslist = unlist(attr_data)
urlslist = urlslist[grep(".html", urlslist)]
urlslist = urlslist[grep("/201", urlslist)]
urlslist = unique(urlslist)
urlslist

#paste NY on front
urlslist = paste("https://www.nytimes.com", urlslist, sep = "")

##start a data frame
NYtimesDF = matrix(NA, nrow = length(urlslist), ncol = 3)
colnames(NYtimesDF) = c("Source", "Url", "Text")
NYtimesDF = as.data.frame(NYtimesDF)

##for loops
for (i in 1:length(urlslist)){
  
  ##read in the URL
  webpage <- read_html(urlslist[i])
  
  ##pull the specific nodes
  headline_data = html_nodes(webpage,'.StoryBodyCompanionColumn') 
  
  ##pull the text
  text_data = html_text(headline_data)
  
  ##save the data
  NYtimesDF$Source[i] = "NY Times"
  NYtimesDF$Url[i] = urlslist[i]
  NYtimesDF$Text[i] = paste(text_data, collapse = "")
  Sys.sleep(runif(1, 1, 10)) #don't make them angry
} ##end for loop

write.csv(NYtimesDF, "NYtimes_gs.csv", row.names = F)
beep(sound = 5)

remDr$close()
# stop the selenium server
rD[["server"]]$stop()
```

````{r NPR_kav, eval = F, include = F}
library(rvest)
library(RSelenium)
library(beepr)

#open the browser
rD <- rsDriver(browser=c("chrome"), chromever="73.0.3683.68")
remDr <- rD[["client"]]

#create a blank space to put the links
urlslist_final = list()

##loop through the page number at the end until done with ~1000 / 20 = 50
for (i in 1:50) { ##change this to 50
  
  url = paste0('https://www.npr.org/search?query=kavanaugh&page=', i)
  
  #navigate to it
  remDr$navigate(url)
  
  #scroll on the page
  webscroll <- remDr$findElement("css", "body")
  webscroll$sendKeysToElement(list(key = "end"))
  
  #get the links
  webElems <- remDr$findElements(using = "css", "[href]")
  urlslist_final[[i]] = unlist(sapply(webElems, function(x) {x$getElementAttribute("href")}))
  
  #don't go too fast
  Sys.sleep(runif(1, 1, 5))
  
} #close the loop

remDr$close()
# stop the selenium server
rD[["server"]]$stop()

beep(sound = 3)

urlslist = unlist(urlslist_final)
#find all the ones with /2018/
urlslist2 = urlslist[grep("/2018", urlslist)]
urlslist2 = urlslist2[grep("npr.org", urlslist2)] #get rid of wbur
urlslist2 = unique(urlslist2)
temp = strsplit(urlslist2, split = "/")
temp2 = plyr::ldply(temp, rbind)
temp2 = as.data.frame(apply(temp2, 2, as.character), stringsAsFactors = F)

#deal with the ones with extra parts
temp2[temp2$`4` == "sections" , 4:8] = temp2[temp2$`4` == "sections" , 6:10]

#pull only the right dates
rows = rownames(temp2[ temp2$`4` == 2018 & #must be 2018 and 
                         (temp2$`5` == "09" & as.numeric(temp2$`6`)>=13 | 
                            temp2$`5` == "10" & as.numeric(temp2$`6`)<=11), #Must be 9 or 10 
                ])

  
urlslist3 = urlslist2[as.numeric(rows)]

##start a data frame
NPRDF = matrix(NA, nrow = length(urlslist3), ncol = 3)
colnames(NPRDF) = c("Source", "Url", "Text")
NPRDF = as.data.frame(NPRDF)


##for loops
for (i in 1:length(urlslist3)){
  
  ##read in the URL
  webpage2 <- read_html(urlslist3[i])
  
  ##pull the specific nodes
  headline_data2 = html_nodes(webpage2,'#storytext > p') 
  
  ##pull the text
  text_data2 = html_text(headline_data2)
  
  ##save the data
  NPRDF$Source[i] = "NPR"
  NPRDF$Url[i] = urlslist3[i]
  NPRDF$Text[i] = paste(text_data2, collapse = "")
  Sys.sleep(runif(1, 1, 10))
} ##end for loop

beep(sound = 3)

write.csv(NPRDF, "NPR_kav.csv", row.names = F)

```

```{r NPR_shut, eval = F, include = F}

library(rvest)
library(RSelenium)
library(beepr)

#open the browser
rD <- rsDriver(browser=c("chrome"), chromever="73.0.3683.68")
remDr <- rD[["client"]]

#create a blank space to put the links
urlslist_final = list()

##loop through the page number at the end until done with ~3316 / 20 = 166
for (i in 1:166) { 
  
  url = paste0('https://www.npr.org/search?query=government%20shutodwn&page=', i)
  
  #navigate to it
  remDr$navigate(url)
  
  #scroll on the page
  webscroll <- remDr$findElement("css", "body")
  webscroll$sendKeysToElement(list(key = "end"))
  
  #get the links
  webElems <- remDr$findElements(using = "css", "[href]")
  urlslist_final[[i]] = unlist(sapply(webElems, function(x) {x$getElementAttribute("href")}))
  
  #don't go too fast
  Sys.sleep(runif(1, 1, 5))
  
} #close the loop

remDr$close()
# stop the selenium server
rD[["server"]]$stop()

beep(sound = 3)

urlslist = unlist(urlslist_final)
#find all the ones with /2018/
urlslist2 = urlslist[grep("/201", urlslist)]
urlslist2 = urlslist2[grep("npr.org", urlslist2)] #get rid of wbur
urlslist2 = unique(urlslist2)
temp = strsplit(urlslist2, split = "/")
temp2 = plyr::ldply(temp, rbind)
temp2 = as.data.frame(apply(temp2, 2, as.character), stringsAsFactors = F)

#deal with the ones with extra parts
temp2[temp2$`4` == "sections" , 4:8] = temp2[temp2$`4` == "sections" , 6:10]

#pull only the right dates
rows = rownames(temp2[ (temp2$`4` == 2018 | temp2$`4` == 2019) & #must be 2018/19 and  
                         (
                           (temp2$`5` == "12" & as.numeric(temp2$`6`)>8 & temp2$`4` == 2018) | #after dec 12
                            (temp2$`5`== "01" & temp2$`4` == 2019) | #all of jan
                            (temp2$`5` == "02" & as.numeric(temp2$`6`)<8 & temp2$`4` == 2019)
                           ), #before feb 8
                ])

  
urlslist3 = urlslist2[as.numeric(rows)]

##start a data frame
NPRDF = matrix(NA, nrow = length(urlslist3), ncol = 3)
colnames(NPRDF) = c("Source", "Url", "Text")
NPRDF = as.data.frame(NPRDF)


##for loops
for (i in 315:length(urlslist3)){
  
  ##read in the URL
  webpage2 <- read_html(urlslist3[i])
  
  ##pull the specific nodes
  headline_data2 = html_nodes(webpage2,'#storytext > p') 
  
  ##pull the text
  text_data2 = html_text(headline_data2)
  
  ##save the data
  NPRDF$Source[i] = "NPR"
  NPRDF$Url[i] = urlslist3[i]
  NPRDF$Text[i] = paste(text_data2, collapse = "")
  Sys.sleep(runif(1, 1, 10))
} ##end for loop

beep(sound = 3)

write.csv(NPRDF, "NPR_gs.csv", row.names = F)
```

````{r Slate_kav, eval = F, include = F}
library(rvest)
library(RSelenium)
library(beepr)

#open the browser
rD <- rsDriver(browser=c("chrome"), chromever="73.0.3683.68")
remDr <- rD[["client"]]

url = "https://slate.com/search?q=kavanaugh&via=homepage_nav_search"

remDr$navigate(url)

#create a blank space to put the links
urlslist_final = list()

#loop over the results and go through them
#appear to be 10 pages
for (i in 1:10){
  
  #find the button to click on 
  webClick <- remDr$findElements(using = "css", ".gsc-cursor-page")
  
  #don't go too fast for the page to load
  Sys.sleep(runif(1, 1, 5))
  
  #click on the page number
  webClick[[i]]$clickElement()
  
  #page reload
  Sys.sleep(runif(1, 1, 5))
  
  #find the urls for page X
  webElems <- remDr$findElements(using = "css", "[href]")
  
  #add the urls to a list
  urlslist_final[[i]] = unlist(sapply(webElems, function(x) {x$getElementAttribute("href")}))
  
  #don't go too fast
  Sys.sleep(runif(1, 1, 5))
}

remDr$close()
# stop the selenium server
rD[["server"]]$stop()

beep(sound = 3)

##deal with the URLS
urlslist = unlist(urlslist_final)

#find all the ones with /2018/
urlslist2 = urlslist[grep("/2018/10|/2018/09", urlslist)]
urlslist2 = unique(urlslist2)

#71 articles seems suspiciously low, so also scraped this page
#https://slate.com/tag/brett-kavanaugh/

#open the browser
rD <- rsDriver(browser=c("chrome"), chromever="73.0.3683.68")
remDr <- rD[["client"]]

#create a blank space to put the links
urlslist_final = list()

#loop over the results and go through them
#337 / 20 = 17 pages
for (i in 1:17){
  
  url = paste0("https://slate.com/tag/brett-kavanaugh/", i)
  
  remDr$navigate(url)
  
  #don't go too fast
  Sys.sleep(runif(1, 1, 5))
  
  #find the urls for page X
  webElems <- remDr$findElements(using = "css", "[href]")
  
  #add the urls to a list
  urlslist_final[[i]] = unlist(sapply(webElems, function(x) {x$getElementAttribute("href")}))
  
  #don't go too fast
  Sys.sleep(runif(1, 1, 5))
}

remDr$close()
# stop the selenium server
rD[["server"]]$stop()

beep(sound = 3)

##deal with the URLS
urlslist = unlist(urlslist_final)

#find all the ones with /2018/
urlslist2.1 = urlslist[grep("/2018/10|/2018/09", urlslist)]
urlslist2.1 = unique(urlslist2.1)

urlslist3 = unique(c(urlslist2, urlslist2.1))

##start a data frame
SLATEDF = matrix(NA, nrow = length(urlslist3), ncol = 3)
colnames(SLATEDF) = c("Source", "Url", "Text")
SLATEDF = as.data.frame(SLATEDF)


##for loops
for (i in 1:length(urlslist3)){
  
  ##read in the URL
  webpage2 <- read_html(urlslist3[i])
  
  ##pull the specific nodes
  headline_data2 = html_nodes(webpage2,'.slate-paragraph') 
  
  ##pull the text
  text_data2 = html_text(headline_data2)
  
  ##save the data
  SLATEDF$Source[i] = "SLATE"
  SLATEDF$Url[i] = urlslist3[i]
  SLATEDF$Text[i] = paste(text_data2, collapse = "")
  Sys.sleep(runif(1, 1, 10))
} ##end for loop

beep(sound = 3)

write.csv(SLATEDF, "SLATE_kav.csv", row.names = F)

```

```{r SLATE_shut, eval = F, include = F}
library(rvest)
library(RSelenium)
library(beepr)

#open the browser
rD <- rsDriver(browser=c("chrome"), chromever="73.0.3683.68")
remDr <- rD[["client"]]

url = "https://slate.com/search?q=government+shutdown"

remDr$navigate(url)

#create a blank space to put the links
urlslist_final = list()

#loop over the results and go through them
#appear to be 10 pages
for (i in 1:10){
  
  #find the button to click on 
  webClick <- remDr$findElements(using = "css", ".gsc-cursor-page")
  
  #don't go too fast for the page to load
  Sys.sleep(runif(1, 1, 5))
  
  #click on the page number
  webClick[[i]]$clickElement()
  
  #page reload
  Sys.sleep(runif(1, 1, 5))
  
  #find the urls for page X
  webElems <- remDr$findElements(using = "css", "[href]")
  
  #add the urls to a list
  urlslist_final[[i]] = unlist(sapply(webElems, function(x) {x$getElementAttribute("href")}))
  
  #don't go too fast
  Sys.sleep(runif(1, 1, 5))
}

remDr$close()
# stop the selenium server
rD[["server"]]$stop()

beep(sound = 3)

##deal with the URLS
urlslist = unlist(urlslist_final)

#find all the ones from 12 2018 to 02 2019
urlslist2 = urlslist[grep("/2018/12|/2019/01|/2019/02", urlslist)]
urlslist2 = unique(urlslist2)

#60 something articles seems suspiciously low, so also scraped this page
#https://slate.com/tag/governtment-shutdown

#open the browser
rD <- rsDriver(browser=c("chrome"), chromever="73.0.3683.68")
remDr <- rD[["client"]]

#create a blank space to put the links
urlslist_final = list()

#loop over the results and go through them
#354 / 20 = 18 pages
for (i in 1:18){
  
  url = paste0("https://slate.com/tag/government-shutdown/", i)
  
  remDr$navigate(url)
  
  #don't go too fast
  Sys.sleep(runif(1, 1, 5))
  
  #find the urls for page X
  webElems <- remDr$findElements(using = "css", "[href]")
  
  #add the urls to a list
  urlslist_final[[i]] = unlist(sapply(webElems, function(x) {x$getElementAttribute("href")}))
  
  #don't go too fast
  Sys.sleep(runif(1, 1, 5))
}

remDr$close()
# stop the selenium server
rD[["server"]]$stop()

beep(sound = 3)

##deal with the URLS
urlslist = unlist(urlslist_final)

#find all the ones with /2018/
urlslist2.1 = urlslist[grep("/2018/12|/2019/01|/2019/02", urlslist)]
urlslist2.1 = unique(urlslist2.1)

urlslist3 = unique(c(urlslist2, urlslist2.1))

##start a data frame
SLATEDF = matrix(NA, nrow = length(urlslist3), ncol = 3)
colnames(SLATEDF) = c("Source", "Url", "Text")
SLATEDF = as.data.frame(SLATEDF)


##for loops
for (i in 1:length(urlslist3)){
  
  ##read in the URL
  webpage2 <- read_html(urlslist3[i])
  
  ##pull the specific nodes
  headline_data2 = html_nodes(webpage2,'.slate-paragraph') 
  
  ##pull the text
  text_data2 = html_text(headline_data2)
  
  ##save the data
  SLATEDF$Source[i] = "SLATE"
  SLATEDF$Url[i] = urlslist3[i]
  SLATEDF$Text[i] = paste(text_data2, collapse = "")
  Sys.sleep(runif(1, 1, 10))
} ##end for loop

#beep(sound = 3)

write.csv(SLATEDF, "SLATE_gs.csv", row.names = F)
```

````{r HuffPo_kav, eval = F, include = F}
library(rvest)
library(RSelenium)
library(beepr)
library(lubridate)

#open the browser
rD <- rsDriver(browser=c("chrome"), chromever="73.0.3683.68")
remDr <- rD[["client"]]

url = "https://search.huffpost.com/search?utf8=%E2%9C%93&p=kavanaugh&fr=huffpost_desktop-s"

remDr$navigate(url)

#create a blank space to put the links
urlslist_final = list()

#loop over the results and go through them
#943 results with 10 per page 94 clicks
for (i in 1:94){
  
  #find the urls for page X
  webElems <- remDr$findElements(using = "class name", "fz-20")
  
  #add the urls to a list
  urlslist_final[[i]] = unlist(sapply(webElems, function(x) {x$getElementAttribute("href")}))
  
  #find the button to click on 
  webClick <- remDr$findElements(using = "class name", "next")
  
  #click on the page number
  webClick[[1]]$clickElement()
  
  #page reload
  Sys.sleep(runif(1, 1, 5))
  
}

remDr$close()
# stop the selenium server
rD[["server"]]$stop()

beep(sound = 3)

##deal with the URLS
urlslist = unlist(urlslist_final)
urlslist = unique(urlslist)

##start a data frame
HUFFPODF = matrix(NA, nrow = length(urlslist), ncol = 3)
colnames(HUFFPODF) = c("Source", "Url", "Text")
HUFFPODF = as.data.frame(HUFFPODF)
int <- interval(ymd("2018-09-13"), ymd("2018-10-11"))


for (i in 937:length(urlslist)){
  
  #convert the url
  temp = paste("https", unlist(strsplit(unlist(strsplit(urlslist[i], "https"))[3], "/RK=0"))[1], sep = "")
  temp = gsub("%3a", ":", temp)
  temp = gsub("%2f", "/", temp)
  
  webpage = read_html(temp)
  #figure out the date
  headline_data = html_nodes(webpage, '.timestamp__date--published')
  text_data = html_text(headline_data)
  time = unlist(strsplit(text_data, split = "\n| "))[2]
  time = as.Date(time, format = "%m/%d/%Y")
  
  if (time %within% int) {
    ##pull out the text
    headline_data2 = html_nodes(webpage,'p') 
    text_data2 = html_text(headline_data2)
    
    ##save the data
    HUFFPODF$Source[i] = "HUFFPO"
    HUFFPODF$Url[i] = urlslist[i]
    HUFFPODF$Text[i] = paste(text_data2, collapse = "")
  }
  
  Sys.sleep(runif(1,1,10))
  
  }

beep(sound = 3)

write.csv(HUFFPODF, "HUFFPO_kav.csv", row.names = F)
```

```{r HuffPo_shut, eval = F, include = F}
library(rvest)
library(RSelenium)
library(beepr)
library(lubridate)

#open the browser
rD <- rsDriver(browser=c("chrome"), chromever="73.0.3683.68")
remDr <- rD[["client"]]

url = "https://search.huffpost.com/search?utf8=%E2%9C%93&p=government+shutdown&fr=huffpost_desktop"

remDr$navigate(url)

#create a blank space to put the links
urlslist_final = list()

#loop over the results and go through them
#942 results with 10 per page 94 clicks
for (i in 1:94){
  
  #find the urls for page X
  webElems <- remDr$findElements(using = "class name", "fz-20")
  
  #add the urls to a list
  urlslist_final[[i]] = unlist(sapply(webElems, function(x) {x$getElementAttribute("href")}))
  
  #find the button to click on 
  webClick <- remDr$findElements(using = "class name", "next")
  
  #click on the page number
  webClick[[1]]$clickElement()
  
  #page reload
  Sys.sleep(runif(1, 1, 5))
  
}

remDr$close()
# stop the selenium server
rD[["server"]]$stop()

beep(sound = 2)

##deal with the URLS
urlslist = unlist(urlslist_final)
urlslist = unique(urlslist)

##start a data frame
HUFFPODF = matrix(NA, nrow = length(urlslist), ncol = 3)
colnames(HUFFPODF) = c("Source", "Url", "Text")
HUFFPODF = as.data.frame(HUFFPODF)
int <- interval(ymd("2018-12-08"), ymd("2019-02-08"))


for (i in 1:length(urlslist)){
  
  #convert the url
  temp = paste("https", unlist(strsplit(unlist(strsplit(urlslist[i], "https"))[3], "/RK=0"))[1], sep = "")
  temp = gsub("%3a", ":", temp)
  temp = gsub("%2f", "/", temp)
  
  webpage = read_html(temp)
  #figure out the date
  headline_data = html_nodes(webpage, '.timestamp__date--published')
  text_data = html_text(headline_data)
  time = unlist(strsplit(text_data, split = "\n| "))[2]
  time = as.Date(time, format = "%m/%d/%Y")
  
  if (time %within% int) {
    ##pull out the text
    headline_data2 = html_nodes(webpage,'p') 
    text_data2 = html_text(headline_data2)
    
    ##save the data
    HUFFPODF$Source[i] = "HUFFPO"
    HUFFPODF$Url[i] = urlslist[i]
    HUFFPODF$Text[i] = paste(text_data2, collapse = "")
  }
  
  Sys.sleep(runif(1,1,10))
  
  }

beep(sound = 2)

write.csv(HUFFPODF, "HUFFPO_gs.csv", row.names = F)
```

````{r Politico_kav, eval = F, include = F}
library(rvest)
library(RSelenium)
library(beepr)
library(lubridate)

#open the browser
rD <- rsDriver(browser=c("chrome"), chromever="73.0.3683.68")
remDr <- rD[["client"]]

url = "https://www.politico.com/search?adv=true&userInitiated=true&s=newest&q=kavanaugh&pv=&c=&r=&start=09%2F13%2F2018&start_submit=09%2F13%2F2018&end=10%2F11%2F2018&end_submit=10%2F11%2F2018"

remDr$navigate(url)

#create a blank space to put the links
urlslist_final = list()

#loop over the results and go through them
#749 with 20 per page with 37 clicks

for (i in 1:37){
  
  if (i > 1){
    #navigate to the next page
    url = paste0("https://www.politico.com/search/", i, 
                 "?s=newest&q=kavanaugh&adv=true&start=09/13/2018&end=10/11/2018")
    remDr$navigate(url)
  }
  
  #find the urls for page X
  webElems <- remDr$findElements(using = "css", "[href]")
  
  #add the urls to a list
  urlslist_final[[i]] = unlist(sapply(webElems, function(x) {x$getElementAttribute("href")}))

  #page reload
  Sys.sleep(runif(1, 1, 5))
  
}

remDr$close()
# stop the selenium server
rD[["server"]]$stop()

beep(sound = 7)

##deal with the URLS
urlslist = unlist(urlslist_final)
urlslist = unique(urlslist)
urlslist = urlslist[grep("/2018/", urlslist)]
urlslist = urlslist[-c(grep("subscriber.", urlslist))] #take out the stuff we can't see

##start a data frame
POLITICODF = matrix(NA, nrow = length(urlslist), ncol = 3)
colnames(POLITICODF) = c("Source", "Url", "Text")
POLITICODF = as.data.frame(POLITICODF)


for (i in 1:length(urlslist)){
  
  webpage = read_html(urlslist[i])
  
  headline_data2 = html_nodes(webpage,'p') 
  text_data2 = html_text(headline_data2)
  
  ##save the data
  POLITICODF$Source[i] = "POLITICO"
  POLITICODF$Url[i] = urlslist[i]
  POLITICODF$Text[i] = paste(text_data2, collapse = "")
  
  }
  
  Sys.sleep(runif(1,1,10))
  
beep(sound = 7)

write.csv(POLITICODF, "POLITICO_kav.csv", row.names = F)
#easiest one yet
```

```{r Politico_shut, eval = F, include = F}
library(rvest)
library(RSelenium)
library(beepr)
library(lubridate)

#open the browser
rD <- rsDriver(browser=c("chrome"), chromever="73.0.3683.68")
remDr <- rD[["client"]]

url = "https://www.politico.com/search?adv=true&userInitiated=true&s=newest&q=government+shutdown&pv=&c=&r=&start=12%2F08%2F2018&start_submit=12%2F08%2F2018&end=02%2F08%2F2019&end_submit=02%2F08%2F2019"

remDr$navigate(url)

#create a blank space to put the links
urlslist_final = list()

#loop over the results and go through them
#1329 with 20 per page with 66 clicks

for (i in 1:66){
  
  if (i > 1){
    #navigate to the next page
    url = paste0("https://www.politico.com/search/", i, 
                 "?s=newest&q=government shutdown&adv=true&start=12/08/2018&end=02/08/2019")
    remDr$navigate(url)
  }
  
  #find the urls for page X
  webElems <- remDr$findElements(using = "css", "[href]")
  
  #add the urls to a list
  urlslist_final[[i]] = unlist(sapply(webElems, function(x) {x$getElementAttribute("href")}))

  #page reload
  Sys.sleep(runif(1, 1, 5))
  
}

remDr$close()
# stop the selenium server
rD[["server"]]$stop()

beep(sound = 7)

##deal with the URLS
urlslist = unlist(urlslist_final)
urlslist = unique(urlslist)
urlslist = urlslist[grep("/2018/", urlslist)]
urlslist = urlslist[-c(grep("subscriber.", urlslist))] #take out the stuff we can't see

##start a data frame
POLITICODF = matrix(NA, nrow = length(urlslist), ncol = 3)
colnames(POLITICODF) = c("Source", "Url", "Text")
POLITICODF = as.data.frame(POLITICODF)


for (i in 1:length(urlslist)){
  
  webpage = read_html(urlslist[i])
  
  headline_data2 = html_nodes(webpage,'p') 
  text_data2 = html_text(headline_data2)
  
  ##save the data
  POLITICODF$Source[i] = "POLITICO"
  POLITICODF$Url[i] = urlslist[i]
  POLITICODF$Text[i] = paste(text_data2, collapse = "")
  
  
  Sys.sleep(runif(1,1,10))
  
  }
  
  
beep(sound = 7)

write.csv(POLITICODF, "POLITICO_gs.csv", row.names = F)

```

````{r Fox_kav, eval = F, include = F}

##start a data frame
library(rvest)
library(RSelenium)
library(beepr)
library(lubridate)

#open the browser
rD <- rsDriver(browser=c("chrome"), chromever="73.0.3683.68")
remDr <- rD[["client"]]

#create a blank space to put the links
urlslist_final = list()

#loop over the results and go through them
#1651 with 10 per page with 165 clicks

for (i in 1:165){
  
    #navigate to the page
    url = paste0("https://www.foxnews.com/search-results/search?q=kavanaugh&ss=fn&min_date=2018-09-13&max_date=2018-10-11&start=",
                 (i-1)*10)
    remDr$navigate(url)
  
  #find the urls for page X
  webElems <- remDr$findElements(using = "css", ".ng-binding")
  
  #add the urls to a list
  urlslist_final[[i]] = unlist(sapply(webElems, function(x) {x$getElementAttribute("href")}))

  #page reload
  Sys.sleep(runif(1, 1, 5))
  
}

remDr$close()
# stop the selenium server
rD[["server"]]$stop()

beep(sound = 6)

##deal with the URLS
urlslist = unlist(urlslist_final)
urlslist = unique(urlslist)
urlslist = urlslist[-c(grep("video.", urlslist))] #take out videos
urlslist = urlslist[-c(grep("/search-results", urlslist))] #take out search result fake links


##start a data frame
FOXDF = matrix(NA, nrow = length(urlslist), ncol = 3)
colnames(FOXDF) = c("Source", "Url", "Text")
FOXDF = as.data.frame(FOXDF)


for (i in 1:length(urlslist)){
  
  webpage = read_html(urlslist[i])
  
  headline_data2 = html_nodes(webpage,'p') 
  text_data2 = html_text(headline_data2)
  
  ##save the data
  FOXDF$Source[i] = "FOX"
  FOXDF$Url[i] = urlslist[i]
  FOXDF$Text[i] = paste(text_data2, collapse = "")
  
  
  Sys.sleep(runif(1,1,10))
  
  }
  
  
beep(sound = 7)

write.csv(FOXDF, "FOX_kav.csv", row.names = F)
```

````{r Breitbart_kav, eval = F, include = F}
library(rvest)
library(RSelenium)
library(beepr)
library(lubridate)

#open the browser
rD <- rsDriver(browser=c("chrome"), chromever="73.0.3683.68")
remDr <- rD[["client"]]

#create a blank space to put the links
urlslist_final = list()

url = "https://www.breitbart.com/tag/brett-kavanaugh/"

remDr$navigate(url)

#loop over the results and go through them
for (i in 1:42){
  
  #find the urls for page X
  webElems <- remDr$findElements(using = "css", "h2 a")
  
  #add the urls to a list
  urlslist_final[[i]] = unlist(sapply(webElems, function(x) {x$getElementAttribute("href")}))

  if (i > 1)
    {
  url = paste0("https://www.breitbart.com/tag/brett-kavanaugh/page/",
               i+1, "/")
  remDr$navigate(url)
  }
  
  #page reload
  Sys.sleep(runif(1, 1, 5))
  
}

remDr$close()
# stop the selenium server
rD[["server"]]$stop()

beep(sound = 6)

##deal with the URLS
urlslist = unlist(urlslist_final)
urlslist = unique(urlslist)
urlslist = urlslist[-c(grep("store", urlslist))] #take out store

##find the right dates
temp = strsplit(urlslist, split = "/")
temp2 = plyr::ldply(temp, rbind)
temp2 = as.data.frame(apply(temp2, 2, as.character), stringsAsFactors = F)

#pull only the right dates
rows = rownames(temp2[ temp2$`5` == 2018 & #must be 2018 and 
                         (temp2$`6` == "09" & as.numeric(temp2$`7`)>=13 | 
                            temp2$`6` == "10" & as.numeric(temp2$`7`)<=11), #Must be 9 or 10 
                       ])

urlslist3 = urlslist[as.numeric(rows)]

##start a data frame
BREITBARTDF = matrix(NA, nrow = length(urlslist3), ncol = 3)
colnames(BREITBARTDF) = c("Source", "Url", "Text")
BREITBARTDF = as.data.frame(BREITBARTDF)


for (i in 1:length(urlslist3)){
  
  webpage = read_html(urlslist3[i])
  
  headline_data2 = html_nodes(webpage,'h2, p') 
  text_data2 = html_text(headline_data2)
  
  ##save the data
  BREITBARTDF$Source[i] = "BREITBART"
  BREITBARTDF$Url[i] = urlslist3[i]
  BREITBARTDF$Text[i] = paste(text_data2, collapse = "")
  
  
  Sys.sleep(runif(1,1,10))
  
  }
  
  
beep(sound = 7)

write.csv(BREITBARTDF, "BREITBART_kav.csv", row.names = F)
```

```{r Breitbart_gs, eval = F, include = F}
library(rvest)
library(RSelenium)
library(beepr)
library(lubridate)

#open the browser
rD <- rsDriver(browser=c("chrome"), chromever="73.0.3683.68")
remDr <- rD[["client"]]

#create a blank space to put the links
urlslist_final = list()

url = "https://www.breitbart.com/tag/government-shutdown/"

remDr$navigate(url)

#loop over the results and go through them
for (i in 1:16){
  
  #find the urls for page X
  webElems <- remDr$findElements(using = "css", "h2 a")
  
  #add the urls to a list
  urlslist_final[[i]] = unlist(sapply(webElems, function(x) {x$getElementAttribute("href")}))

  if (i > 1)
    {
  url = paste0("https://www.breitbart.com/tag/government-shutdown/page/",
               i+1, "/")
  remDr$navigate(url)
  }
  
  #page reload
  Sys.sleep(runif(1, 1, 5))
  
}

remDr$close()
# stop the selenium server
rD[["server"]]$stop()

beep(sound = 6)

##deal with the URLS
urlslist = unlist(urlslist_final)
urlslist = unique(urlslist)
urlslist = urlslist[-c(grep("store", urlslist))] #take out store

##find the right dates
temp = strsplit(urlslist, split = "/")
temp2 = plyr::ldply(temp, rbind)
temp2 = as.data.frame(apply(temp2, 2, as.character), stringsAsFactors = F)

#pull only the right dates
rows = rownames(temp2[ (temp2$`5` == 2018 | temp2$`5` == 2019) & #must be 2018/19 and  
                         (
                           (temp2$`6` == "12" & as.numeric(temp2$`7`)>8 & temp2$`5` == 2018) | #after dec 12
                             (temp2$`6`== "01" & temp2$`5` == 2019) | #all of jan
                             (temp2$`6` == "02" & as.numeric(temp2$`7`)<8 & temp2$`5` == 2019)
                         ), #before feb 8
                       ])

urlslist3 = urlslist[as.numeric(rows)]

##start a data frame
BREITBARTDF = matrix(NA, nrow = length(urlslist3), ncol = 3)
colnames(BREITBARTDF) = c("Source", "Url", "Text")
BREITBARTDF = as.data.frame(BREITBARTDF)


for (i in 1:length(urlslist3)){
  
  webpage = read_html(urlslist3[i])
  
  headline_data2 = html_nodes(webpage,'h2, p') 
  text_data2 = html_text(headline_data2)
  
  ##save the data
  BREITBARTDF$Source[i] = "BREITBART"
  BREITBARTDF$Url[i] = urlslist3[i]
  BREITBARTDF$Text[i] = paste(text_data2, collapse = "")
  
  
  Sys.sleep(runif(1,1,10))
  
  }
  
  
beep(sound = 7)

write.csv(BREITBARTDF, "BREITBART_gs.csv", row.names = F)
```

````{r Rush_kav, eval = F, include = F}
library(rvest)
library(RSelenium)
library(beepr)
library(lubridate)

#open the browser
rD <- rsDriver(browser=c("chrome"), chromever="73.0.3683.68")
remDr <- rD[["client"]]

#create a blank space to put the links
urlslist_final = list()

url = "https://www.rushlimbaugh.com/?s=kavanaugh&is_v=1"

remDr$navigate(url)

#loop over the results and go through them
##18 pages
for (i in 1:18){
  
  #find the urls for page X
  webElems <- remDr$findElements(using = "css", "a")
  
  #add the urls to a list
  urlslist_final[[i]] = unlist(sapply(webElems, function(x) {x$getElementAttribute("href")}))

  #find the thing to click on 
  webClick <- remDr$findElements(using = "css", ".ais-pagination--link")
  
  #click on the page number
  webClick[[18]]$clickElement()
  
  #page reload
  Sys.sleep(runif(1, 1, 5))
  
}

remDr$close()
# stop the selenium server
rD[["server"]]$stop()

beep(sound = 6)

##deal with the URLS
urlslist = unlist(urlslist_final)
urlslist = unique(urlslist)
urlslist = urlslist[-c(grep("store", urlslist))] #take out store
urlslist = urlslist[grep("/daily/[0-9]*/[0-9]*/[0-9]*/[a-zA-Z]", urlslist)]

##find the right dates
temp = strsplit(urlslist, split = "/")
temp2 = plyr::ldply(temp, rbind)
temp2 = as.data.frame(apply(temp2, 2, as.character), stringsAsFactors = F)

#pull only the right dates
rows = rownames(temp2[ temp2$`5` == 2018 & #must be 2018 and 
                         (temp2$`6` == "09" & as.numeric(temp2$`7`)>=13 | 
                            temp2$`6` == "10" & as.numeric(temp2$`7`)<=11), #Must be 9 or 10 
                       ])

urlslist3 = urlslist[as.numeric(rows)]

##start a data frame
RUSHDF = matrix(NA, nrow = length(urlslist3), ncol = 3)
colnames(RUSHDF) = c("Source", "Url", "Text")
RUSHDF = as.data.frame(RUSHDF)


for (i in 1:length(urlslist3)){
  
  webpage = read_html(urlslist3[i])
  
  headline_data2 = html_nodes(webpage,'p') 
  text_data2 = html_text(headline_data2)
  
  ##save the data
  RUSHDF$Source[i] = "RUSH"
  RUSHDF$Url[i] = urlslist3[i]
  RUSHDF$Text[i] = paste(text_data2, collapse = "")
  
  
  Sys.sleep(runif(1,1,10))
  
  }
  
  
beep(sound = 7)

write.csv(RUSHDF, "RUSH_kav.csv", row.names = F)
```

```{r Rush_shut, eval = F, include = F}
library(rvest)
library(RSelenium)
library(beepr)
library(lubridate)

#open the browser
rD <- rsDriver(browser=c("chrome"), chromever="73.0.3683.68")
remDr <- rD[["client"]]

#create a blank space to put the links
urlslist_final = list()

url = "https://www.rushlimbaugh.com/?s=government%20shutdown&is_v=1"

remDr$navigate(url)

#loop over the results and go through them
##47 pages
for (i in 1:47){
  
  #find the urls for page X
  webElems <- remDr$findElements(using = "css", "a")
  
  #add the urls to a list
  urlslist_final[[i]] = unlist(sapply(webElems, function(x) {x$getElementAttribute("href")}))

  #find the thing to click on 
  webClick <- remDr$findElements(using = "css", ".ais-pagination--link")
  
  #click on the page number
  webClick[[18]]$clickElement()
  
  #page reload
  Sys.sleep(runif(1, 1, 5))
  
}

remDr$close()
# stop the selenium server
rD[["server"]]$stop()

beep(sound = 6)

##deal with the URLS
urlslist = unlist(urlslist_final)
urlslist = unique(urlslist)
urlslist = urlslist[-c(grep("store", urlslist))] #take out store
urlslist = urlslist[grep("/daily/[0-9]*/[0-9]*/[0-9]*/[a-zA-Z]", urlslist)]

##find the right dates
temp = strsplit(urlslist, split = "/")
temp2 = plyr::ldply(temp, rbind)
temp2 = as.data.frame(apply(temp2, 2, as.character), stringsAsFactors = F)

#pull only the right dates
rows = rownames(temp2[ (temp2$`5` == 2018 | temp2$`5` == 2019) & #must be 2018/19 and  
                         (
                           (temp2$`6` == "12" & as.numeric(temp2$`7`)>8 & temp2$`5` == 2018) | #after dec 12
                             (temp2$`6`== "01" & temp2$`5` == 2019) | #all of jan
                             (temp2$`6` == "02" & as.numeric(temp2$`7`)<8 & temp2$`5` == 2019)
                         ), #before feb 8
                       ])
urlslist3 = urlslist[as.numeric(rows)]

##start a data frame
RUSHDF = matrix(NA, nrow = length(urlslist3), ncol = 3)
colnames(RUSHDF) = c("Source", "Url", "Text")
RUSHDF = as.data.frame(RUSHDF)


for (i in 1:length(urlslist3)){
  
  webpage = read_html(urlslist3[i])
  
  headline_data2 = html_nodes(webpage,'p') 
  text_data2 = html_text(headline_data2)
  
  ##save the data
  RUSHDF$Source[i] = "RUSH"
  RUSHDF$Url[i] = urlslist3[i]
  RUSHDF$Text[i] = paste(text_data2, collapse = "")
  
  
  Sys.sleep(runif(1,1,10))
  
  }
  
  
beep(sound = 7)

write.csv(RUSHDF, "RUSH_gs.csv", row.names = F)
 
```

````{r Blaze_kav, eval = F, include = F}
url9 = 'https://www.theblaze.com/search/?q=kavanaugh'
webpage9 = read_html(url9)
headline_data9 = html_nodes(webpage9, '.widget__headline-text')

attr_data9 = html_attrs(headline_data9) 
attr_data9

urlslist9 = unlist(attr_data9)
urlslist9 = urlslist9[grep(urlslist9)]

##start a data frame
BlazeDF = matrix(NA, nrow = length(urlslist9), ncol = 3)
colnames(BlazeDF) = c("Source", "Url", "Text")
BlazeDF = as.data.frame(BlazeDF)

##for loops
for (i in 1:length(urlslist9)){
  
  ##read in the URL
  webpage9 <- read_html(urlslist9[i])
  
  ##pull the specific nodes
  headline_data9 = html_nodes(webpage9,'p') 
  
  ##pull the text
  text_data9 = html_text(headline_data9)
  
  ##save the data
  BlazeDF$Source[i] = "The Blaze"
  BlazeDF$Url[i] = urlslist9[i]
  BlazeDF$Text[i] = paste(text_data9, collapse = "")
} ##end for loop
```

```{r Blaze_shut, eval = F, include = F}
library(rvest)
library(RSelenium)
library(beepr)
library(lubridate)

#open the browser
rD <- rsDriver(browser=c("chrome"), chromever="73.0.3683.68")
remDr <- rD[["client"]]

url = "https://www.theblaze.com/search/?q=government+shutdown"

remDr$navigate(url)

webClick <- remDr$findElements(using = "css", ".action-btn")

#click next until done
while (length(webClick) > 0 ) { 
  
  #find the urls for page X
  webClick <- remDr$findElements(using = "css", ".action-btn")
  
  webClick[[1]]$clickElement()
  
  Sys.sleep(runif(1, 1, 5))
}

#find the urls for page X
webElems <- remDr$findElements(using = "css", "a")

#add the urls to a list
urlslist_final = unlist(sapply(webElems, function(x) {x$getElementAttribute("href")}))

remDr$close()
# stop the selenium server
rD[["server"]]$stop()


##deal with the URLS
urlslist = urlslist_final
urlslist = unique(urlslist)
urlslist = urlslist[-c(grep("video/", urlslist))]
urlslist = urlslist[grep("/news/", urlslist)]
urlslist = urlslist[-c(grep("/201[3-7]", urlslist))]

##start a data frame
BLAZEDF = matrix(NA, nrow = length(urlslist), ncol = 3)
colnames(BLAZEDF) = c("Source", "Url", "Text")
BLAZEDF = as.data.frame(BLAZEDF)
int <- interval(ymd("2018-12-08"), ymd("2019-02-08"))


for (i in 1:length(urlslist)){
  
  webpage = read_html(urlslist[i])
  
  headline_data = html_nodes(webpage, '.post-date')
  text_data = html_text(headline_data)
  date = as.Date(text_data, "%B %d, %Y")
  
  if (date %within% int){
    headline_data2 = html_nodes(webpage,'h1, h3, p') 
    text_data2 = html_text(headline_data2)
    
    ##save the data
    BLAZEDF$Source[i] = "BLAZE"
    BLAZEDF$Url[i] = urlslist[i]
    BLAZEDF$Text[i] = paste(text_data2, collapse = "")
  }
  
  Sys.sleep(runif(1,1,10))
  
  }
  
  
beep(sound = 7)

write.csv(BLAZEDF, "BLAZE_gs.csv", row.names = F)

```

```{r Hannity_kav, eval = F, include = F}
library(rvest)
library(RSelenium)
library(beepr)
library(lubridate)

#open the browser
rD <- rsDriver(browser=c("chrome"), chromever="73.0.3683.68")
remDr <- rD[["client"]]

url = "https://www.hannity.com/trending/brett-kavanaugh/"

remDr$navigate(url)

urlslist_final = list()

for (i in 1:4){
  
  #find the urls for page X
  webElems <- remDr$findElements(using = "css", "a")
  
  #add the urls to a list
  urlslist_final[[i]] = unlist(sapply(webElems, function(x) {x$getElementAttribute("href")}))
  
  #move on to the next page
  url = paste0("https://www.hannity.com/trending/brett-kavanaugh/?pg=", i+1)
  
  remDr$navigate(url)
  
  Sys.sleep(runif(1,1,5))
  
}

remDr$close()
# stop the selenium server
rD[["server"]]$stop()


##deal with the URLS
urlslist = unlist(urlslist_final)
urlslist = unique(urlslist)

#also use google search to see if we can find more

#open the browser
rD <- rsDriver(browser=c("chrome"), chromever="73.0.3683.68")
remDr <- rD[["client"]]

url = "https://www.google.com/search?q=kavanaugh+site%3Ahannity.com&source=lnt&tbs=cdr%3A1%2Ccd_min%3A9%2F13%2F2018%2Ccd_max%3A10%2F11%2F2018&tbm="

remDr$navigate(url)

urlslist_final2 = list()

for (i in 1:3){
  
  #find the urls for page X
  webElems <- remDr$findElements(using = "css", "a")
  
  #add the urls to a list
  urlslist_final2[[i]] = unlist(sapply(webElems, function(x) {x$getElementAttribute("href")}))
  
  #find the thing to click on 
  webClick <- remDr$findElements(using = "link text", "Next")
  
  #click on the page number
  webClick[[1]]$clickElement()
  
  Sys.sleep(runif(1,1,5))
  
}

remDr$close()
# stop the selenium server
rD[["server"]]$stop()

urlslist2 = unlist(urlslist_final2)
urlslist2 = c(urlslist, urlslist2)
urlslist2 = unique(urlslist2)
urlslist3 = urlslist2[grep("/media-room/[a-zA-Z]", urlslist2)]
urlslist3 = urlslist3[-c(grep("webcache", urlslist3))]
urlslist3 = urlslist3[-c(grep("radio-show", urlslist3))]


##start a data frame
HANNITYDF = matrix(NA, nrow = length(urlslist), ncol = 3)
colnames(HANNITYDF) = c("Source", "Url", "Text")
HANNITYDF = as.data.frame(HANNITYDF)
int <- interval(ymd("2018-09-13"), ymd("2018-10-11"))

for (i in 1:length(urlslist3)){
  
  webpage = read_html(urlslist3[i])
  
  headline_data = html_nodes(webpage, '.author-info')
  text_data = html_text(headline_data)
  text_data = strsplit(text_data, " ")[[1]][7]
  date = as.Date(text_data, "%m.%d.%y")
  
  if (date %within% int){
    headline_data2 = html_nodes(webpage,'h1, p') 
    text_data2 = html_text(headline_data2)
    
    ##save the data
    HANNITYDF$Source[i] = "HANNITY"
    HANNITYDF$Url[i] = urlslist3[i]
    HANNITYDF$Text[i] = paste(text_data2, collapse = "")
  }
  
  Sys.sleep(runif(1,1,10))
  
  }
  
  
beep(sound = 7)

write.csv(HANNITYDF, "HANNITY_kav.csv", row.names = F)
```

```{r Hannity_shut, eval = F, include = F}
library(rvest)
library(RSelenium)
library(beepr)
library(lubridate)

#open the browser
rD <- rsDriver(browser=c("chrome"), chromever="73.0.3683.68")
remDr <- rD[["client"]]

url = "http://www.hannity.com/trending/shutdown/"

remDr$navigate(url)

urlslist_final = list()

for (i in 1:4){
  
  #find the urls for page X
  webElems <- remDr$findElements(using = "css", "a")
  
  #add the urls to a list
  urlslist_final[[i]] = unlist(sapply(webElems, function(x) {x$getElementAttribute("href")}))
  
  #move on to the next page
  url = paste0("http://www.hannity.com/trending/shutdown/?pg=", i+1)
  
  remDr$navigate(url)
  
  Sys.sleep(runif(1,1,5))
  
}

remDr$close()
# stop the selenium server
rD[["server"]]$stop()


##deal with the URLS
urlslist = unlist(urlslist_final)
urlslist = unique(urlslist)

#also use google search to see if we can find more

#open the browser
rD <- rsDriver(browser=c("chrome"), chromever="73.0.3683.68")
remDr <- rD[["client"]]

url = "https://www.google.com/search?biw=1366&bih=632&tbs=cdr%3A1%2Ccd_min%3A12%2F8%2F2018%2Ccd_max%3A2%2F8%2F2019&ei=gECaXNe8Aaft_Qb9w7G4BA&q=government+shutdown+site%3Ahannity.com&oq=government+shutdown+site%3Ahannity.com&gs_l=psy-ab.3...0.0..1915...0.0..0.0.0.......0......gws-wiz.aFps0nA33mY"

remDr$navigate(url)

urlslist_final2 = list()

for (i in 1:9){
  
  #find the urls for page X
  webElems <- remDr$findElements(using = "css", "a")
  
  #add the urls to a list
  urlslist_final2[[i]] = unlist(sapply(webElems, function(x) {x$getElementAttribute("href")}))
  
  #find the thing to click on 
  webClick <- remDr$findElements(using = "link text", "Next")
  
  #click on the page number
  webClick[[1]]$clickElement()
  
  Sys.sleep(runif(1,1,5))
  
}

remDr$close()
# stop the selenium server
rD[["server"]]$stop()

urlslist2 = unlist(urlslist_final2)
urlslist2 = c(urlslist, urlslist2)
urlslist2 = unique(urlslist2)
urlslist3 = urlslist2[grep("/media-room/[a-zA-Z]", urlslist2)]
urlslist3 = urlslist3[-c(grep("webcache", urlslist3))]
urlslist3 = urlslist3[-c(grep("radio-show", urlslist3))]


##start a data frame
HANNITYDF = matrix(NA, nrow = length(urlslist), ncol = 3)
colnames(HANNITYDF) = c("Source", "Url", "Text")
HANNITYDF = as.data.frame(HANNITYDF)
int <- interval(ymd("2018-12-08"), ymd("2019-02-08"))

for (i in 1:length(urlslist3)){
  
  webpage = read_html(urlslist3[i])
  
  headline_data = html_nodes(webpage, '.author-info')
  text_data = html_text(headline_data)
  text_data = strsplit(text_data, " ")[[1]][7]
  date = as.Date(text_data, "%m.%d.%y")
  
  if (date %within% int){
    headline_data2 = html_nodes(webpage,'h1, p') 
    text_data2 = html_text(headline_data2)
    
    ##save the data
    HANNITYDF$Source[i] = "HANNITY"
    HANNITYDF$Url[i] = urlslist3[i]
    HANNITYDF$Text[i] = paste(text_data2, collapse = "")
  }
  
  Sys.sleep(runif(1,1,10))
  
  }
  
  
beep(sound = 7)

write.csv(HANNITYDF, "HANNITY_gs.csv", row.names = F)

```

## Sources

Articles pertaining to the Brett Kavanaugh Supreme Court confirmation hearing and the 2018-2019 U.S. Government shutdown were scraped from the websites of 10 U.S. news sources. As in Experiment 1, these sources were selected owing to their favorability among political partisans according to @Mitchell2014. The sources favored by the highest proportion of consistent liberals were *The New York Times*, *National Public Radio (NPR)*, *Slate*, *Huffington Post*, and *Politico* [@Mitchell2014]. The sources favored by the highest proportion of consistent conservatives included *Fox News*, *Breitbart*, *The Rush Limbaugh Show*, *The Blaze*, and *Sean Hannity*. Political articles referencing Brett Kavanaugh's nomination process were identified and subsequently scraped by including the URL for each source's coverage of the nomination in the *R* script. All code for this manuscript can be found at https://osf.io/5kpj7/, and the scripts, again written with the *papaja* library in *R*, are provided inline with this manuscript [@Aust2017].

## Materials

Using the *rvest* library in the statistical package *R*, we pulled body text for individual articles from each of the aforementioned 10 news sources (identified using CSS language). We compiled the articles into a dataset [@Wickham2016]. Using this dataset, we identified word count and average word count per source. This process was run for articles pertaining to Kavanaugh's nomination that were published between September 13, 2018 and October 11, 2018 inclusive. This date range was selected in reference to the widely-publicized and viewed nomination hearing on September 27, 2018. We set the start date at September 13 (two weeks before the hearing) and the end date at October 11 (two weeks after the hearing) so that we could capture a large amount of data (roughly one month) during which Kavanaugh's nomination was at its peak saturation in news coverage. 

The same process was followed for scraping articles related to the partial U.S. Government shutdown of 2018-2019. The articles scraped were published between December 8, 2018 and February 8, 2019 inclusive. Once again, the researchers elected to scrape articles published two weeks before and after the event in question in order to capitalize on the shutdown's saturation in American news media.

```{r data_clean_kav, eval = T, echo = F}
##combine all kav data into one dataset
#read it in
NYtimes_kav = read.csv("../exp2_data/NYtimes_kav.csv", stringsAsFactors = F, fileEncoding="UTF-8") #ok
NPR_kav = read.csv("../exp2_data/NPR_kav.csv", stringsAsFactors = F, fileEncoding="UTF-8") #ok
SLATE_kav = read.csv("../exp2_data/SLATE_kav.csv", stringsAsFactors = F, fileEncoding="UTF-8") #ok
HUFFPO_kav = read.csv("../exp2_data/HUFFPO_kav.csv", stringsAsFactors = F, fileEncoding="UTF-8") #ok
POLITICO_kav = read.csv("../exp2_data/POLITICO_kav.csv", stringsAsFactors = F, fileEncoding="UTF-8") #ok 
FOX_kav = read.csv("../exp2_data/FOX_kav.csv", stringsAsFactors = F) #ok 
BREITBART_kav = read.csv("../exp2_data/BREITBART_kav.csv", stringsAsFactors = F) #ok
RUSH_kav = read.csv("../exp2_data/RUSH_kav.csv", stringsAsFactors = F, fileEncoding="UTF-8") #ok 
BLAZE_kav = read.csv("../exp2_data/BLAZE_kav.csv", stringsAsFactors = F, fileEncoding="UTF-8") #ok 
HANNITY_kav = read.csv("../exp2_data/HANNITY_kav.csv", stringsAsFactors = F, fileEncoding="UTF-8") #ok 


#merge it together
master_kav = rbind(NYtimes_kav, NPR_kav, SLATE_kav, 
                        HUFFPO_kav, POLITICO_kav, FOX_kav, 
                        BREITBART_kav, RUSH_kav, BLAZE_kav, 
                        HANNITY_kav)

#fix this issue
master_kav$Text = gsub("\\.", "\\. ", master_kav$Text)

##get rid of the blank stuff
library(ngram)

for (i in 1:nrow(master_kav)) {
  master_kav$wordcount[i] = wordcount(master_kav$Text[i])
}

nozero = subset(master_kav, wordcount > 0)
nozero = subset(nozero, Source == "NY Times" | Source == "BLAZE" | Source == "NPR" | Source == "HUFFPO" | Source == "FOX" | Source == "HANNITY" | Source == "SLATE" | Source == "POLITICO" | Source == "RUSH" | Source == "BREITBART")
# tapply(nozero$wordcount, nozero$Source, mean)
# tapply(nozero$wordcount, nozero$Source, sum)

##take out the disqus ones - there are none
#nodis = nozero[ -c(grep("disqus", nozero$Url)), ]

##all data
final_kav = nozero
```

```{r data_clean_gs, eval = T, echo = F}
##combine all gs data into one dataset
#read it in
NYtimes_gs = read.csv("../exp2_data/NYtimes_gs.csv", stringsAsFactors = F, fileEncoding = "UTF-8") #ok
NPR_gs = read.csv("../exp2_data/NPR_gs.csv", stringsAsFactors = F) #ok
SLATE_gs = read.csv("../exp2_data/SLATE_gs.csv", stringsAsFactors = F) #ok 
HUFFPO_gs = read.csv("../exp2_data/HUFFPO_gs.csv", stringsAsFactors = F) #ok 
POLITICO_gs = read.csv("../exp2_data/POLITICO_gs.csv", stringsAsFactors = F, fileEncoding = "UTF-8") #not ok 
FOX_gs = read.csv("../exp2_data/FOX_gs.csv", stringsAsFactors = F) #ok 
BREITBART_gs = read.csv("../exp2_data/BREITBART_gs.csv", stringsAsFactors = F) #ok 
RUSH_gs = read.csv("../exp2_data/RUSH_gs.csv", stringsAsFactors = F, fileEncoding = "UTF-8") #not ok 
BLAZE_gs = read.csv("../exp2_data/BLAZE_gs.csv", stringsAsFactors = F) #ok 
HANNITY_gs = read.csv("../exp2_data/HANNITY_gs.csv", stringsAsFactors = F) #ok 

#merge it together
master_gs = rbind(NYtimes_gs, NPR_gs, SLATE_gs, 
                        HUFFPO_gs, POLITICO_gs, FOX_gs, 
                        BREITBART_gs, RUSH_gs, BLAZE_gs, 
                        HANNITY_gs)

#fix this issue
master_gs$Text = gsub("\\.", "\\. ", master_gs$Text)

for (i in 1:nrow(master_gs)) {
  master_gs$wordcount[i] = wordcount(master_gs$Text[i])
}

nozero = subset(master_gs, wordcount > 0)
nozero = subset(nozero, Source == "NY Times" | Source == "BLAZE" | Source == "NPR" | Source == "HUFFPO" | Source == "FOX" | Source == "HANNITY" | Source == "SLATE" | Source == "POLITICO" | Source == "RUSH" | Source == "BREITBART")

# tapply(nozero$wordcount, nozero$Source, mean)
# tapply(nozero$wordcount, nozero$Source, sum)

##take out the disqus ones
#nodis = nozero[ -c(grep("disqus", nozero$Url)), ]

##all data
final_gs = nozero
```

```{r stem_text_both, eval = T, echo = F}
#process the Text column (write a loop) - save processed text as a separate column
for(i in 1:nrow(final_kav)) {
  
  final_kav$edited[i] = preprocess(final_kav$Text[i], #one value at a time
           case = "lower", 
           remove.punct = TRUE,
           remove.numbers = FALSE, 
           fix.spacing = TRUE)

##stem the words - do this second in the loop
  final_kav$stemmed[i] = stemDocument(final_kav$edited[i], language = "english")
}

#process the Text column (write a loop) - save processed text as a separate column
for(i in 1:nrow(final_gs)) {
  
  final_gs$edited[i] = preprocess(final_gs$Text[i], #one value at a time
           case = "lower", 
           remove.punct = TRUE,
           remove.numbers = FALSE, 
           fix.spacing = TRUE)

##stem the words - do this second in the loop
  final_gs$stemmed[i] = stemDocument(final_gs$edited[i], language = "english")
}

```

## Data analysis

As in Experiment 1, the text was scanned with *ngram*. Again, blank articles, text from the Disqus system, and duplicate articles were removed [@Schmidt2017]. The text was processed and stemmed in order to convert to a usable form for further analysis [@Feinerer2017]. Words were subsequently matched with their valences from @Warriner2013. Depending on the results of @Jordan2019's multi-trait multi-method analysis of the MFD and the Moral Foundations Questionnaire, alternative forms of the Moral Foundations Dictionary with additional words may be imported instead of the original dictionary.

Using the *tm* and *ngram* packages in *R*, the researchers processed the text in order to convert it to lowercase, fix spacing anomalies, and remove punctuation [@Feinerer2017]. Each individual word was reduced to its stem (i.e., *diseased* was stemmed to *diseas*). Once again, the same procedure was applied to the MFD words and the words in the @Warriner2013 dataset. Using the @Warriner2013 dictionary, the words in the MFD were assigned their respective valence. The researchers obtained the words' percent occurrence in the text. Percents were multiplied by *z*-scored valence and categorized into their proper MFD category.

```{r create_datasets_harm_kav, eval = T, echo = F, include = F}
#read in the harm total words from the mtmm project
harm_all_kav = read.csv("../exp1_data/harm_words_total.csv", stringsAsFactors = F)

harm_final_data_kav = data.frame("URL" = NA, "Source" = NA, "Var1" = NA, "Freq" = NA, "wordcount" = NA)

for (i in 1:nrow(final_kav)) {
##first make a table of a response, unlisting everything
temp = as.data.frame(table(unlist(strsplit(final_kav$stemmed[i], " "))))

#only pull the words in the dataset for each foundation
temp2 = temp[temp$Var1 %in% unique(harm_all_kav$words[harm_all_kav$words != "" & harm_all_kav$words != "NA" & !is.na(harm_all_kav$words)]), ]

#control here for times when no words
if (nrow(temp2) == 0){
  temp2 = data.frame("Var1" = 0, "Freq" = 0)
}

#add to the dataset
temp2$URL = final_kav$Url[i]
temp2$Source = final_kav$Source[i]
temp2$wordcount = final_kav$wordcount[i]

#bind with original
harm_final_data_kav = rbind(harm_final_data_kav, temp2)
}

#pull in the data from Warriner 
harm_final_data_kav$valence = vlookup(harm_final_data_kav$Var1, #index in original data
                                  warriner, #where's the stuff you want to put in
                                  "V.Mean.Sum", #what column is the stuff you want
                                  "Word2") #what's the matching index

#figure out stupid words
#table(harm_final_data_kav$Var1[is.na(harm_final_data_kav$valence) & harm_final_data_kav$Freq > 0])
#fought, safer, spurn
harm_final_data_kav$valence[harm_final_data_kav$Var1 == "fought"] = warriner$V.Mean.Sum[warriner$Word2 == "fight"]
harm_final_data_kav$valence[harm_final_data_kav$Var1 == "safer"] = warriner$V.Mean.Sum[warriner$Word2 == "safe"]
#no spurn avaliable
```

```{r create_datasets_fair_kav, include=FALSE}
fair_all_kav = read.csv("../exp1_data/fair_words_total.csv", stringsAsFactors = F)

fair_final_data_kav = data.frame("URL" = NA, "Source" = NA, "Var1" = NA, "Freq" = NA, "wordcount" = NA)

for (i in 1:nrow(final_kav)) {
##first make a table of a response, unlisting everything
temp = as.data.frame(table(unlist(strsplit(final_kav$stemmed[i], " "))))

#only pull the words in the dataset for each foundation
temp2 = temp[temp$Var1 %in% unique(fair_all_kav$words[fair_all_kav$words != "" & fair_all_kav$words != "NA" & !is.na(fair_all_kav$words)]), ]

#control here for times when no words
if (nrow(temp2) == 0){
  temp2 = data.frame("Var1" = 0, "Freq" = 0)
}

#add to the dataset
temp2$URL = final_kav$Url[i]
temp2$Source = final_kav$Source[i]
temp2$wordcount = final_kav$wordcount[i]

#bind with original
fair_final_data_kav = rbind(fair_final_data_kav, temp2)
}

fair_final_data_kav$valence = vlookup(fair_final_data_kav$Var1, #index in original data
                                  warriner, #where's the stuff you want to put in
                                  "V.Mean.Sum", #what column is the stuff you want
                                  "Word2") #what's the matching index

#figure out stupid words
table(fair_final_data_kav$Var1[is.na(fair_final_data_kav$valence) & fair_final_data_kav$Freq > 0])
#just plagiar safer
fair_final_data_kav$valence[fair_final_data_kav$Var1 == "safer"] = warriner$V.Mean.Sum[warriner$Word2 == "safe"]
#nothing for just or plagiarism
```

```{r create_datasets_ingroup_kav, include=FALSE}
ingroup_all_kav = read.csv("../exp1_data/ingroup_words_total.csv", stringsAsFactors = F)

ingroup_final_data_kav = data.frame("URL" = NA, "Source" = NA, "Var1" = NA, "Freq" = NA, "wordcount" = NA)

for (i in 1:nrow(final_kav)) {
##first make a table of a response, unlisting everything
temp = as.data.frame(table(unlist(strsplit(final_kav$stemmed[i], " "))))

#only pull the words in the dataset for each foundation
temp2 = temp[temp$Var1 %in% unique(ingroup_all_kav$words[ingroup_all_kav$words != "" & ingroup_all_kav$words != "NA" & !is.na(ingroup_all_kav$words)]), ]

#control here for times when no words
if (nrow(temp2) == 0){
  temp2 = data.frame("Var1" = 0, "Freq" = 0)
}

#add to the dataset
temp2$URL = final_kav$Url[i]
temp2$Source = final_kav$Source[i]
temp2$wordcount = final_kav$wordcount[i]

#bind with original
ingroup_final_data_kav = rbind(ingroup_final_data_kav, temp2)
}

ingroup_final_data_kav$valence = vlookup(ingroup_final_data_kav$Var1, #index in original data
                                  warriner, #where's the stuff you want to put in
                                  "V.Mean.Sum", #what column is the stuff you want
                                  "Word2") #what's the matching index

#figure out stupid words
table(ingroup_final_data_kav$Var1[is.na(ingroup_final_data_kav$valence) & ingroup_final_data_kav$Freq > 0])
#backstabb  benedict      best      fals   indvidu      juda    knight      true
ingroup_final_data_kav$valence[ingroup_final_data_kav$Var1 == "fals"] = warriner$V.Mean.Sum[warriner$Word2 == "FALSE"]
ingroup_final_data_kav$valence[ingroup_final_data_kav$Var1 == "indvidu"] = mean(warriner$V.Mean.Sum[warriner$Word2 == "individu"])
ingroup_final_data_kav$valence[ingroup_final_data_kav$Var1 == "true"] = mean(warriner$V.Mean.Sum[warriner$Word2 == "TRUE"])
```

```{r create_datasets_authority_kav, include=FALSE}
authority_all_kav = read.csv("../exp1_data/authority_words_total.csv", stringsAsFactors = F)

authority_final_data_kav = data.frame("URL" = NA, "Source" = NA, "Var1" = NA, "Freq" = NA, "wordcount" = NA)

for (i in 1:nrow(final_kav)) {
##first make a table of a response, unlisting everything
temp = as.data.frame(table(unlist(strsplit(final_kav$stemmed[i], " "))))

#only pull the words in the dataset for each foundation
temp2 = temp[temp$Var1 %in% unique(authority_all_kav$words[authority_all_kav$words != "" & authority_all_kav$words != "NA" & !is.na(authority_all_kav$words)]), ]

#control here for times when no words
if (nrow(temp2) == 0){
  temp2 = data.frame("Var1" = 0, "Freq" = 0)
}

#add to the dataset
temp2$URL = final_kav$Url[i]
temp2$Source = final_kav$Source[i]
temp2$wordcount = final_kav$wordcount[i]

#bind with original
authority_final_data_kav = rbind(authority_final_data_kav, temp2)
}

authority_final_data_kav$valence = vlookup(authority_final_data_kav$Var1, #index in original data
                                  warriner, #where's the stuff you want to put in
                                  "V.Mean.Sum", #what column is the stuff you want
                                  "Word2") #what's the matching index

#figure out stupid words
table(authority_final_data_kav$Var1[is.na(authority_final_data_kav$valence) & authority_final_data_kav$Freq > 0])
#  abov behind   gase higher  older  taken 
authority_final_data_kav$valence[authority_final_data_kav$Var1 == "gase"] = warriner$V.Mean.Sum[warriner$Word2 == "gas"]
authority_final_data_kav$valence[authority_final_data_kav$Var1 == "higher"] = mean(warriner$V.Mean.Sum[warriner$Word2 == "high"])
authority_final_data_kav$valence[authority_final_data_kav$Var1 == "older"] = mean(warriner$V.Mean.Sum[warriner$Word2 == "old"])
authority_final_data_kav$valence[authority_final_data_kav$Var1 == "taken"] = mean(warriner$V.Mean.Sum[warriner$Word2 == "take"])
```

```{r create_datasets_purity_kav, include=FALSE}
purity_all_kav = read.csv("../exp1_data/purity_words_total.csv", stringsAsFactors = F)

purity_final_data_kav = data.frame("URL" = NA, "Source" = NA, "Var1" = NA, "Freq" = NA, "wordcount" = NA)

for (i in 1:nrow(final_kav)) {
##first make a table of a response, unlisting everything
temp = as.data.frame(table(unlist(strsplit(final_kav$stemmed[i], " "))))

#only pull the words in the dataset for each foundation
temp2 = temp[temp$Var1 %in% unique(purity_all_kav$words[purity_all_kav$words != "" & purity_all_kav$words != "NA" & !is.na(purity_all_kav$words)]), ]

#control here for times when no words
if (nrow(temp2) == 0){
  temp2 = data.frame("Var1" = 0, "Freq" = 0)
}

#add to the dataset
temp2$URL = final_kav$Url[i]
temp2$Source = final_kav$Source[i]
temp2$wordcount = final_kav$wordcount[i]

#bind with original
purity_final_data_kav = rbind(purity_final_data_kav, temp2)
}

purity_final_data_kav$valence = vlookup(purity_final_data_kav$Var1, #index in original data
                                  warriner, #where's the stuff you want to put in
                                  "V.Mean.Sum", #what column is the stuff you want
                                  "Word2") #what's the matching index

#figure out stupid words
table(purity_final_data_kav$Var1[is.na(purity_final_data_kav$valence) & purity_final_data_kav$Freq > 0])
#befor christian     jesus      mari      none    sacred    someon      true     women 
purity_final_data_kav$valence[purity_final_data_kav$Var1 == "sacred"] = warriner$V.Mean.Sum[warriner$Word2 == "sacr"]
purity_final_data_kav$valence[purity_final_data_kav$Var1 == "true"] = mean(warriner$V.Mean.Sum[warriner$Word2 == "TRUE"])
purity_final_data_kav$valence[purity_final_data_kav$Var1 == "women"] = mean(warriner$V.Mean.Sum[warriner$Word2 == "woman"])
```

```{r create_datasets_harm_gs, eval = T, echo = F, include = F}
#read in the harm total words from the mtmm project
harm_all_gs = read.csv("../exp1_data/harm_words_total.csv", stringsAsFactors = F)

harm_final_data_gs = data.frame("URL" = NA, "Source" = NA, "Var1" = NA, "Freq" = NA, "wordcount" = NA)

for (i in 1:nrow(final_gs)) {
##first make a table of a response, unlisting everything
temp = as.data.frame(table(unlist(strsplit(final_gs$stemmed[i], " "))))

#only pull the words in the dataset for each foundation
temp2 = temp[temp$Var1 %in% unique(harm_all_gs$words[harm_all_gs$words != "" & harm_all_gs$words != "NA" & !is.na(harm_all_gs$words)]), ]

#control here for times when no words
if (nrow(temp2) == 0){
  temp2 = data.frame("Var1" = 0, "Freq" = 0)
}

#add to the dataset
temp2$URL = final_gs$Url[i]
temp2$Source = final_gs$Source[i]
temp2$wordcount = final_gs$wordcount[i]

#bind with original
harm_final_data_gs = rbind(harm_final_data_gs, temp2)
}

#pull in the data from Warriner 
harm_final_data_gs$valence = vlookup(harm_final_data_gs$Var1, #index in original data
                                  warriner, #where's the stuff you want to put in
                                  "V.Mean.Sum", #what column is the stuff you want
                                  "Word2") #what's the matching index

#figure out stupid words
#table(harm_final_data_gs$Var1[is.na(harm_final_data_gs$valence) & harm_final_data_gs$Freq > 0])
#fought, safer, spurn
harm_final_data_gs$valence[harm_final_data_gs$Var1 == "fought"] = warriner$V.Mean.Sum[warriner$Word2 == "fight"]
harm_final_data_gs$valence[harm_final_data_gs$Var1 == "safer"] = warriner$V.Mean.Sum[warriner$Word2 == "safe"]
#no spurn avaliable
```

```{r create_datasets_fair_gs, echo = F, include=FALSE}
fair_all_gs = read.csv("../exp1_data/fair_words_total.csv", stringsAsFactors = F)

fair_final_data_gs = data.frame("URL" = NA, "Source" = NA, "Var1" = NA, "Freq" = NA, "wordcount" = NA)

for (i in 1:nrow(final_gs)) {
##first make a table of a response, unlisting everything
temp = as.data.frame(table(unlist(strsplit(final_gs$stemmed[i], " "))))

#only pull the words in the dataset for each foundation
temp2 = temp[temp$Var1 %in% unique(fair_all_gs$words[fair_all_gs$words != "" & fair_all_gs$words != "NA" & !is.na(fair_all_gs$words)]), ]

#control here for times when no words
if (nrow(temp2) == 0){
  temp2 = data.frame("Var1" = 0, "Freq" = 0)
}

#add to the dataset
temp2$URL = final_gs$Url[i]
temp2$Source = final_gs$Source[i]
temp2$wordcount = final_gs$wordcount[i]

#bind with original
fair_final_data_gs = rbind(fair_final_data_gs, temp2)
}

fair_final_data_gs$valence = vlookup(fair_final_data_gs$Var1, #index in original data
                                  warriner, #where's the stuff you want to put in
                                  "V.Mean.Sum", #what column is the stuff you want
                                  "Word2") #what's the matching index

#figure out stupid words
table(fair_final_data_gs$Var1[is.na(fair_final_data_gs$valence) & fair_final_data_gs$Freq > 0])
#just plagiar safer
fair_final_data_gs$valence[fair_final_data_gs$Var1 == "safer"] = warriner$V.Mean.Sum[warriner$Word2 == "safe"]
#nothing for just or plagiarism
```

```{r create_datasets_ingroup_gs, echo = F, include=FALSE}
ingroup_all_gs = read.csv("../exp1_data/ingroup_words_total.csv", stringsAsFactors = F)

ingroup_final_data_gs = data.frame("URL" = NA, "Source" = NA, "Var1" = NA, "Freq" = NA, "wordcount" = NA)

for (i in 1:nrow(final_gs)) {
##first make a table of a response, unlisting everything
temp = as.data.frame(table(unlist(strsplit(final_gs$stemmed[i], " "))))

#only pull the words in the dataset for each foundation
temp2 = temp[temp$Var1 %in% unique(ingroup_all_gs$words[ingroup_all_gs$words != "" & ingroup_all_gs$words != "NA" & !is.na(ingroup_all_gs$words)]), ]

#control here for times when no words
if (nrow(temp2) == 0){
  temp2 = data.frame("Var1" = 0, "Freq" = 0)
}

#add to the dataset
temp2$URL = final_gs$Url[i]
temp2$Source = final_gs$Source[i]
temp2$wordcount = final_gs$wordcount[i]

#bind with original
ingroup_final_data_gs = rbind(ingroup_final_data_gs, temp2)
}

ingroup_final_data_gs$valence = vlookup(ingroup_final_data_gs$Var1, #index in original data
                                  warriner, #where's the stuff you want to put in
                                  "V.Mean.Sum", #what column is the stuff you want
                                  "Word2") #what's the matching index

#figure out stupid words
table(ingroup_final_data_gs$Var1[is.na(ingroup_final_data_gs$valence) & ingroup_final_data_gs$Freq > 0])
#backstabb  benedict      best      fals   indvidu      juda    knight      true
ingroup_final_data_gs$valence[ingroup_final_data_gs$Var1 == "fals"] = warriner$V.Mean.Sum[warriner$Word2 == "FALSE"]
ingroup_final_data_gs$valence[ingroup_final_data_gs$Var1 == "indvidu"] = mean(warriner$V.Mean.Sum[warriner$Word2 == "individu"])
ingroup_final_data_gs$valence[ingroup_final_data_gs$Var1 == "true"] = mean(warriner$V.Mean.Sum[warriner$Word2 == "TRUE"])
```

```{r create_datasets_authority_gs, echo = F, include=FALSE}
authority_all_gs = read.csv("../exp1_data/authority_words_total.csv", stringsAsFactors = F)

authority_final_data_gs = data.frame("URL" = NA, "Source" = NA, "Var1" = NA, "Freq" = NA, "wordcount" = NA)

for (i in 1:nrow(final_gs)) {
##first make a table of a response, unlisting everything
temp = as.data.frame(table(unlist(strsplit(final_gs$stemmed[i], " "))))

#only pull the words in the dataset for each foundation
temp2 = temp[temp$Var1 %in% unique(authority_all_gs$words[authority_all_gs$words != "" & authority_all_gs$words != "NA" & !is.na(authority_all_gs$words)]), ]

#control here for times when no words
if (nrow(temp2) == 0){
  temp2 = data.frame("Var1" = 0, "Freq" = 0)
}

#add to the dataset
temp2$URL = final_gs$Url[i]
temp2$Source = final_gs$Source[i]
temp2$wordcount = final_gs$wordcount[i]

#bind with original
authority_final_data_gs = rbind(authority_final_data_gs, temp2)
}

authority_final_data_gs$valence = vlookup(authority_final_data_gs$Var1, #index in original data
                                  warriner, #where's the stuff you want to put in
                                  "V.Mean.Sum", #what column is the stuff you want
                                  "Word2") #what's the matching index

#figure out stupid words
table(authority_final_data_gs$Var1[is.na(authority_final_data_gs$valence) & authority_final_data_gs$Freq > 0])
#  abov behind   gase higher  older  taken 
authority_final_data_gs$valence[authority_final_data_gs$Var1 == "gase"] = warriner$V.Mean.Sum[warriner$Word2 == "gas"]
authority_final_data_gs$valence[authority_final_data_gs$Var1 == "higher"] = mean(warriner$V.Mean.Sum[warriner$Word2 == "high"])
authority_final_data_gs$valence[authority_final_data_gs$Var1 == "older"] = mean(warriner$V.Mean.Sum[warriner$Word2 == "old"])
authority_final_data_gs$valence[authority_final_data_gs$Var1 == "taken"] = mean(warriner$V.Mean.Sum[warriner$Word2 == "take"])
```

```{r create_datasets_purity_gs, echo = F, include=FALSE}
purity_all_gs = read.csv("../exp1_data/purity_words_total.csv", stringsAsFactors = F)

purity_final_data_gs = data.frame("URL" = NA, "Source" = NA, "Var1" = NA, "Freq" = NA, "wordcount" = NA)

for (i in 1:nrow(final_gs)) {
##first make a table of a response, unlisting everything
temp = as.data.frame(table(unlist(strsplit(final_gs$stemmed[i], " "))))

#only pull the words in the dataset for each foundation
temp2 = temp[temp$Var1 %in% unique(purity_all_gs$words[purity_all_gs$words != "" & purity_all_gs$words != "NA" & !is.na(purity_all_gs$words)]), ]

#control here for times when no words
if (nrow(temp2) == 0){
  temp2 = data.frame("Var1" = 0, "Freq" = 0)
}

#add to the dataset
temp2$URL = final_gs$Url[i]
temp2$Source = final_gs$Source[i]
temp2$wordcount = final_gs$wordcount[i]

#bind with original
purity_final_data_gs = rbind(purity_final_data_gs, temp2)
}

purity_final_data_gs$valence = vlookup(purity_final_data_gs$Var1, #index in original data
                                  warriner, #where's the stuff you want to put in
                                  "V.Mean.Sum", #what column is the stuff you want
                                  "Word2") #what's the matching index

#figure out stupid words
table(purity_final_data_gs$Var1[is.na(purity_final_data_gs$valence) & purity_final_data_gs$Freq > 0])
#befor christian     jesus      mari      none    sacred    someon      true     women 
purity_final_data_gs$valence[purity_final_data_gs$Var1 == "sacred"] = warriner$V.Mean.Sum[warriner$Word2 == "sacr"]
purity_final_data_gs$valence[purity_final_data_gs$Var1 == "true"] = mean(warriner$V.Mean.Sum[warriner$Word2 == "TRUE"])
purity_final_data_gs$valence[purity_final_data_gs$Var1 == "women"] = mean(warriner$V.Mean.Sum[warriner$Word2 == "woman"])
```

```{r descriptives_both, echo = F, include = FALSE}
##summarize the dataset so that you have the total and percent for each foundation
#kav
summary(harm_final_data_kav)
summary(fair_final_data_kav)
summary(ingroup_final_data_kav)
summary(authority_final_data_kav)
summary(purity_final_data_kav)
#gs
summary(harm_final_data_gs)
summary(fair_final_data_gs)
summary(ingroup_final_data_gs)
summary(authority_final_data_gs)
summary(purity_final_data_gs)
##report descriptive statistics of the foundations
#kav
mean(harm_final_data_kav$valence, na.rm = T)
sd(harm_final_data_kav$valence, na.rm = T)
mean(fair_final_data_kav$valence, na.rm = T)
sd(fair_final_data_kav$valence, na.rm = T)
mean(ingroup_final_data_kav$valence, na.rm = T)
sd(ingroup_final_data_kav$valence, na.rm = T)
mean(authority_final_data_kav$valence, na.rm = T)
sd(authority_final_data_kav$valence, na.rm = T)
mean(purity_final_data_kav$valence, na.rm = T)
sd(purity_final_data_kav$valence, na.rm = T)
#gs
mean(harm_final_data_gs$valence, na.rm = T)
sd(harm_final_data_gs$valence, na.rm = T)
mean(fair_final_data_gs$valence, na.rm = T)
sd(fair_final_data_gs$valence, na.rm = T)
mean(ingroup_final_data_gs$valence, na.rm = T)
sd(ingroup_final_data_gs$valence, na.rm = T)
mean(authority_final_data_gs$valence, na.rm = T)
sd(authority_final_data_gs$valence, na.rm = T)
mean(purity_final_data_gs$valence, na.rm = T)
sd(purity_final_data_gs$valence, na.rm = T)
#note that a big problem is the valence of the words 
```

```{r analysis_harm_kav, include = FALSE}
#final_data clean up
#multiply by frequency and create percentage
harm_final_data_kav$weight = harm_final_data_kav$Freq * harm_final_data_kav$valence / harm_final_data_kav$wordcount * 100

#flatten down to sum by article 
harm_final_temp_kav = aggregate(harm_final_data_kav$weight, 
          by = list(harm_final_data_kav$URL),
          FUN = sum, na.rm = T)
colnames(harm_final_temp_kav) = c("URL", "MFDweight")

harm_final_temp_kav$lean[grepl("nytimes.com", harm_final_temp_kav$URL) |
                       grepl("npr.org", harm_final_temp_kav$URL) |
                        grepl("slate.com", harm_final_temp_kav$URL) |
                        grepl("huffingtonpost.com", harm_final_temp_kav$URL) |
                        grepl("politico.com", harm_final_temp_kav$URL)] = "Liberal"
                              
harm_final_temp_kav$lean[grepl("breitbart.com", harm_final_temp_kav$URL) |
                       grepl("foxnews.com", harm_final_temp_kav$URL) |
                       grepl("rushlimbaugh.com", harm_final_temp_kav$URL) |
                       grepl("theblaze.com", harm_final_temp_kav$URL) |
                       grepl("hannity.com", harm_final_temp_kav$URL)] = "Conservative"

harm_final_temp_kav$Source[grepl("nytimes.com", harm_final_temp_kav$URL)] = "NY Times"
harm_final_temp_kav$Source[grepl("npr.org", harm_final_temp_kav$URL)] = "NPR"
harm_final_temp_kav$Source[grepl("slate.com", harm_final_temp_kav$URL)] = "Slate"
harm_final_temp_kav$Source[grepl("huffingtonpost.com", harm_final_temp_kav$URL)] = "Huffington Post"
harm_final_temp_kav$Source[grepl("politico.com", harm_final_temp_kav$URL)] = "Politico"
harm_final_temp_kav$Source[grepl("foxnews.com", harm_final_temp_kav$URL)] = "Fox"
harm_final_temp_kav$Source[grepl("breitbart.com", harm_final_temp_kav$URL)] = "Breitbart"
harm_final_temp_kav$Source[grepl("rushlimbaugh.com", harm_final_temp_kav$URL)] = "Rush Limbaugh"
harm_final_temp_kav$Source[grepl("theblaze.com", harm_final_temp_kav$URL)] = "The Blaze"
harm_final_temp_kav$Source[grepl("hannity.com", harm_final_temp_kav$URL)] = "Sean Hannity"
harm_model_kav = lme(MFDweight ~ lean, 
                 data = harm_final_temp_kav, 
                 method = "ML",
                 na.action = "na.omit",
                 random = list(~1|Source))
summary(harm_model_kav)
r.squaredGLMM(harm_model_kav)
```

```{r analysis_fair_kav, include = FALSE}
#final_data clean up
#multiply by frequency and create percentage
fair_final_data_kav$weight = fair_final_data_kav$Freq * fair_final_data_kav$valence / fair_final_data_kav$wordcount * 100

#flatten down to sum by article 
fair_final_temp_kav = aggregate(fair_final_data_kav$weight, 
          by = list(fair_final_data_kav$URL),
          FUN = sum, na.rm = T)
colnames(fair_final_temp_kav) = c("URL", "MFDweight")

fair_final_temp_kav$lean[grepl("nytimes.com", fair_final_temp_kav$URL) |
                       grepl("npr.org", fair_final_temp_kav$URL) |
                        grepl("slate.com", fair_final_temp_kav$URL) |
                        grepl("huffingtonpost.com", fair_final_temp_kav$URL) |
                        grepl("politico.com", fair_final_temp_kav$URL)] = "Liberal"
                              
fair_final_temp_kav$lean[grepl("breitbart.com", fair_final_temp_kav$URL) |
                       grepl("foxnews.com", fair_final_temp_kav$URL) |
                       grepl("rushlimbaugh.com", fair_final_temp_kav$URL) |
                       grepl("theblaze.com", fair_final_temp_kav$URL) |
                       grepl("hannity.com", fair_final_temp_kav$URL)] = "Conservative"

fair_final_temp_kav$Source[grepl("nytimes.com", fair_final_temp_kav$URL)] = "NY Times"
fair_final_temp_kav$Source[grepl("npr.org", fair_final_temp_kav$URL)] = "NPR"
fair_final_temp_kav$Source[grepl("slate.com", fair_final_temp_kav$URL)] = "Slate"
fair_final_temp_kav$Source[grepl("huffingtonpost.com", fair_final_temp_kav$URL)] = "Huffington Post"
fair_final_temp_kav$Source[grepl("politico.com", fair_final_temp_kav$URL)] = "Politico"
fair_final_temp_kav$Source[grepl("foxnews.com", fair_final_temp_kav$URL)] = "Fox"
fair_final_temp_kav$Source[grepl("breitbart.com", fair_final_temp_kav$URL)] = "Breitbart"
fair_final_temp_kav$Source[grepl("rushlimbaugh.com", fair_final_temp_kav$URL)] = "Rush Limbaugh"
fair_final_temp_kav$Source[grepl("theblaze.com", fair_final_temp_kav$URL)] = "The Blaze"
fair_final_temp_kav$Source[grepl("hannity.com", fair_final_temp_kav$URL)] = "Sean Hannity"
fair_model_kav = lme(MFDweight ~ lean, 
                 data = fair_final_temp_kav, 
                 method = "ML",
                 na.action = "na.omit",
                 random = list(~1|Source))
summary(fair_model_kav)
```

```{r analysis_ingroup_kav, include = FALSE}
#final_data clean up
#multiply by frequency and create percentage
ingroup_final_data_kav$weight = ingroup_final_data_kav$Freq * ingroup_final_data_kav$valence / ingroup_final_data_kav$wordcount * 100

#flatten down to sum by article 
ingroup_final_temp_kav = aggregate(ingroup_final_data_kav$weight, 
          by = list(ingroup_final_data_kav$URL),
          FUN = sum, na.rm = T)
colnames(ingroup_final_temp_kav) = c("URL", "MFDweight")

ingroup_final_temp_kav$lean[grepl("nytimes.com", ingroup_final_temp_kav$URL) |
                       grepl("npr.org", ingroup_final_temp_kav$URL) |
                        grepl("slate.com", ingroup_final_temp_kav$URL) |
                        grepl("huffingtonpost.com", ingroup_final_temp_kav$URL) |
                        grepl("politico.com", ingroup_final_temp_kav$URL)] = "Liberal"
                              
ingroup_final_temp_kav$lean[grepl("breitbart.com", ingroup_final_temp_kav$URL) |
                       grepl("foxnews.com", ingroup_final_temp_kav$URL) |
                       grepl("rushlimbaugh.com", ingroup_final_temp_kav$URL) |
                       grepl("theblaze.com", ingroup_final_temp_kav$URL) |
                       grepl("hannity.com", ingroup_final_temp_kav$URL)] = "Conservative"

ingroup_final_temp_kav$Source[grepl("nytimes.com", ingroup_final_temp_kav$URL)] = "NY Times"
ingroup_final_temp_kav$Source[grepl("npr.org", ingroup_final_temp_kav$URL)] = "NPR"
ingroup_final_temp_kav$Source[grepl("slate.com", ingroup_final_temp_kav$URL)] = "Slate"
ingroup_final_temp_kav$Source[grepl("huffingtonpost.com", ingroup_final_temp_kav$URL)] = "Huffington Post"
ingroup_final_temp_kav$Source[grepl("politico.com", ingroup_final_temp_kav$URL)] = "Politico"
ingroup_final_temp_kav$Source[grepl("foxnews.com", ingroup_final_temp_kav$URL)] = "Fox"
ingroup_final_temp_kav$Source[grepl("breitbart.com", ingroup_final_temp_kav$URL)] = "Breitbart"
ingroup_final_temp_kav$Source[grepl("rushlimbaugh.com", ingroup_final_temp_kav$URL)] = "Rush Limbaugh"
ingroup_final_temp_kav$Source[grepl("theblaze.com", ingroup_final_temp_kav$URL)] = "The Blaze"
ingroup_final_temp_kav$Source[grepl("hannity.com", ingroup_final_temp_kav$URL)] = "Sean Hannity"
ingroup_model_kav = lme(MFDweight ~ lean, 
                 data = ingroup_final_temp_kav, 
                 method = "ML",
                 na.action = "na.omit",
                 random = list(~1|Source))
summary(ingroup_model_kav)
```

```{r analysis_authority_kav, include = FALSE}
#final_data clean up
#multiply by frequency and create percentage
authority_final_data_kav$weight = authority_final_data_kav$Freq * authority_final_data_kav$valence / authority_final_data_kav$wordcount * 100

#flatten down to sum by article 
authority_final_temp_kav = aggregate(authority_final_data_kav$weight, 
          by = list(authority_final_data_kav$URL),
          FUN = sum, na.rm = T)
colnames(authority_final_temp_kav) = c("URL", "MFDweight")

authority_final_temp_kav$lean[grepl("nytimes.com", authority_final_temp_kav$URL) |
                       grepl("npr.org", authority_final_temp_kav$URL) |
                        grepl("slate.com", authority_final_temp_kav$URL) |
                        grepl("huffingtonpost.com", authority_final_temp_kav$URL) |
                        grepl("politico.com", authority_final_temp_kav$URL)] = "Liberal"
                              
authority_final_temp_kav$lean[grepl("breitbart.com", authority_final_temp_kav$URL) |
                       grepl("foxnews.com", authority_final_temp_kav$URL) |
                       grepl("rushlimbaugh.com", authority_final_temp_kav$URL) |
                       grepl("theblaze.com", authority_final_temp_kav$URL) |
                       grepl("hannity.com", authority_final_temp_kav$URL)] = "Conservative"

authority_final_temp_kav$Source[grepl("nytimes.com", authority_final_temp_kav$URL)] = "NY Times"
authority_final_temp_kav$Source[grepl("npr.org", authority_final_temp_kav$URL)] = "NPR"
authority_final_temp_kav$Source[grepl("slate.com", authority_final_temp_kav$URL)] = "Slate"
authority_final_temp_kav$Source[grepl("huffingtonpost.com", authority_final_temp_kav$URL)] = "Huffington Post"
authority_final_temp_kav$Source[grepl("politico.com", authority_final_temp_kav$URL)] = "Politico"
authority_final_temp_kav$Source[grepl("foxnews.com", authority_final_temp_kav$URL)] = "Fox"
authority_final_temp_kav$Source[grepl("breitbart.com", authority_final_temp_kav$URL)] = "Breitbart"
authority_final_temp_kav$Source[grepl("rushlimbaugh.com", authority_final_temp_kav$URL)] = "Rush Limbaugh"
authority_final_temp_kav$Source[grepl("theblaze.com", authority_final_temp_kav$URL)] = "The Blaze"
authority_final_temp_kav$Source[grepl("hannity.com", authority_final_temp_kav$URL)] = "Sean Hannity"
authority_model_kav = lme(MFDweight ~ lean, 
                 data = authority_final_temp_kav, 
                 method = "ML",
                 na.action = "na.omit",
                 random = list(~1|Source))
summary(authority_model_kav)
```

```{r analysis_purity_kav, include = FALSE}
#final_data clean up
#multiply by frequency and create percentage
purity_final_data_kav$weight = purity_final_data_kav$Freq * purity_final_data_kav$valence / purity_final_data_kav$wordcount * 100

#flatten down to sum by article 
purity_final_temp_kav = aggregate(purity_final_data_kav$weight, 
          by = list(purity_final_data_kav$URL),
          FUN = sum, na.rm = T)
colnames(purity_final_temp_kav) = c("URL", "MFDweight")

purity_final_temp_kav$lean[grepl("nytimes.com", purity_final_temp_kav$URL) |
                       grepl("npr.org", purity_final_temp_kav$URL) |
                        grepl("slate.com", purity_final_temp_kav$URL) |
                        grepl("huffingtonpost.com", purity_final_temp_kav$URL) |
                        grepl("politico.com", purity_final_temp_kav$URL)] = "Liberal"
                              
purity_final_temp_kav$lean[grepl("breitbart.com", purity_final_temp_kav$URL) |
                       grepl("foxnews.com", purity_final_temp_kav$URL) |
                       grepl("rushlimbaugh.com", purity_final_temp_kav$URL) |
                       grepl("theblaze.com", purity_final_temp_kav$URL) |
                       grepl("hannity.com", purity_final_temp_kav$URL)] = "Conservative"

purity_final_temp_kav$Source[grepl("nytimes.com", purity_final_temp_kav$URL)] = "NY Times"
purity_final_temp_kav$Source[grepl("npr.org", purity_final_temp_kav$URL)] = "NPR"
purity_final_temp_kav$Source[grepl("slate.com", purity_final_temp_kav$URL)] = "Slate"
purity_final_temp_kav$Source[grepl("huffingtonpost.com", purity_final_temp_kav$URL)] = "Huffington Post"
purity_final_temp_kav$Source[grepl("politico.com", purity_final_temp_kav$URL)] = "Politico"
purity_final_temp_kav$Source[grepl("foxnews.com", purity_final_temp_kav$URL)] = "Fox"
purity_final_temp_kav$Source[grepl("breitbart.com", purity_final_temp_kav$URL)] = "Breitbart"
purity_final_temp_kav$Source[grepl("rushlimbaugh.com", purity_final_temp_kav$URL)] = "Rush Limbaugh"
purity_final_temp_kav$Source[grepl("theblaze.com", purity_final_temp_kav$URL)] = "The Blaze"
purity_final_temp_kav$Source[grepl("hannity.com", purity_final_temp_kav$URL)] = "Sean Hannity"
purity_model_kav = lme(MFDweight ~ lean, 
                 data = purity_final_temp_kav, 
                 method = "ML",
                 na.action = "na.omit",
                 random = list(~1|Source))
summary(purity_model_kav)
```

```{r analysis_harm_gs, include = FALSE}
#final_data clean up
#multiply by frequency and create percentage
harm_final_data_gs$weight = harm_final_data_gs$Freq * harm_final_data_gs$valence / harm_final_data_gs$wordcount * 100

#flatten down to sum by article 
harm_final_temp_gs = aggregate(harm_final_data_gs$weight, 
          by = list(harm_final_data_gs$URL),
          FUN = sum, na.rm = T)
colnames(harm_final_temp_gs) = c("URL", "MFDweight")

harm_final_temp_gs$lean[grepl("nytimes.com", harm_final_temp_gs$URL) |
                       grepl("npr.org", harm_final_temp_gs$URL) |
                        grepl("slate.com", harm_final_temp_gs$URL) |
                        grepl("huffingtonpost.com", harm_final_temp_gs$URL) |
                        grepl("politico.com", harm_final_temp_gs$URL)] = "Liberal"
                              
harm_final_temp_gs$lean[grepl("breitbart.com", harm_final_temp_gs$URL) |
                       grepl("foxnews.com", harm_final_temp_gs$URL) |
                       grepl("rushlimbaugh.com", harm_final_temp_gs$URL) |
                       grepl("theblaze.com", harm_final_temp_gs$URL) |
                       grepl("hannity.com", harm_final_temp_gs$URL)] = "Conservative"

harm_final_temp_gs$Source[grepl("nytimes.com", harm_final_temp_gs$URL)] = "NY Times"
harm_final_temp_gs$Source[grepl("npr.org", harm_final_temp_gs$URL)] = "NPR"
harm_final_temp_gs$Source[grepl("slate.com", harm_final_temp_gs$URL)] = "Slate"
harm_final_temp_gs$Source[grepl("huffingtonpost.com", harm_final_temp_gs$URL)] = "Huffington Post"
harm_final_temp_gs$Source[grepl("politico.com", harm_final_temp_gs$URL)] = "Politico"
harm_final_temp_gs$Source[grepl("foxnews.com", harm_final_temp_gs$URL)] = "Fox"
harm_final_temp_gs$Source[grepl("breitbart.com", harm_final_temp_gs$URL)] = "Breitbart"
harm_final_temp_gs$Source[grepl("rushlimbaugh.com", harm_final_temp_gs$URL)] = "Rush Limbaugh"
harm_final_temp_gs$Source[grepl("theblaze.com", harm_final_temp_gs$URL)] = "The Blaze"
harm_final_temp_gs$Source[grepl("hannity.com", harm_final_temp_gs$URL)] = "Sean Hannity"
harm_model_gs = lme(MFDweight ~ lean, 
                 data = harm_final_temp_gs, 
                 method = "ML",
                 na.action = "na.omit",
                 random = list(~1|Source))
summary(harm_model_gs)
```

```{r analysis_fair_gs, include = FALSE}
#final_data clean up
#multiply by frequency and create percentage
fair_final_data_gs$weight = fair_final_data_gs$Freq * fair_final_data_gs$valence / fair_final_data_gs$wordcount * 100

#flatten down to sum by article 
fair_final_temp_gs = aggregate(fair_final_data_gs$weight, 
          by = list(fair_final_data_gs$URL),
          FUN = sum, na.rm = T)
colnames(fair_final_temp_gs) = c("URL", "MFDweight")

fair_final_temp_gs$lean[grepl("nytimes.com", fair_final_temp_gs$URL) |
                       grepl("npr.org", fair_final_temp_gs$URL) |
                        grepl("slate.com", fair_final_temp_gs$URL) |
                        grepl("huffingtonpost.com", fair_final_temp_gs$URL) |
                        grepl("politico.com", fair_final_temp_gs$URL)] = "Liberal"
                              
fair_final_temp_gs$lean[grepl("breitbart.com", fair_final_temp_gs$URL) |
                       grepl("foxnews.com", fair_final_temp_gs$URL) |
                       grepl("rushlimbaugh.com", fair_final_temp_gs$URL) |
                       grepl("theblaze.com", fair_final_temp_gs$URL) |
                       grepl("hannity.com", fair_final_temp_gs$URL)] = "Conservative"

fair_final_temp_gs$Source[grepl("nytimes.com", fair_final_temp_gs$URL)] = "NY Times"
fair_final_temp_gs$Source[grepl("npr.org", fair_final_temp_gs$URL)] = "NPR"
fair_final_temp_gs$Source[grepl("slate.com", fair_final_temp_gs$URL)] = "Slate"
fair_final_temp_gs$Source[grepl("huffingtonpost.com", fair_final_temp_gs$URL)] = "Huffington Post"
fair_final_temp_gs$Source[grepl("politico.com", fair_final_temp_gs$URL)] = "Politico"
fair_final_temp_gs$Source[grepl("foxnews.com", fair_final_temp_gs$URL)] = "Fox"
fair_final_temp_gs$Source[grepl("breitbart.com", fair_final_temp_gs$URL)] = "Breitbart"
fair_final_temp_gs$Source[grepl("rushlimbaugh.com", fair_final_temp_gs$URL)] = "Rush Limbaugh"
fair_final_temp_gs$Source[grepl("theblaze.com", fair_final_temp_gs$URL)] = "The Blaze"
fair_final_temp_gs$Source[grepl("hannity.com", fair_final_temp_gs$URL)] = "Sean Hannity"
fair_model_gs = lme(MFDweight ~ lean, 
                 data = fair_final_temp_gs, 
                 method = "ML",
                 na.action = "na.omit",
                 random = list(~1|Source))
summary(fair_model_gs)
```

```{r analysis_ingroup_gs, include = FALSE}
#final_data clean up
#multiply by frequency and create percentage
ingroup_final_data_gs$weight = ingroup_final_data_gs$Freq * ingroup_final_data_gs$valence / ingroup_final_data_gs$wordcount * 100

#flatten down to sum by article 
ingroup_final_temp_gs = aggregate(ingroup_final_data_gs$weight, 
          by = list(ingroup_final_data_gs$URL),
          FUN = sum, na.rm = T)
colnames(ingroup_final_temp_gs) = c("URL", "MFDweight")

ingroup_final_temp_gs$lean[grepl("nytimes.com", ingroup_final_temp_gs$URL) |
                       grepl("npr.org", ingroup_final_temp_gs$URL) |
                        grepl("slate.com", ingroup_final_temp_gs$URL) |
                        grepl("huffingtonpost.com", ingroup_final_temp_gs$URL) |
                        grepl("politico.com", ingroup_final_temp_gs$URL)] = "Liberal"
                              
ingroup_final_temp_gs$lean[grepl("breitbart.com", ingroup_final_temp_gs$URL) |
                       grepl("foxnews.com", ingroup_final_temp_gs$URL) |
                       grepl("rushlimbaugh.com", ingroup_final_temp_gs$URL) |
                       grepl("theblaze.com", ingroup_final_temp_gs$URL) |
                       grepl("hannity.com", ingroup_final_temp_gs$URL)] = "Conservative"

ingroup_final_temp_gs$Source[grepl("nytimes.com", ingroup_final_temp_gs$URL)] = "NY Times"
ingroup_final_temp_gs$Source[grepl("npr.org", ingroup_final_temp_gs$URL)] = "NPR"
ingroup_final_temp_gs$Source[grepl("slate.com", ingroup_final_temp_gs$URL)] = "Slate"
ingroup_final_temp_gs$Source[grepl("huffingtonpost.com", ingroup_final_temp_gs$URL)] = "Huffington Post"
ingroup_final_temp_gs$Source[grepl("politico.com", ingroup_final_temp_gs$URL)] = "Politico"
ingroup_final_temp_gs$Source[grepl("foxnews.com", ingroup_final_temp_gs$URL)] = "Fox"
ingroup_final_temp_gs$Source[grepl("breitbart.com", ingroup_final_temp_gs$URL)] = "Breitbart"
ingroup_final_temp_gs$Source[grepl("rushlimbaugh.com", ingroup_final_temp_gs$URL)] = "Rush Limbaugh"
ingroup_final_temp_gs$Source[grepl("theblaze.com", ingroup_final_temp_gs$URL)] = "The Blaze"
ingroup_final_temp_gs$Source[grepl("hannity.com", ingroup_final_temp_gs$URL)] = "Sean Hannity"
ingroup_model_gs = lme(MFDweight ~ lean, 
                 data = ingroup_final_temp_gs, 
                 method = "ML",
                 na.action = "na.omit",
                 random = list(~1|Source))
summary(ingroup_model_gs)
```

```{r analysis_authority_gs, include = FALSE}
#final_data clean up
#multiply by frequency and create percentage
authority_final_data_gs$weight = authority_final_data_gs$Freq * authority_final_data_gs$valence / authority_final_data_gs$wordcount * 100

#flatten down to sum by article 
authority_final_temp_gs = aggregate(authority_final_data_gs$weight, 
          by = list(authority_final_data_gs$URL),
          FUN = sum, na.rm = T)
colnames(authority_final_temp_gs) = c("URL", "MFDweight")

authority_final_temp_gs$lean[grepl("nytimes.com", authority_final_temp_gs$URL) |
                       grepl("npr.org", authority_final_temp_gs$URL) |
                        grepl("slate.com", authority_final_temp_gs$URL) |
                        grepl("huffingtonpost.com", authority_final_temp_gs$URL) |
                        grepl("politico.com", authority_final_temp_gs$URL)] = "Liberal"
                              
authority_final_temp_gs$lean[grepl("breitbart.com", authority_final_temp_gs$URL) |
                       grepl("foxnews.com", authority_final_temp_gs$URL) |
                       grepl("rushlimbaugh.com", authority_final_temp_gs$URL) |
                       grepl("theblaze.com", authority_final_temp_gs$URL) |
                       grepl("hannity.com", authority_final_temp_gs$URL)] = "Conservative"

authority_final_temp_gs$Source[grepl("nytimes.com", authority_final_temp_gs$URL)] = "NY Times"
authority_final_temp_gs$Source[grepl("npr.org", authority_final_temp_gs$URL)] = "NPR"
authority_final_temp_gs$Source[grepl("slate.com", authority_final_temp_gs$URL)] = "Slate"
authority_final_temp_gs$Source[grepl("huffingtonpost.com", authority_final_temp_gs$URL)] = "Huffington Post"
authority_final_temp_gs$Source[grepl("politico.com", authority_final_temp_gs$URL)] = "Politico"
authority_final_temp_gs$Source[grepl("foxnews.com", authority_final_temp_gs$URL)] = "Fox"
authority_final_temp_gs$Source[grepl("breitbart.com", authority_final_temp_gs$URL)] = "Breitbart"
authority_final_temp_gs$Source[grepl("rushlimbaugh.com", authority_final_temp_gs$URL)] = "Rush Limbaugh"
authority_final_temp_gs$Source[grepl("theblaze.com", authority_final_temp_gs$URL)] = "The Blaze"
authority_final_temp_gs$Source[grepl("hannity.com", authority_final_temp_gs$URL)] = "Sean Hannity"
authority_model_gs = lme(MFDweight ~ lean, 
                 data = authority_final_temp_gs, 
                 method = "ML",
                 na.action = "na.omit",
                 random = list(~1|Source))
summary(authority_model_gs)
```

```{r analysis_purity_gs, include = FALSE}
#final_data clean up
#multiply by frequency and create percentage
purity_final_data_gs$weight = purity_final_data_gs$Freq * purity_final_data_gs$valence / purity_final_data_gs$wordcount * 100

#flatten down to sum by article 
purity_final_temp_gs = aggregate(purity_final_data_gs$weight, 
          by = list(purity_final_data_gs$URL),
          FUN = sum, na.rm = T)
colnames(purity_final_temp_gs) = c("URL", "MFDweight")

purity_final_temp_gs$lean[grepl("nytimes.com", purity_final_temp_gs$URL) |
                       grepl("npr.org", purity_final_temp_gs$URL) |
                        grepl("slate.com", purity_final_temp_gs$URL) |
                        grepl("huffingtonpost.com", purity_final_temp_gs$URL) |
                        grepl("politico.com", purity_final_temp_gs$URL)] = "Liberal"
                              
purity_final_temp_gs$lean[grepl("breitbart.com", purity_final_temp_gs$URL) |
                       grepl("foxnews.com", purity_final_temp_gs$URL) |
                       grepl("rushlimbaugh.com", purity_final_temp_gs$URL) |
                       grepl("theblaze.com", purity_final_temp_gs$URL) |
                       grepl("hannity.com", purity_final_temp_gs$URL)] = "Conservative"

purity_final_temp_gs$Source[grepl("nytimes.com", purity_final_temp_gs$URL)] = "NY Times"
purity_final_temp_gs$Source[grepl("npr.org", purity_final_temp_gs$URL)] = "NPR"
purity_final_temp_gs$Source[grepl("slate.com", purity_final_temp_gs$URL)] = "Slate"
purity_final_temp_gs$Source[grepl("huffingtonpost.com", purity_final_temp_gs$URL)] = "Huffington Post"
purity_final_temp_gs$Source[grepl("politico.com", purity_final_temp_gs$URL)] = "Politico"
purity_final_temp_gs$Source[grepl("foxnews.com", purity_final_temp_gs$URL)] = "Fox"
purity_final_temp_gs$Source[grepl("breitbart.com", purity_final_temp_gs$URL)] = "Breitbart"
purity_final_temp_gs$Source[grepl("rushlimbaugh.com", purity_final_temp_gs$URL)] = "Rush Limbaugh"
purity_final_temp_gs$Source[grepl("theblaze.com", purity_final_temp_gs$URL)] = "The Blaze"
purity_final_temp_gs$Source[grepl("hannity.com", purity_final_temp_gs$URL)] = "Sean Hannity"
purity_model_gs = lme(MFDweight ~ lean, 
                 data = purity_final_temp_gs, 
                 method = "ML",
                 na.action = "na.omit",
                 random = list(~1|Source))
summary(purity_model_gs)
```

# Results

## Descriptive Statistics

The researchers calculated descriptive statistics for each news source per topic in order to reveal the presence (if any) of linguistic differences in the sources' use of language. As in Experiment 1, statistics calculated include *z*-scored valence, number of articles per source, total words per source, mean tokens per article in each source, mean types per article in each source, and mean readability level (using the Flesch-Kincaid Grade Level Readability formula) per source [@Kincaid1975].

Table \@ref(tab:exp2-source-descriptives-kav) displays the descriptive statistics for sources' writing on the Kavanaugh confirmation hearing. The sources were similar in most basic linguistic aspects, except for number of articles. For example, *Sean Hannity* appears to have published only 27 articles while *Breitbart* published 757 articles on this topic. Valence was found to be slightly positive across all sources. *Fox News* produced the most total words with the most tokens on average. This is likely due to the fact *Fox News* transcribes many of their videos and publishes them in article form. *Politico* featured the highest number of types on average. *Rush Limbaugh* featured the lowest readability score on average by grade level while *Slate* featured the highest grade-level readability score. The large standard deviations for these statistics, however, preclude conclusions regarding differences in the sources' use of language, as there is likely a lot of overlap between sources' use of language. 

Table \@ref(tab:exp2-source-descriptives-gs) displays descriptive statistics for articles about the partial government shutdown. Like the Kavanaugh hearing, the sources were similar in average valence (slightly positive). Once again, there was variation in the number of articles published by each source on this topic. *Sean Hannity*, *Rush Limbaugh*, and *The Blaze* published fewer than 100 articles while *Fox News* published 1,013 articles. *Fox News* again featured the most total words and mean tokens, but this is likely due to to the presence of a large amount of video transcriptions that the organization published as articles. *Politico* had the most types on average. For this topic, *Fox News* featured the lowest reading grade level while *Slate* featured the highest reading grade level. For each statistic, the excessively high standard deviations render any assertions regarding linguistic differences inconclusive on a descriptive level due to the aforementioned overlap in sources' language use. 


```{r exp2-source-descriptives-kav, results = 'asis', echo = FALSE, message = F}
#Kav
final_kav$uniquewords = NA
final_kav$valenceavg = NA

for (i in 1:nrow(final_kav)){
  
  #add unique
  final_kav$uniquewords[i] = length(unique(unlist(strsplit(final_kav$stemmed[i], " "))))
  
  #add valence
  tempwords_kav = as.data.frame(unlist(strsplit(final_kav$stemmed[i], " ")))
  colnames(tempwords_kav) = "theword"
  tempwords_kav$tempval = vlookup(tempwords_kav$theword, #index in original data
                    warriner, #where's the stuff you want to put in
                    "V.Mean.Sum", #what column is the stuff you want
                    "Word2") #what's the matching index
  
  final_kav$valenceavg[i] = mean(tempwords_kav$tempval, na.rm = T)

}


# tapply(final_kav$valenceavg, final_kav$Source, mean)
# tapply(final_kav$valenceavg, final_kav$Source, sd)
# tapply(final_kav$wordcount, final_kav$Source, mean)
# tapply(final_kav$wordcount, final_kav$Source, sd)
# tapply(final_kav$uniquewords, final_kav$Source, mean)
# tapply(final_kav$uniquewords, final_kav$Source, sd)
# tapply(final_kav$Source, final_kav$Source, length)
# tapply(final_kav$wordcount, final_kav$Source, sum)

library(quanteda)

final_kav$FK = NA

for (i in 1:nrow(final_kav)){
  final_kav$FK[i] = unlist(textstat_readability(final_kav$Text[i], 
                     measure = "Flesch.Kincaid")$Flesch.Kincaid)
}

# tapply(final_kav$FK, final_kav$Source, mean)
# tapply(final_kav$FK, final_kav$Source, sd)

#Kav Table
####build the table here####
tableprint = matrix(NA, nrow=10, ncol=11)
colnames(tableprint) = c("Source", "Mean_Valence", "SD_Valence", "Number_of_Articles", "Total_Words", "Mean_Tokens", "SD_Tokens", "Mean_Types", "SD_Types", "Mean_Readability", "SD_Readability")

#tableprint[ , 1] = unique(final_kav$Source)
tableprint[ , 1] = c("The Blaze", "Breitbart", "Fox News", "Sean Hannity", 
                     "Huffington Post", "NPR", "New York Times", "Politico",
                     "Rush Limbaugh", "Slate")
tableprint[ , 2] = tapply(final_kav$valenceavg, final_kav$Source, mean, na.rm = T)
tableprint[ , 3] = tapply(final_kav$valenceavg, final_kav$Source, sd, na.rm = T)
tableprint[ , 4] = tapply(final_kav$valenceavg, final_kav$Source, length)
tableprint[ , 5] = tapply(final_kav$wordcount, final_kav$Source, sum, na.rm = T)
tableprint[ , 6] = tapply(final_kav$wordcount, final_kav$Source, mean, na.rm = T)
tableprint[ , 7] = tapply(final_kav$wordcount, final_kav$Source, sd)
tableprint[ , 8] = tapply(final_kav$uniquewords, final_kav$Source, mean, na.rm = T)
tableprint[ , 9] = tapply(final_kav$uniquewords, final_kav$Source, sd, na.rm = T)
tableprint[ , 10] = tapply(final_kav$FK, final_kav$Source, mean, na.rm = T)
tableprint[ , 11] = tapply(final_kav$FK, final_kav$Source, sd, na.rm = T)

tableprint[ , c(2,3,6,7,8,9,10,11)] = printnum(as.numeric(tableprint[ , c(2,3,6,7,8,9,10,11)]))

tableprint = tableprint[c(5, 6, 7, 8, 10, 1, 2, 3, 4, 9), ]

#change this to match
apa_table(as.data.frame(tableprint),
          note = "Readability statistics were calculated using the Flesch-Kincaid Grade Level readability formula. V = Valence, T = Tokens or total words, Ty = Types or unique words, FK = Flesch-Kincaid",
          caption = "Kavanaugh - Descriptive Statistics by Source", 
          col.names = c("Source", "$M_V$", "$SD_V$", "$N_{Article}$", "$N_{Words}$", "$M_T$", "$SD_T$", "$M_{Ty}$", "$SD_{Ty}$", "$M_{FK}$", "$SD_{FK}$"),
          format = "latex",
          escape = F,
          small = T,
          align = c("l", rep("c", 6))
          )
```

```{r exp2-source-descriptives-gs, results = 'asis', echo = FALSE, message = F}
#GS
final_gs$uniquewords = NA
final_gs$valenceavg = NA

for (i in 1:nrow(final_gs)){
  
  #add unique
  final_gs$uniquewords[i] = length(unique(unlist(strsplit(final_gs$stemmed[i], " "))))
  
  #add valence
  tempwords_gs = as.data.frame(unlist(strsplit(final_gs$stemmed[i], " ")))
  colnames(tempwords_gs) = "theword"
  tempwords_gs$tempval = vlookup(tempwords_gs$theword, #index in original data
                    warriner, #where's the stuff you want to put in
                    "V.Mean.Sum", #what column is the stuff you want
                    "Word2") #what's the matching index
  
  final_gs$valenceavg[i] = mean(tempwords_gs$tempval, na.rm = T)

}


# tapply(final_gs$valenceavg, final_gs$Source, mean)
# tapply(final_gs$valenceavg, final_gs$Source, sd)
# tapply(final_gs$wordcount, final_gs$Source, mean)
# tapply(final_gs$wordcount, final_gs$Source, sd)
# tapply(final_gs$uniquewords, final_gs$Source, mean)
# tapply(final_gs$uniquewords, final_gs$Source, sd)
# tapply(final_gs$Source, final_gs$Source, length)
# tapply(final_gs$wordcount, final_gs$Source, sum)

library(quanteda)

final_gs$FK = NA

for (i in 1:nrow(final_gs)){
  final_gs$FK[i] = unlist(textstat_readability(final_gs$Text[i], 
                     measure = "Flesch.Kincaid")$Flesch.Kincaid)
}

# tapply(final_gs$FK, final_gs$Source, mean)
# tapply(final_gs$FK, final_gs$Source, sd)

#GS Table
####build the table here####
tableprint = matrix(NA, nrow=10, ncol=11)
colnames(tableprint) = c("Source", "Mean_Valence", "SD_Valence", "Number_of_Articles", "Total_Words", "Mean_Tokens", "SD_Tokens", "Mean_Types", "SD_Types", "Mean_Readability", "SD_Readability")

#tableprint[ , 1] = unique(final_gs$Source)
tableprint[ , 1] = c("The Blaze", "Breitbart", "Fox News", "Sean Hannity", 
                     "Huffington Post", "NPR", "New York Times", "Politico",
                     "Rush Limbaugh", "Slate")
tableprint[ , 2] = tapply(final_gs$valenceavg, final_gs$Source, mean)
tableprint[ , 3] = tapply(final_gs$valenceavg, final_gs$Source, sd)
tableprint[ , 4] = tapply(final_gs$valenceavg, final_gs$Source, length)
tableprint[ , 5] = tapply(final_gs$wordcount, final_gs$Source, sum)
tableprint[ , 6] = tapply(final_gs$wordcount, final_gs$Source, mean)
tableprint[ , 7] = tapply(final_gs$wordcount, final_gs$Source, sd)
tableprint[ , 8] = tapply(final_gs$uniquewords, final_gs$Source, mean)
tableprint[ , 9] = tapply(final_gs$uniquewords, final_gs$Source, sd)
tableprint[ , 10] = tapply(final_gs$FK, final_gs$Source, mean)
tableprint[ , 11] = tapply(final_gs$FK, final_gs$Source, sd)

tableprint[ , c(2,3,6,7,8,9,10,11)] = printnum(as.numeric(tableprint[ , c(2,3,6,7,8,9,10,11)]))

tableprint = tableprint[c(5, 6, 7, 8, 10, 1, 2, 3, 4, 9), ]

#change this to match
apa_table(as.data.frame(tableprint),
          note = "Readability statistics were calculated using the Flesch-Kincaid Grade Level readability formula. V = Valence, T = Tokens or total words, Ty = Types or unique words, FK = Flesch-Kincaid",
          caption = "Government Shutdown - Descriptive Statistics by Source", 
          col.names = c("Source", "$M_V$", "$SD_V$", "$N_{Article}$", "$N_{Words}$", "$M_T$", "$SD_T$", "$M_{Ty}$", "$SD_{Ty}$", "$M_{FK}$", "$SD_{FK}$"),
          format = "latex",
          escape = F,
          small = T, 
          align = c("l", rep("c", 6))
          )
```

## Inferential Statistics

To analyze if news sources adhered to differences in word use based on their target audience, the researchers utilized a multilevel model (MLM) to analyze if news sources leveraged different vocabularies based on target audience. The researchers used the *nlme* library in *R* [@Pinheiro2017]. Each foundation's weighted percentage was predicted by the source's political lean, using the individual source as a random intercept to control for the nested data structure. Two separate MLMs were constructed from datasets compiled for each topic of interest: the Kavanaugh hearing and the partial government shutdown of 2018-2019.

For the Kavanaugh topic, the multilevel model indicated the presence of a statistically significant effect for *harm/care*, but the practical effect denoted by Cohen's *d* was found to be small. There were no other significant or practical effects of political lean for any of the other four moral foundations. The effect for *harm/care* was in the hypothesized direction with liberal sources tending to use more positively *harm/care* words than conservative sources. Descriptive and test statistics, *p*-values and effect sizes (Cohen's *d*) can be found in Table \@ref(tab:exp2-tablekav). 

For news articles about the partial U.S. Federal Government Shutdown of 2018-2019, there were no significant or practical effects of political lean for the moral foundations. A small-to-medium effect size was observed for *authority/respect*. The effect was in the predicted direction as well, as conservative sources tended to offer more positive endorsements of the foundation than liberal sources. This effect was similar to Experiment 1 in which the largest effect size was observed for *authority/respect*. As noted before, the effect found in Experiment 1 was in the opposite direction as what was hypothesized. Thus, it is difficult to draw comparisons between the two studies despite the similar pattern for effect size. Owing to a lack of similar effects for either Experiment 1 or the Kavanaugh topic, there is doubt as to whether or not a practical or generalized effect exists for *authority/respect*. 

Based on the weighted percent values for the five foundations applied to both topics, MFD words seem to make up little of the article text. A similar pattern was observed for the results in Experiment 1. As in Experiment 1, the percentages and means seem to indicate a generally positive endorsement of all five moral foundations across both political leanings.

```{r exp2-tablekav, results = 'asis', echo = F}
#MFD, mean, sd, mean, sd, t, p, d
tableprint_kav = matrix(NA, ncol = 8, nrow = 5)
colnames(tableprint_kav) = c("Foundation", "M_C", "SD_C", "M_L", "SD_L", "t", "p", "d")

#calculate d
#harm
harm_d_kav = d.ind.t(m1 = mean(harm_final_temp_kav$MFDweight[harm_final_temp_kav$lean == "Conservative"], na.rm = T), 
                 m2 = mean(harm_final_temp_kav$MFDweight[harm_final_temp_kav$lean == "Liberal"], na.rm = T),
                 sd1 = sd(harm_final_temp_kav$MFDweight[harm_final_temp_kav$lean == "Conservative"], na.rm = T),
                 sd2 = sd(harm_final_temp_kav$MFDweight[harm_final_temp_kav$lean == "Liberal"], na.rm = T),
                 n1 = length(harm_final_temp_kav$MFDweight[harm_final_temp_kav$lean == "Conservative"]),
                 n2 = length(harm_final_temp_kav$MFDweight[harm_final_temp_kav$lean == "Liberal"]),
                 a = .05)

#fair
fair_d_kav = d.ind.t(m1 = mean(fair_final_temp_kav$MFDweight[fair_final_temp_kav$lean == "Conservative"], na.rm = T), 
                 m2 = mean(fair_final_temp_kav$MFDweight[fair_final_temp_kav$lean == "Liberal"], na.rm = T),
                 sd1 = sd(fair_final_temp_kav$MFDweight[fair_final_temp_kav$lean == "Conservative"], na.rm = T),
                 sd2 = sd(fair_final_temp_kav$MFDweight[fair_final_temp_kav$lean == "Liberal"], na.rm = T),
                 n1 = length(fair_final_temp_kav$MFDweight[fair_final_temp_kav$lean == "Conservative"]),
                 n2 = length(fair_final_temp_kav$MFDweight[fair_final_temp_kav$lean == "Liberal"]),
                 a = .05)

#ingroup
ingroup_d_kav = d.ind.t(m1 = mean(ingroup_final_temp_kav$MFDweight[ingroup_final_temp_kav$lean == "Conservative"], na.rm = T), 
                 m2 = mean(ingroup_final_temp_kav$MFDweight[ingroup_final_temp_kav$lean == "Liberal"], na.rm = T),
                 sd1 = sd(ingroup_final_temp_kav$MFDweight[ingroup_final_temp_kav$lean == "Conservative"], na.rm = T),
                 sd2 = sd(ingroup_final_temp_kav$MFDweight[ingroup_final_temp_kav$lean == "Liberal"], na.rm = T),
                 n1 = length(ingroup_final_temp_kav$MFDweight[ingroup_final_temp_kav$lean == "Conservative"]),
                 n2 = length(ingroup_final_temp_kav$MFDweight[ingroup_final_temp_kav$lean == "Liberal"]),
                 a = .05)

#authority
authority_d_kav = d.ind.t(m1 = mean(authority_final_temp_kav$MFDweight[authority_final_temp_kav$lean == "Conservative"], na.rm = T), 
                 m2 = mean(authority_final_temp_kav$MFDweight[authority_final_temp_kav$lean == "Liberal"], na.rm = T),
                 sd1 = sd(authority_final_temp_kav$MFDweight[authority_final_temp_kav$lean == "Conservative"], na.rm = T),
                 sd2 = sd(authority_final_temp_kav$MFDweight[authority_final_temp_kav$lean == "Liberal"], na.rm = T),
                 n1 = length(authority_final_temp_kav$MFDweight[authority_final_temp_kav$lean == "Conservative"]),
                 n2 = length(authority_final_temp_kav$MFDweight[authority_final_temp_kav$lean == "Liberal"]),
                 a = .05)

#purity
purity_d_kav = d.ind.t(m1 = mean(purity_final_temp_kav$MFDweight[purity_final_temp_kav$lean == "Conservative"], na.rm = T), 
                 m2 = mean(purity_final_temp_kav$MFDweight[purity_final_temp_kav$lean == "Liberal"], na.rm = T),
                 sd1 = sd(purity_final_temp_kav$MFDweight[purity_final_temp_kav$lean == "Conservative"], na.rm = T),
                 sd2 = sd(purity_final_temp_kav$MFDweight[purity_final_temp_kav$lean == "Liberal"], na.rm = T),
                 n1 = length(purity_final_temp_kav$MFDweight[purity_final_temp_kav$lean == "Conservative"]),
                 n2 = length(purity_final_temp_kav$MFDweight[purity_final_temp_kav$lean == "Liberal"]),
                 a = .05)

#put in the numbers
tableprint_kav[1, ] = c("Harm/Care", harm_d_kav$M1, harm_d_kav$sd1, harm_d_kav$M2, harm_d_kav$sd1, 
                    summary(harm_model_kav)$tTable[2,4], summary(harm_model_kav)$tTable[2,5],
                    harm_d_kav$d)
tableprint_kav[2, ] = c("Fairness/Reciprocity", fair_d_kav$M1, fair_d_kav$sd1, fair_d_kav$M2, fair_d_kav$sd1, 
                    summary(fair_model_kav)$tTable[2,4], summary(fair_model_kav)$tTable[2,5],
                    fair_d_kav$d)
tableprint_kav[3, ] = c("Ingroup/Loyalty", ingroup_d_kav$M1, ingroup_d_kav$sd1, ingroup_d_kav$M2, ingroup_d_kav$sd1, 
                    summary(ingroup_model_kav)$tTable[2,4], summary(ingroup_model_kav)$tTable[2,5],
                    ingroup_d_kav$d)
tableprint_kav[4, ] = c("Authority/Respect", authority_d_kav$M1, authority_d_kav$sd1, authority_d_kav$M2, authority_d_kav$sd1, 
                    summary(authority_model_kav)$tTable[2,4], summary(authority_model_kav)$tTable[2,5],
                    authority_d_kav$d)
tableprint_kav[5, ] = c("Purity/Sanctity", purity_d_kav$M1, purity_d_kav$sd1, purity_d_kav$M2, purity_d_kav$sd1, 
                    summary(purity_model_kav)$tTable[2,4], summary(purity_model_kav)$tTable[2,5],
                    purity_d_kav$d)

tableprint_kav[ , c(2:6, 8)] = printnum(as.numeric(tableprint_kav[ , c(2:6, 8)]))
tableprint_kav[ , 7] = printp(as.numeric(tableprint_kav[ , 7]))

apa_table(tableprint_kav,
          note = "For mean and standard deviation values, 'C' and 'L' refer to 'conservative' and 'liberal,' respectively",
          caption = "Kavanaugh Results - Multilevel Model", 
          col.names = c("Foundation", "$M_C$", "$SD_C$", "$M_L$", "$SD_L$", "$t$", "$p$", "$d$"),
          format = "latex",
          escape = F,
          align = c("l", rep("c", 7))
          )
```

```{r exp2-tablegs, results = 'asis', echo = F}
#MFD, mean, sd, mean, sd, t, p, d
tableprint_gs = matrix(NA, ncol = 8, nrow = 5)
colnames(tableprint_gs) = c("Foundation", "M_C", "SD_C", "M_L", "SD_L", "t", "p", "d")

#calculate d
#harm
harm_d_gs = d.ind.t(m1 = mean(harm_final_temp_gs$MFDweight[harm_final_temp_gs$lean == "Conservative"], na.rm = T), 
                 m2 = mean(harm_final_temp_gs$MFDweight[harm_final_temp_gs$lean == "Liberal"], na.rm = T),
                 sd1 = sd(harm_final_temp_gs$MFDweight[harm_final_temp_gs$lean == "Conservative"], na.rm = T),
                 sd2 = sd(harm_final_temp_gs$MFDweight[harm_final_temp_gs$lean == "Liberal"], na.rm = T),
                 n1 = length(harm_final_temp_gs$MFDweight[harm_final_temp_gs$lean == "Conservative"]),
                 n2 = length(harm_final_temp_gs$MFDweight[harm_final_temp_gs$lean == "Liberal"]),
                 a = .05)

#fair
fair_d_gs = d.ind.t(m1 = mean(fair_final_temp_gs$MFDweight[fair_final_temp_gs$lean == "Conservative"], na.rm = T), 
                 m2 = mean(fair_final_temp_gs$MFDweight[fair_final_temp_gs$lean == "Liberal"], na.rm = T),
                 sd1 = sd(fair_final_temp_gs$MFDweight[fair_final_temp_gs$lean == "Conservative"], na.rm = T),
                 sd2 = sd(fair_final_temp_gs$MFDweight[fair_final_temp_gs$lean == "Liberal"], na.rm = T),
                 n1 = length(fair_final_temp_gs$MFDweight[fair_final_temp_gs$lean == "Conservative"]),
                 n2 = length(fair_final_temp_gs$MFDweight[fair_final_temp_gs$lean == "Liberal"]),
                 a = .05)

#ingroup
ingroup_d_gs = d.ind.t(m1 = mean(ingroup_final_temp_gs$MFDweight[ingroup_final_temp_gs$lean == "Conservative"], na.rm = T), 
                 m2 = mean(ingroup_final_temp_gs$MFDweight[ingroup_final_temp_gs$lean == "Liberal"], na.rm = T),
                 sd1 = sd(ingroup_final_temp_gs$MFDweight[ingroup_final_temp_gs$lean == "Conservative"], na.rm = T),
                 sd2 = sd(ingroup_final_temp_gs$MFDweight[ingroup_final_temp_gs$lean == "Liberal"], na.rm = T),
                 n1 = length(ingroup_final_temp_gs$MFDweight[ingroup_final_temp_gs$lean == "Conservative"]),
                 n2 = length(ingroup_final_temp_gs$MFDweight[ingroup_final_temp_gs$lean == "Liberal"]),
                 a = .05)

#authority
authority_d_gs = d.ind.t(m1 = mean(authority_final_temp_gs$MFDweight[authority_final_temp_gs$lean == "Conservative"], na.rm = T), 
                 m2 = mean(authority_final_temp_gs$MFDweight[authority_final_temp_gs$lean == "Liberal"], na.rm = T),
                 sd1 = sd(authority_final_temp_gs$MFDweight[authority_final_temp_gs$lean == "Conservative"], na.rm = T),
                 sd2 = sd(authority_final_temp_gs$MFDweight[authority_final_temp_gs$lean == "Liberal"], na.rm = T),
                 n1 = length(authority_final_temp_gs$MFDweight[authority_final_temp_gs$lean == "Conservative"]),
                 n2 = length(authority_final_temp_gs$MFDweight[authority_final_temp_gs$lean == "Liberal"]),
                 a = .05)

#purity
purity_d_gs = d.ind.t(m1 = mean(purity_final_temp_gs$MFDweight[purity_final_temp_gs$lean == "Conservative"], na.rm = T), 
                 m2 = mean(purity_final_temp_gs$MFDweight[purity_final_temp_gs$lean == "Liberal"], na.rm = T),
                 sd1 = sd(purity_final_temp_gs$MFDweight[purity_final_temp_gs$lean == "Conservative"], na.rm = T),
                 sd2 = sd(purity_final_temp_gs$MFDweight[purity_final_temp_gs$lean == "Liberal"], na.rm = T),
                 n1 = length(purity_final_temp_gs$MFDweight[purity_final_temp_gs$lean == "Conservative"]),
                 n2 = length(purity_final_temp_gs$MFDweight[purity_final_temp_gs$lean == "Liberal"]),
                 a = .05)

#put in the numbers
tableprint_gs[1, ] = c("Harm/Care", harm_d_gs$M1, harm_d_gs$sd1, harm_d_gs$M2, harm_d_gs$sd1, 
                    summary(harm_model_gs)$tTable[2,4], summary(harm_model_gs)$tTable[2,5],
                    harm_d_gs$d)
tableprint_gs[2, ] = c("Fairness/Reciprocity", fair_d_gs$M1, fair_d_gs$sd1, fair_d_gs$M2, fair_d_gs$sd1, 
                    summary(fair_model_gs)$tTable[2,4], summary(fair_model_gs)$tTable[2,5],
                    fair_d_gs$d)
tableprint_gs[3, ] = c("Ingroup/Loyalty", ingroup_d_gs$M1, ingroup_d_gs$sd1, ingroup_d_gs$M2, ingroup_d_gs$sd1, 
                    summary(ingroup_model_gs)$tTable[2,4], summary(ingroup_model_gs)$tTable[2,5],
                    ingroup_d_gs$d)
tableprint_gs[4, ] = c("Authority/Respect", authority_d_gs$M1, authority_d_gs$sd1, authority_d_gs$M2, authority_d_gs$sd1, 
                    summary(authority_model_gs)$tTable[2,4], summary(authority_model_gs)$tTable[2,5],
                    authority_d_gs$d)
tableprint_gs[5, ] = c("Purity/Sanctity", purity_d_gs$M1, purity_d_gs$sd1, purity_d_gs$M2, purity_d_gs$sd1, 
                    summary(purity_model_gs)$tTable[2,4], summary(purity_model_gs)$tTable[2,5],
                    purity_d_gs$d)

tableprint_gs[ , c(2:6, 8)] = printnum(as.numeric(tableprint_gs[ , c(2:6, 8)]))
tableprint_gs[ , 7] = printp(as.numeric(tableprint_gs[ , 7]))

apa_table(tableprint_gs,
          note = "For mean and standard deviation values, 'C' and 'L' refer to 'conservative' and 'liberal,' respectively",
          caption = "Government Shutdown Results - Multilevel Model", 
          col.names = c("Foundation", "$M_C$", "$SD_C$", "$M_L$", "$SD_L$", "$t$", "$p$", "$d$"),
          format = "latex",
          escape = F,
          align = c("l", rep("c", 7))
          )
```

```{r conference-pic_kav, eval = F, include = F}
library(ggplot2)
library(reshape)

cleanup = theme(panel.grid.major = element_blank(), 
                panel.grid.minor = element_blank(), 
                panel.background = element_blank(), 
                axis.line.x = element_line(color = "black"),
                axis.line.y = element_line(color = "black"),
                legend.key = element_rect(fill = "white"),
                text = element_text(size = 10))

graphdata_kav = as.data.frame(tableprint_kav)
graphlong_kav = melt(graphdata_kav[ , c(1,2,4)], 
                 id = "Foundation")
graphlong2_kav = melt(graphdata_kav[ , c(1, 3,5)],
                  id = "Foundation",
                  measured = c("SD_C", "SD_L"))
graphlong_kav$sd = graphlong2_kav$value
graphlong_kav$lower = as.numeric(as.character(graphlong_kav$value)) - 2*as.numeric(as.character(graphlong_kav$sd))
graphlong_kav$upper = as.numeric(as.character(graphlong_kav$value)) + 2*as.numeric(as.character(graphlong_kav$sd))

graphlong_kav$value = as.numeric(as.character(graphlong_kav$value))

graph_data_kav = ggplot(graphlong_kav, aes(Foundation, value, color = variable)) + 
  geom_point(position = position_dodge(width = 0.90)) +
  geom_errorbar(aes(ymin = lower, ymax = upper), 
                position = position_dodge(width = 0.90),
                width = .2) + 
  cleanup +
  ylab("Mean Weighted Valence") + 
  scale_color_manual(values = c("red", "blue"),
                       labels = c("Conservative", "Liberal"),
                       name = "Party") +
  theme(axis.text.x = element_text(angle = 60, hjust = 1)) + 
  geom_hline(yintercept = 0, linetype = 1, color = "black")
  
#tiff(filename = "exp2_graph_kav.tiff", res = 300, width = 4, 
#     height = 3, units = 'in', compression = "lzw")
#plot(graph_data_kav)
#dev.off()
```

```{r conference-pic_gs, eval = F, include = F}
graphdata_gs = as.data.frame(tableprint_gs)
graphlong_gs = melt(graphdata_gs[ , c(1,2,4)], 
                 id = "Foundation")
graphlong2_gs = melt(graphdata_gs[ , c(1, 3,5)],
                  id = "Foundation",
                  measured = c("SD_C", "SD_L"))
graphlong_gs$sd = graphlong2_gs$value
graphlong_gs$lower = as.numeric(as.character(graphlong_gs$value)) - 2*as.numeric(as.character(graphlong_gs$sd))
graphlong_gs$upper = as.numeric(as.character(graphlong_gs$value)) + 2*as.numeric(as.character(graphlong_gs$sd))

graphlong_gs$value = as.numeric(as.character(graphlong_gs$value))

graph_data_gs = ggplot(graphlong_gs, aes(Foundation, value, color = variable)) + 
  geom_point(position = position_dodge(width = 0.90)) +
  geom_errorbar(aes(ymin = lower, ymax = upper), 
                position = position_dodge(width = 0.90),
                width = .2) + 
  cleanup +
  ylab("Mean Weighted Valence") + 
  scale_color_manual(values = c("red", "blue"),
                       labels = c("Conservative", "Liberal"),
                       name = "Party") +
  theme(axis.text.x = element_text(angle = 60, hjust = 1)) + 
  geom_hline(yintercept = 0, linetype = 1, color = "black")
  
#tiff(filename = "exp2_graph_gs.tiff", res = 300, width = 4, 
#     height = 3, units = 'in', compression = "lzw")
#plot(graph_data_gs)
#dev.off()
```

# Discussion

The results obtained in Experiment 2 are slightly more in line with expectations from Moral Foundations Theory, however, in all but one case, the differences were not statistically significant. The only statistically significant difference was in the harm foundation; liberals did endorse harm more than conservatives during the Kavanaugh hearings, but the effect was smaller than expected given @Graham2009. Generally for both the Kavanaugh hearings and the Government shutdown, the differences were in the expected direction (liberals endorsing harm more and conservatives endorsing authority and loyalty more), but the differences were not statistically significant and the effects much smaller than reported in @Graham2009. While not significant, fairness was endorsed more by conservatives in both cases. Contradictorily, purity was endorsed more by conservatives for the government shutdown, but more by liberals for the Kavanaugh hearing though neither difference was statistically significant.

Together with Experiment 1 and past work, Experiment 2 further calls into question the usefulness and efficacy of the Moral Foundations as well as cast doubt on the predictions made by Moral Foundations Theory. One issue that Experiment 2 brings up is that moral differences between liberals and conservatives may be moderated by a number of factors as shown in @Sterling2018. For example, here we have liberals concerned with purity for the Kavanaugh hearings (a moral bad for liberals and a moral good for conservatives) and conservatives concerned with purity during the government shutdown (a moral bad for conservatives and a moral good for liberals). The original predictions made by the Moral Foundations are very simplistic and likely much more complex in reality.

# Conclusions 

Within the theoretical framework of Moral Foundations Theory [@Haidt2007], we attempted to devise a method leveraging the Moral Foundations Dictionary [@Graham2009] in order to quantify political bias stemming from content published by several prominent American news sources. In Experiment 1,the results obtained were not significant in any statistical or practical sense. There was a small effect found for the *authority/respect* foundation according to Cohen's *d*, but the effect suggested endorsement for that foundation opposite to what was hypothesized. In Experiment 2, the direction of the differences were generally in line with MFT, however, the difference were very small and lacks statistically significance. Building on @Frimer2020 and other critiques of MFT and the MFD, we show that MFD is not a useful tool for measuring moral language while also calling into question the validity of Moral Foundations Theory in terms of theorized partisan differences.

Despite the fact the results regarding political bias were inconclusive, the researchers still retain confidence in the overall structure of the methodology established in the current study. Specifically, the procedure for scraping text from the web, processing, stemming, and weighting the scores with valence seems to represent a solid method for preparing a high quantity of text passages for data analysis. The researchers implemented valence as an indicator of the directionality of endorsement due to the inherent ambiguity of simply calculating MFD word percent occurrence. This method served both to augment the face validity of the MFD by incorporating valence (thus reducing ambiguity) and to generate a score that is easy to understand and analyze.

However, political lean and other important political/moral constructs may be communicated through methods other than through the use of particular words. As mentioned before, MFD is a measure developed from Moral Foundations Theory [@Graham2009]. Based upon the results obtained, it might be necessary to investigate alternative instruments that could better elucidate the differences of interest. Going beyond specific instruments, other theoretical perspectives may be more equipped to explain political differences in discourse. Likewise, an atheoretical approach in which large quantities of data are collected from which theories are formulated may be best suited to this area of research.

In critiquing Moral Foundations Theory, @Schien2015 proposed an alternative theory explaining moral choices and potential partisan differences: Dyadic Theory of Morality. Across many studies, researchers find that supposed differences in moral foundation can be more easily explained by a harm-focused morality where someone/thing is harmed by someone/thing else [@Gray2014, @Schien2015]. Rather than liberals and conservatives having difference conceptions of morality, dyadic morality argues that they simply identify harm differently. For example, in the case of abortion, liberals tend to identify the primary harm to the mother leading to a pro-choice position whereas conservative tend to identify the primary harm to the fetus leading to a pro-life position. While as of yet, no linguistic measurement of dyadic morality exists; such an approach may be able to better identify the systematic differences between liberals and conservatives.

Turning to atheoretical approaches, future studies aiming to uncover political lean in discourse may benefit from a bottom-up, data-driven approach. Researchers might derive substantive insights into political lean through gathering data after which they may formulate theories that explain systematic observations obtained from that data. The availability of text corpora along with methods for extracting large amounts of text from the internet (as was demonstrated in the current study) potentially make this a feasible option. Likewise, there are several approaches to analyzing such data, including linear models (like multi-level models) and network-style models such as latent semantic analysis [@Landauer1998]. Owing to the wealth of representative data as well as the sophistication of current analytic tools, there is high potential for the explanatory power of new theories involving political discourse.  

Never before has political discourse represented such fertile ground for psychological research. The plethora of options for news sources has created not only an abundance of choice but also vast quantities of text data. Along with this recent increase in the amount of text information, there is now an obligation on the part of researchers to devise proper methods for analyzing that text.  Solid methodologies must be constructed and periodically improved to keep pace with evolving technologies. Likewise, a strong theoretical foundation is paramount to making sense of the current and future media ecosystem. Therefore, it is incumbent upon social scientists to continue investigating the information consumed by millions of Americans every day so that insights into the nature and consequences of political discourse can be more completely understood.


\newpage

# References

\begingroup
\setlength{\parindent}{-0.5in}
\setlength{\leftskip}{0.5in}

<div id = "refs"></div>
\endgroup

\newpage

# Appendix

```{r appendix_mfd, results = 'asis', echo = FALSE, message = F}
orig_mfd = read.csv("../exp1_data/original_mfd.csv", stringsAsFactors = F, fileEncoding="UTF-8")
####build the table here####
tableprint = matrix(NA, nrow=length(orig_mfd), ncol=5)
colnames(tableprint) = c("Harm/Care", "Fairness/Reciprocity", "Ingroup/Loyalty", "Authority/Respect", "Purity/Sanctity")

#tableprint[ , 1] = unique(final_gs$Source)
#tableprint[ , 1] = orig_mfd[,1]
#tableprint[ , 2] = orig_mfd[,2]
#tableprint[ , 3] = orig_mfd[,3]
#tableprint[ , 4] = orig_mfd[,4]
#tableprint[ , 5] = orig_mfd[,5]

tableprint = tableprint[c(1,2,3,4,5)]
tableprint = orig_mfd
#change this to match
apa_table(as.data.frame(tableprint),
          note = "Original Moral Foundations Dictionary from Graham et al. (2009)",
          caption = "Moral Foundations Dictionary", 
          col.names = c("Harm/Care", "Fairness/Reciprocity", "Ingroup/Loyalty", "Authority/Respect", "Purity/Sanctity"),
          format = "latex",
          escape = F,
          small = T, 
          align = c("l", rep("c", 6))
          )
```

