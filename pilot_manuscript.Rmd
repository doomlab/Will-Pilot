---
title             : "Moral Foundations of U.S. Political News Organizations"
shorttitle        : "MORAL NEWS"

author: 
  - name          : "William E. Padfield"
    affiliation   : "1"
    corresponding : yes    # Define only one corresponding author
    address       : "901 S. National Ave, Springfield, MO, 65897"
    email         : "Padfield94@live.missouristate.edu"
  - name          : "Erin M. Buchanan, Ph.D."
    affiliation   : "2"

affiliation:
  - id            : "1"
    institution   : "Missouri State University"
  - id            : "2"
    institution  : "Harrisburg University of Science and Technology"
  
author_note: |
  William Padfield is a master's degree candidate in Psychology at Missouri State University. This thesis partially fulfills the requirements for the Master of Science degree in Psychology.

abstract: |
  The media ecosystem has grown, and political opinions have diverged such that there are competing conceptions of objective truth. Commentators often point to political biases in news coverage as a catalyst for this political divide. The Moral Foundations Dictionary (MFD) facilitates identification of ideological leanings in text through frequency of the occurrence of certain words. Through web scraping, the researchers extracted articles from popular news sources' websites, calculated MFD word frequencies, and identified words' respective valences. This process attempts to uncover news outlets' positive or negative endorsements of certain moral dimensions concomitant with a particular ideology. In Experiment 1, the researchers gathered poltical articles from four sources. We were unable to reveal significant differences in moral or political endorsements, but we solidified the method to be employed in further research. In Experiment 2, the researchers expanded their number of sources to 10 and analyzed articles that pertain to two specific topics: the 2018 confirmation hearings of U.S. Supreme Court Justice Brett Kavanaugh and the partial U.S. Goverment Shutdown of 2018-2019. Once again, no significant differences in moral or political endorsements were found.
  
keywords          : "politics, morality, psycholinguistics"

bibliography      : ["will_thesis.bib"] #turn this off right now so it knits

figsintext        : no
figurelist        : no
tablelist         : no
footnotelist      : no
lineno            : yes
lang              : "english"
class             : "man"
mask              : no
output            : papaja::apa6_pdf
replace_ampersands: yes
csl               : apa6.csl
---

```{r load_packages, include = FALSE}
library(papaja) 
library(MuMIn)
library(reshape)
library(nlme)
library(ggplot2)
library(expss)
library(MOTE)

knitr::opts_chunk$set(cache = TRUE)
options(tinytex.verbose = TRUE)
```

In the United States, today's media landscape affords consumers a multitude of options for obtaining political news. Since the advent of cable news networks and the World Wide Web in the last decades of the twentieth century, consumers have gained access to an ever-expanding menagerie of news sources, many of which can be called up via a simple click, touch, or swipe. Concurrent with this growth in available news sources, concerns regarding political bias in news reporting have entered public consciousness. For example, commentators argue that networks including Fox News Channel and MSNBC communicate political news from a conservative and liberal slant, respectively. These purported biases have been a cause for concern given the potential for incomplete or inaccurate news reporting potentially resulting from these biases. Given the inherently moral nature of many political arguments and positions, bias in news reporting might manifest as differing moral appeals. Specifically, the use of differing moral language in political articles might be an indicator of political bias in news media. 

Morality and ethics have been of interest to thinkers, academics, and philosophers since antiquity. Starting chiefly in the twentieth century, a scientific approach to humans' understanding of morality emerged under the domain of psychology. Theories attempting to explain the development and application of people's moral intuitions built the foundation for the subfield of moral psychology. As the field developed, however, considerable debate has taken place regarding operational definitions of "morality." Concerns regarding operationalization remain an issue in the field in the twenty-first century as researchers attempt to infer moral and political leanings from text and speech.

## Moral Foundations Theory

As a discipline, modern moral psychology started in the late 1960s with Lawrence Kohlberg [@Haidt2007]. Kohlberg's research popularized his theory of the development of moral reasoning. This theory establishes the steps of moral reasoning through which humans proceed as their cognitive structures assume higher levels of sophistication and nuance [@Kohlberg1977]. Kohlberg borrowed from Jean Piaget's stages of cognitive development in which children progress from the sensorimotor through to the formal operations stage. Similarly, Kohlberg found people typically start with a "pre-conventional" understanding of morality during infancy in which children understand "right" and "wrong" purely in terms of how they interact with resultant experiences of rewards and punishment. Typically, people progress through several steps until they reach a "post-conventional" ethics. People who have reached the post-conventional stage are said to be able to weigh competing abstractions and reason their way to a conclusion that promotes justice based upon their "self-chosen ethical principles" [@Kohlberg1977]. From Kohlberg's perspective, issues of justice and fairness comprise the foundation of morality [@Haidt2007]. This view persisted until it encountered criticism in the early 1980s.    

Kohlberg's conception of morality faced major scrutiny from psychologist Carol Gilligan. In 1982, Gilligan criticized Kohlberg's theory on the grounds that it focused solely on the moral concerns of men, and that it ignored those of women [@Haidt2007]. Gilligan drew attention to purported differences in the ways men and women are taught to relate to self and others. She offered a historic argument contending women have traditionally filled roles related to caring and nurturing. She pushes back against Kohlberg's assumption that moral development replaces "rule of brute force," as enforced by men, with the justice-based "rule of law." According to Gilligan, this assumption implies women are less morally developed, owing to their absence both in masculine displays of violence as well as in enforcement of the law [@Gilligan1982]. Gilligan argues for the existence of a distinct, but equal development process that women and girls must undergo in order to develop their moral selves. Stark differences in the ways women are traditionally taught to interact with their social world cause them to develop ethical systems based upon their non-aggressive relationships with others. Gilligan thus asserted morality was built upon an alternative moral foundation: caring [@Gilligan1982]. This debate between competing conceptions of morality did not resolve until Gilligan and Kohlberg conceded the existence of two moral foundations: justice and caring [@Haidt2007]. While this new direction in moral psychology appeared to represent a more inclusive outlook on the construct, these novel ideas would soon be challenged on the grounds of its apparent western-centric outlook. 

Jonathan Haidt and Jesse Graham formulated Moral Foundations Theory as a method by which to capture the entirety of humans' moral domain [@Haidt2007]. The researchers argued older theories of moral psychology were focused primarily on issues of justice, fairness, and caring - individually focused foundations of morality that align with the beliefs of political liberals [@Haidt2007]. In other words, moral psychology ignored the valid moral foundations of conservatives. Moral Foundations Theory (MFT) holds that people's moral domain can be mapped by quantifying their endorsement of five moral foundations: *harm/care*, *fairness/reciprocity*, *ingroup/loyalty*, *authority/respect*, and *purity/sanctity* [@Haidt2007]. 

In their brief overview of the history of moral psychology, @Graham2009 explained Shweder, Much, Mahapatra, and Park's objections to moral psychology as it stood in the late 1980s. Their criticism centered on the fact moral psychology concerned itself with issues regarding justice and individuals' rights. Such a system, they argued, did not account for moral concerns outside of the western world [@Graham2009]. Individually focused concerns can be grouped under an overarching "ethic of autonomy," which was thought to be one of three ethics upon which humans base moral decisions. The other two ethics were the "ethic of community" (comprising one's duty to their family, tribe, etc.), and the "ethic of divinity" - representing one's duty not to defile their God-given body and soul [@Graham2009]. In the 2000s, @Haidt2007 took this line of reasoning further in their assertion that moral psychology favored certain political ideologies over others. 

Haidt and Graham settled on these specific foundations after the completion of a literature survey of research in anthropology and evolutionary psychology [@Graham2011]. The researchers attempted to locate virtues and morals corresponding to "evolutionary thinking." For instance, the researchers cited Mauss' work on reciprocal gift-giving, which informed the establishment of the *fairness/reciprocity* foundation. Additionally, evolutionary literature on disgust and its correlation to human behavior regarding food and sex informed the *purity/sanctity* foundation [@Graham2011]. The researchers identified the five "top candidates" for the foundations of human cultures' morality [@Graham2011].

The first two foundations (*harm/care* and *fairness/reciprocity*) are termed the "individualizing foundations," as they are centered on the concerns of individuals rather than groups. *harm/care* represents an endorsement of compassion and kindness, while opposing cruelty and harm. *Fairness/reciprocity* represents concerns centered on guaranteeing individual rights as well as justice and equality among all people. The other three foundations (*ingroup/loyalty*, *authority/respect*, and *purity/sanctity*) are the "binding" foundations, owing to their focus on group-related concerns, rather than those of individuals. *Ingroup/loyalty* represents endorsements of patriotism and heroism and discourages nonconformity and dissent. *Authority/respect* represents an endorsement of social hierarchies and traditions while denigrating disobedience. Finally, *purity/sanctity* represents concerns regarding chastity and piety, while discouraging vices and indulgences, including lust, avarice, and gluttony [@Haidt2007]. Liberals tend to endorse the individualizing foundations more than conservatives. Conservatives, on the other hand, tend to endorse the binding foundations more than liberals. It should be noted, however, conservatives also tend to endorse all five foundations equally, implying they base moral judgments on all foundations [@Graham2009].

Moral Foundations Theory has received criticism on the grounds that its assumptions regarding moral intuitions have little empirical basis. @Suhler2011 list several potential weaknesses of MFT that they argue might threaten the theory's validity. First, the authors challenge @Haidt2007's claims the moral intuitions represented by MFT are innate and modular. @Suhler2011 claim that advances in biological sciences (embryology and microbiology, specifically) make it more difficult for researchers to claim any one trait is either innate or learned through experience. Rather, behaviors likely result from interactions between genetics and experience [@Suhler2011]. According to the authors, without solid data supporting the innateness of moral foundations, Haidt and Graham have little from which to make such a claim. Similarly, @Haidt2007 rely on evidence authored by evolutionary psychologists to make a "strong modularity claim" [@Suhler2011]. However, as with innateness, there is little neurobiological evidence to support modularity.

@Suhler2011 also criticized the content and taxonomy of the five foundations. The authors criticize @Haidt2007's omissions of additional foundations, including *industry* and *modesty*, claiming these concepts are moralized in many societies worldwide. Likewise, the authors question whether or not the foundations are sufficiently distinct as to stand as their own foundation. For example, @Suhler2011 posit that *ingroup/loyalty* is merely a group-focused version of *harm/care*. Work by @Graham2011 might serve to rebut this criticism, as the researchers found their original five-factor structure seemed to best fit the data when validating the Moral Foundations Questionnaire. Finally, @Suhler2011 point out that particular concepts related to a foundation, including "anger" in *fairness/reciprocity* and "deception" in *ingroup/loyalty*, could be ascribed to any of the other foundations as well. In other words, it becomes difficult to recognize a particular concept as indicative of any one foundation when, in theory, they could be applied to all five [@Haidt2007; @Suhler2011].

These criticisms of Moral Foundations Theory are valid and should be taken into consideration when conducting research with instruments derived from MFT. Such criticisms are especially valid when considered alongside questions about the Moral Foundations Dictionary in particular. Nevertheless, there exists compelling evidence supporting the validity of Moral Foundations Theory, albeit regarding its application solely through the Moral Foundations Questionnaire. This evidence is discussed within a brief explanation of the Moral Foundations Questionnaire and its relationship to the Moral Foundations Dictionary in the next section.

## Moral Foundations Dictionary

In order to capture language's role in moral and political reasoning, @Graham2009 formulated the Moral Foundations Dictionary (MFD) in order to capture moral reasoning and justification as used in speech and text. The MFD is composed of 259 words, with around 50 words assigned to each of the five foundations. @Graham2009 created a preliminary list of words that they believed would be associated with the five foundations. Then, using the Linguistic Inquiry and Word Count [LIWC; @Pennebaker2007] computer program, they analyzed transcripts of liberal and conservative Christian sermons in order to obtain frequencies of the occurrence of words from the researchers' initial list. @Graham2009 manually checked the results from LIWC in order to make sure the results make sense given the contexts and rhetorical devices used in the sermons, as word frequency analysis ignores sentence context. The researchers offered the following example from a Unitarian sermon as a demonstration of ambiguous statements requiring human verification: "Don't let some self-interested ecclesiastical or government authority tell you what to believe, but read the Bible with your own eyes and open your heart directly to Jesus" [@Graham2009]. This sentence added to the *authority/respect* total in LIWC's analysis, but it appears to suggest that one should reject authority in this context. The researchers eliminated this sentence from the *authority/respect* raw count on account of this discrepancy between the use of authority-related words and the speaker's clear intentions [@Graham2009]. The original Moral Foundations Dictionary can be found in the Appendix.

Similar to previous research on Moral Foundations Theory, liberal ministers used *harm*, *fairness*, and *ingroup* words more often than conservative ministers. Conversely, conservative ministers used *authority* and *purity* words more often than liberal ministers. However, conservative ministers did not use *ingroup/loyalty* words more than liberals. Rather, liberal ministers used words pertaining to *ingroup/loyalty*, but in contexts that promote rebellion and independence - causes *opposite* to positive endorsements of that foundation [@Graham2009].

To this point, most text analysis utilizing the Moral Foundations Dictionary operationalizes endorsement of any one of the foundations as percent occurrence of words in a given text from the foundation's respective word list. As such, most analyses assume that zero percent occurrence is indicative of no endorsement, while any non-zero percent occurrence indicates endorsement of the foundation. This operational definition may not be sufficient in describing the true nature of the writer or speaker's endorsement of one of the sets of moral intuitions. A quick glance at the MFD words for *harm/care* reveals the presence of words that are more closely associated with universally accepted conceptions of *harm* over *care* and vice-versa [@Graham2009]. For example, the word "cruel" has relatively negative connotations compared to "benefit." For the *harm/care* foundation, it is conceivable that use of the word "cruel" might indicate a greater attentional focus of the idea of *harm* rather than *care*. 

For *harm/care*, the definition of the foundation, as well as its name, clearly distinguishes between two somewhat opposite sides of an attentional continuum, with *harm* on the negative end and *care* on the positive side. In other words, the entries in the MFD for *harm/care* have somewhat clear positive and negative valences. The same pattern can be seen in the MFD entries for the other four foundations. *Purity/sanctity* features words that likely have a negative valence to most observers, including "disease" and "trash," along with more positive words, including "right" and "sacred" [@Graham2009]. These dichotomies, however, bring up other questions regarding the definition and names of the other four foundations apart from *harm/care*: *fairness/reciprocity*, *ingroup/loyalty*, *authority/respect*, and *purity/sanctity*. The latter four foundations have names that are harder to understand as a valence continuum, as the concepts in the names are more similar, even to the point of being virtually synonymous in the case of *fairness/reciprocity*.

When considering the issue of positive versus negative valence in MFD words, the question of how texts are analyzed vis-a-vis the MFD remains. How can raw percentage of MFD word occurrence capture the valence and focus of the writer or speaker? If 2% of a politician's speech features positive words (i.e., "benefit" and "defend") from the MFD *harm/care* list, how can researchers be sure the level and nature of the speaker's "endorsement" of the foundation equals that of another politician whose speech contained negatively connoted MFD words from the *harm/care* list? They would have equal endorsements as far as the numbers are concerned, but the words used and focus given are on opposite sides of the *harm/care* spectrum.

This issue is compounded by the fact the Moral Foundations Questionnaire (MFQ) and its subscales assume endorsement lies on a continuum. The MFQ, which was developed subsequent to the MFD, measures individuals’ endorsements of each of the foundations using a six-point scale [@Graham2011]. The questionnaire is made up of judgment items and relevance items. Judgment items are phrased such that the respondent signals their agreement or disagreement with straightforward statements. An example of such a statement reads: “It can never be right to kill another human being” [@Graham2011]. Relevance items gauge the respondent's opinion regarding the importance of foundation-related concerns. For example, the respondent is directed to rate how important the following situation is to their sense of morals: "whether or not someone did something disgusting." This example measures the relevance of the *purity/sanctity* foundations. Each foundation has a judgment and relevance subscale, totaling 10 subscales for the MFQ [@Graham2011].

The Moral Foundations Questionnaire has been validated by multiple researchers. Likewise, its five-factor structure has been demonstrated to fit data in multiple countries [@Graham2011; @Davies2014]. In their article introducing the 30-item MFQ, @Graham2011 conducted an exploratory factor analysis (EFA) and found that a five-factor model fit the data better than a one, two, or three-factor model. @Davies2014 conducted a confirmatory factor analysis (CFA) of the MFQ with a sample from New Zealand and likewise found the five-factor model provided the best fit. While @Davies2014 concede the US and New Zealand share many similarities as Western nations, which could raise questions regarding the validity of the MFQ in non-Western nations. However, there are striking differences between the two countries, including the lack of a two-party political system in New Zealand, that provide grounds for claiming the MFQ generalizes beyond the United States. Furthermore, @Graham2011 claim to find a "reasonable" degree of generalizability for the MFQ across participants from many different regions in the world. These two bodies of work also provide the best available evidence that the five moral foundations are sufficiently distinct from one another, though broader criticisms of MFT raised by @Suhler2011 should still be taken into account in studies involving the theory.

The aforementioned ambiguity of the Moral Foundations Dictionary as an instrument becomes clearer upon closer examination of the items in the Moral Foundations Questionnaire. One item under the *fairness/reciprocity* judgment subscale reads, "Justice is the most important requirement for a society" [@Graham2011]. The survey respondent must select a number on a scale from 1 to 6 indicating responses spanning "strongly disagree" at 1 to "strongly agree" at 6. While the scales in the MFQ do not represent true valence as it pertains to individual words, it does allow for a greater degree of specificity in terms of an individual's endorsement of a particular moral foundation. When a respondent selects a 4 for the aforementioned MFQ statement, they clearly are indicating they "slightly agree" with the statement [@Graham2011]. This specificity is not present in most analyses involving the MFD and percent occurrence, unless they also take into account the valence of the words used in the text or speech of interest.   

## Valence

Borrowing from Osgood's work in the 1950s, @Bradley1999 recognized valence as one of three related dimensions comprising emotion when developing their Affective Norms for English Words (ANEW). As mentioned before, "valence," the first dimension, denotes the pleasantness of a given word. "Arousal," the second dimension, describes the stimulating nature of a word. Lastly, "dominance" or "control" describes the extent to which a word makes one feel in or out of control [@Bradley1999]. The researchers developed ANEW by presenting participants with a list of 100-150 words and asking for them to rate the word on all three dimensions using the Self-Assessment Mannikin (SAM), which allows ratings along either a nine-point scale when using traditional paper instruments or a twenty-point scale when using a computerized version.

Participants saw the stimulus word and responded on each scale. The valence scale featured a smiling figure at one end (representing pleasantness) and a frowning figure at the other end (for unpleasantness). The arousal scale had a "wide-eyed" figure at one end with a sleepy figure at the other, representing stimulating and unstimulating respectively. Finally, the dominance scale featured a large figure, indicating the highest degree of control, at one end and a small figure, indicating a lack of control, at the other end [@Bradley1999]. The end result of this procedure yielded affective norms along the three dimensions for 1,040 English words [@Bradley1999]. ANEW represented an important first step in establishing affective norms for large numbers of English words. However, later researchers found the 1,040-word list to be limiting for a language consisting of thousands of words.

@Warriner2013 exponentially lengthened the list of words with affective norms to 13,915 English lemmas, the base forms of words without inflection (i.e., "watch" rather than "watched" and "watching"). The researchers recognized the importance of affective norms in several areas of study, including emotion, language processing, and memory [@Warriner2013]. They argue the list of words included in ANEW is sufficient for small-scale factorial research designs, but the list is "prohibitively small" for larger-scale "megastudies" that are common in psycholinguistic research today [@Warriner2013]. 

In order to source a large number of lemmas for affective ratings, the researchers drew from several validated sources. These include the 30,000 lemmas with age-of-acquisition (average age at which a particular word is learned) ratings gathered by @Kuperman2012 as well as the content lemmas from the SUBTLEX-US corpus consisting of subtitles from various forms of visual media [@New2007]. This data collection resulted in the final list of 13,915 lemmas. Lists of 346-350 words were presented to participants recruited through the Amazon Mechanical Turk subject pool. Participants rated the words along one of the three dimensions, unlike the ANEW project in which participants rated each word along all three dimensions at once. The researchers used a nine-point scale similar to the one used by @Bradley1999 when collecting ratings for ANEW [@Warriner2013]. 

The researchers noted several points of interest upon observing ratings. First, they found that valence and dominance ratings had a negative skew, indicating more words elicited feelings of happiness and control than their respective opposites. Also, when examining the relationship between valence and arousal ratings, the researchers found a U-shaped relationship. This U-shape indicates words with high degrees of positivity and negativity elicited higher arousal [@Warriner2013]. These observations along with the now-greatly expanded list of affective norms has been applied to several lines of inquiry in psycholinguistics.

@Warriner2015 utilized the new affective norms list in order to investigate the validity of the Pollyanna hypothesis, or the prevalence of a generally optimistic outlook in humans as reflected in language. The researchers were able to conclude the existence of a greater number of positive-valence English words in the list of 13,915 lemmas. Additionally, after observing token frequency in a number of text corpora, including SUBTLEX-US, the Corpus for Contemporary American English (COCA), the British National Corpus (BNC), Touchstone Applied Science Associates, Inc. Corpus (TASA), and the corpus used for the Hyperspace Analogue to Language model (HAL), the researchers found that words with positive valence were also used more frequently [@Warriner2015]. While the researchers concede the possibility of an acquiescence bias in ratings as a possible explanation for the observed positivity bias, this investigation represents one application of the @Warriner2013 list in emotional studies.

In addition to applications in emotion research, the @Warriner2013 norms have been utilized in cognitive research as well. One cognition-based study investigates the relationship between emotion and response latencies in word recognition. @Kuperman2014 sought to use these new norms to fill in the knowledge gaps regarding variance in word recognition. The researchers drew several conclusions regarding emotion and word recognition (specifically in naming and lexical decision tasks - two cognitive processing tasks wherein a participant has to read aloud or judge a word for its lexicality). First, @Kuperman2014 found slower decision-making and reading times in negative-valence words, faster times in neutral words, and even faster times in words with positive valence. The researchers also concluded that words causing higher arousal tend to have slower decision times than less-arousing words. They found valence had a stronger effect on recognition than arousal (both effects were independent, not interactive). They found an interaction between emotion and word frequency such that valence and arousal are more effective on lower frequency words than high frequency words. Finally, @Kuperman2014 found a greater effect of valence and arousal on response latency for lexical decision tasks than for naming tasks [@Kuperman2014]. This research serves as further evidence that the @Warriner2013 list can be used for research inquiries both within and without the field of psycholinguistics. 

In the present studies, the researchers used the @Warriner2013 list in order to denote the valence of the words appearing in the news articles scraped from the internet. Valence was considered as another independent variable and its relationship with the words comprising the Moral Foundations Dictionary were of chief interest to the researchers. The valence was used as a means to determine whether individual words in the MFD represented more positive aspects of their respective foundation or if they denoted a more negative aspect of the foundation. Specifically, valences were used to weight the MFD words by their relative degree of positivity or negativity. Incorporating word valence into a study involving the MFD is meant to alleviate some of the issues regarding the aforementioned ambiguity regarding the words in the Moral Foundations Dictionary.

## News Media and Politics

Research into politics, language, and media has illuminated the complex relationships between all three. Any politically-oriented discussion of word occurrence as an implication of moral or political position assumes that language and ideology are intrinsically linked. Deborah Cameron [-@Cameron2006] points out the expressive nature of ideological beliefs and how that expression is conveyed through language, thus implying a connection between ideology and language. She goes on to criticize the notion that language is either the "pre-existing raw material" used to shape ideologies or the "post-hoc vehicle" for their propagation. Rather, the structure of language itself is shaped by ideology and social processes even when it is used to explain or express ideologies [@Cameron2006]. Owing to the fact the Moral Foundations Dictionary was developed in order to assess the moral, which includes the ideological, orientation of discourse, its purported ability to assess parts of the structure of language (vocabulary) for ideological lean is of chief interest to the researchers in the present study. 

The use of language both to express and further an ideological goal has been documented in the techniques employed by candidates for political office in the U.S., @Druckman2004 considered political "issues" as communication that attempts to persuade constituents to vote for the candidates based on their strengths in matters of public policy. According to the researchers, "image" priming describes techniques deployed in order to sway votes based on favorable aspects of the candidate's behavior and personality [@Druckman2004]. The researchers investigated political issue and image priming on the part of candidates as implied by the disproportionate attention candidates paid to particular issues over others. The researchers found numerous examples of issue and image priming during the 1972 re-election campaign of Richard Nixon. 

They linked the Nixon administration's awareness of the issues for which the president had public support to the issues he should emphasize (and prime) during the campaign. Likewise the researchers found evidence that Nixon's team was aware of negative evaluations of his warmth and trustworthiness, and thus took steps to prime his purportedly positive qualities, including strength and competence [@Druckman2004]. The researchers also cited research from Iyengar and Kinder (1987) suggesting the news media affected perceptions of President Jimmy Carter's competence by emphasizing (e.g., priming) issues related to energy, defense, and the economy. This focus implies news media may contribute to Americans' perception of politicians based on where the media places emphasis.

There is a potential caveat regarding the validity of @Druckman2004's findings: reproductions of several studies purporting to demonstrate social priming effects have failed to replicate the original results. @Pashler2012 point out the distinction between perceptual and social (or goal) priming both in their operational definitions as well as their replicability. Perceptual priming often works through the inducement of a certain response from a related prime, as in, for example, semantic priming. Social (or goal) priming encompasses phenomena by which people exhibit complex behavioral changes subsequent to exposure to a prime. @Pashler2012 point out well-known studies investigating social priming, including the use of elderly-related primes to induce slower walking speeds in participants. Studies investigating perceptual priming have been "directly replicated in hundreds of labs" [@Pashler2012]. This replication rate does not appear to be the case for social priming, as argued by @Pashler2012.  

@Pashler2012 noticed the unusually large effect size values (Cohen's *d*) reported by researchers studying social priming effects. The researchers reproduced two studies from @Williams2008 The first study attempted to prime participants by having them plot points on a Cartesian grid. The independent variable was priming condition and contained three levels: short, middle, and long distance. Those instructed to plot points further apart were hypothesized to express a higher degree of psychological distance regarding their family. The second study used the same priming conditions, but hypothesized that greater distance between points would prime participants to estimate fewer calories in unhealthy foods than those who were primed with shorter distances between points. @Pashler2012 concluded those two studies from @Williams2008 held little validity while also casting doubt on the prevalence of social priming effects themselves, based on the inability of other researchers to replicate previously reported effects in this area.

While these concerns regarding the replication of social priming studies are valid and deserve further investigation, @Druckman2004 does not purport to demonstrate a widespread effect of social priming on the American electorate. In other words, this research makes no claim to empirically supported priming effects. Rather, @Druckman2004 chronicle the efforts on the part of the Nixon Administration to prop up the president's supposed strengths while downplaying his weaknesses. These tactics were deployed through the careful use of language in order to achieve the administration's political goals. As such, @Druckman2004's research on Nixon serves as an example of language's potential utility in the propagation of desirable political opinions. The researcher's investigation of news media's focus on specific issues during the Carter Administration likewise provide an example of language as a potential conduit for the transfer of politically biased information. The idea that even 1970s news media could contain political biases is of particular interest to the current study, which investigates similar phenomena in contemporary news media.

Other research into news media suggests certain media outlets, at least indirectly, may have an effect on the voting records of representatives in Congress [@Clinton2014]. Specifically, the researchers identified a pattern of declining support for President Bill Clinton's policies chiefly among Republicans in the House of Representatives after the Fox News Channel began broadcasting on cable and satellite systems in their respective districts. As Fox News was, at the time of its launch in 1996, the only outwardly ideological national news network, the researchers were able to track its spread across the country and observe voting records of members of Congress both before and after Fox News' arrival. The researchers concluded that members of Congress, excluding those newly elected at the time of Fox News Channel's emergence, attempted to anticipate resultant conservative-leaning shifts among their constituents by bolstering their conservative voting record before the next election [@Clinton2014].

Therefore, the current study sought to combine both methods related questions and extension/replication of previous moral foundation results found for liberal and conservative sources. First, the MFD was combined with previous research by the current authors (see below) and weighted by valence to create weighted percentages to better specify endorsement. Second, these weighted percentages were examined for their differences in across liberal and conservative news sources. 

# Experiment 1 

# Method

For Experiment 1, the researchers approached the study with the intention to answer a method question. That is, this portion of the current research was conducted in order to solidify the best method by which to analyze political news text under the Moral Foundations Theory framework while also alleviating some of the aforementioned valence problem observed in the Moral Foundations Dictionary. The researchers hypothesized the news sources generally perceived as liberal leaning (*NPR* and *The New York Times*) would contain MFD words and valences indicating endorsements of the individualizing moral foundations (*harm/care* and *fairness/reciprocity*). Additionally, the researchers hypothesized the two sources generally perceived to be conservative leaning (*Fox News* and *Breitbart*) would feature MFD words and valences indicating equal endorsement of all five foundations. Owing to the lack of a need for human participants, the researchers did not petition Missouri State University's Institutional Review Board, as no such approval was needed to conduct this study.

## Sources

Political articles were collected from the websites of four notable U.S. news sources, a process known as web scraping. The sources were *The New York Times*, *National Public Radio (NPR)*, *Fox News*, and *Breitbart*. They were selected for their widespread recognition and the fact political partisans have strong preferences for some sources over others. The researchers determined the political lean of each source by referencing @Mitchell2014's article demonstrating the self-reported ideological consistency represented by the consumers of several news sources. In general, *The New York Times* and *NPR* are preferred by consumers reporting a liberal bias or lean. In contrast, *Fox News* and *Breitbart* are believed to have a conservative bias or lean. @Mitchell2014's article presented political ideology as a scale ranging from "consistently liberal" to "consistently conservative." In between these extremes lie more moderate positions, including "mostly liberal," "mixed," and "mostly conservative." Owing to the lower number of sources analyzed herein, the researchers elected to categorize the sources as either "liberal" and "conservative" in order to form a basis for comparison.   

Political articles in particular were identified and subsequently scraped by including the specific URL directing to each source's political content in the *R* script. For example, rather than scrape from nytimes.com, which would return undesired results (non-political features, reviews, etc.), we instead included nytimes.com/section/politics so that more or less exclusively political content was obtained. All code for this manuscript can be found at https://osf.io/5kpj7/, and the scripts are provided inline with this manuscript written with the *papaja* library [@Aust2017]. 

Identification of the sources' political URLs presented a problem for two of the sources owing to complications with how their particular sites were structured. While in the multi-week process of scraping articles, we noticed word counts for *NPR* and *Fox News* were not growing at a similar pace as those from *The New York Times* and *Breitbart*. Upon investigation, we found another, more robust URL for political content from NPR: their politics content "archive." The page structure on NPR's website was such that only a limited selection of articles is displayed to the user at a given time. Scraping both the archive and the normal politics page ensured we were obtaining most (if not all) new articles as they were published. We later ran a process in order to exclude any duplicate articles. *Fox News* presented a similar issue. We discovered *Fox News* utilized six URLs in addition to the regular politics page. These URLs led to pages containing content pertaining the U.S. Executive Branch, Senate, House of Representatives, Judicial Branch, foreign policy, and elections. Once again, duplicates were subsequently eliminated from any analyses.  

## Materials

Using the *rvest* library in the statistical package *R*, we pulled body text for individual articles from each of the aforementioned sources (identified using CSS language) and compiled them into a dataset [@Wickham2016]. Using this dataset, we identified word count and average word count per source. This process was completed once daily starting in February 2018 until March 2018. Starting in mid-March 2018, the process was completed twice daily - once in the morning and again in the evening. Data collection was terminated once 250,000 words per source was collected in April 2018.  

```{r scraping, eval=FALSE, include=FALSE}

library(rvest)
####NY Times####
#Specifying the url for desired website to be scrapped
url <- 'https://www.nytimes.com/section/politics'

#Reading the HTML code from the website - headlines
webpage <- read_html(url)
headline_data = html_nodes(webpage,'.story-link a, .story-body a')

attr_data = html_attrs(headline_data) 
attr_data

urlslist = unlist(attr_data)
urlslist = urlslist[grep("http", urlslist)]
urlslist = unique(urlslist)
urlslist

##start a data frame
NYtimesDF = matrix(NA, nrow = length(urlslist), ncol = 3)
colnames(NYtimesDF) = c("Source", "Url", "Text")
NYtimesDF = as.data.frame(NYtimesDF)

##for loops
for (i in 1:length(urlslist)){
  
  ##read in the URL
  webpage <- read_html(urlslist[i])
  
  ##pull the specific nodes
  headline_data = html_nodes(webpage,'.story-content') 
  
  ##pull the text
  text_data = html_text(headline_data)
  
  ##save the data
  NYtimesDF$Source[i] = "NY Times"
  NYtimesDF$Url[i] = urlslist[i]
  NYtimesDF$Text[i] = paste(text_data, collapse = "")
    } ##end for loop

####NPR original front page####
url2 = 'https://www.npr.org/sections/politics/'
webpage2 = read_html(url2)
headline_data2 = html_nodes(webpage2,'.title a')

#URLs
attr_data2 = html_attrs(headline_data2) 
attr_data2

urlslist2 = unlist(attr_data2)
urlslist2 = urlslist2[grep("http", urlslist2)]
urlslist2

##start a data frame
NPRDF = matrix(NA, nrow = length(urlslist2), ncol = 3)
colnames(NPRDF) = c("Source", "Url", "Text")
NPRDF = as.data.frame(NPRDF)

##for loops
for (i in 1:length(urlslist2)){
  
  ##read in the URL
  webpage2 <- read_html(urlslist2[i])
  
  ##pull the specific nodes
  headline_data2 = html_nodes(webpage2,'#storytext > p') 
  
  ##pull the text
  text_data2 = html_text(headline_data2)
  
  ##save the data
  NPRDF$Source[i] = "NPR"
  NPRDF$Url[i] = urlslist2[i]
  NPRDF$Text[i] = paste(text_data2, collapse = "")
} ##end for loop

####Fox News####
url3 = 'http://www.foxnews.com/politics.html'
url3.1 = 'http://www.foxnews.com/category/politics/executive.html'
url3.2 = 'http://www.foxnews.com/category/politics/senate.htm.html'
url3.3 = 'http://www.foxnews.com/category/politics/house-representatives.html'
url3.4 = 'http://www.foxnews.com/category/politics/judiciary.html'
url3.5 = 'http://www.foxnews.com/category/politics/foreign-policy.html'
url3.6 = 'http://www.foxnews.com/category/politics/elections.html'
webpage3 = read_html(url3)
webpage3.1 = read_html(url3.1)
webpage3.2 = read_html(url3.2)
webpage3.3 = read_html(url3.3)
webpage3.4 = read_html(url3.4)
webpage3.5 = read_html(url3.5)
webpage3.6 = read_html(url3.6)

headline_data3 = html_nodes(webpage3, '.story- a , .article-list .title a')
headline_data3.1 = html_nodes(webpage3.1, '.story- a , .article-list .title a')
headline_data3.2 = html_nodes(webpage3.2, '.story- a , .article-list .title a')
headline_data3.3 = html_nodes(webpage3.3, '.story- a , .article-list .title a')
headline_data3.4 = html_nodes(webpage3.4, '.story- a , .article-list .title a')
headline_data3.5 = html_nodes(webpage3.5, '.story- a , .article-list .title a')
headline_data3.6 = html_nodes(webpage3.6, '.story- a , .article-list .title a')

#headline_data = html_text(headline_data)
head(headline_data3) 

attr_data3 = html_attrs(headline_data3) 
attr_data3.1 = html_attrs(headline_data3.1) 
attr_data3.2 = html_attrs(headline_data3.2) 
attr_data3.3 = html_attrs(headline_data3.3) 
attr_data3.4 = html_attrs(headline_data3.4) 
attr_data3.5 = html_attrs(headline_data3.5) 
attr_data3.6 = html_attrs(headline_data3.6) 

attr_data3

urlslist3 = c(unlist(attr_data3), unlist(attr_data3.1), 
              unlist(attr_data3.2), unlist(attr_data3.3), 
              unlist(attr_data3.4), unlist(attr_data3.5, attr_data3.6))
##find all that are href
urlslist3 = urlslist3[grep("http|.html", urlslist3)]
##fix the ones without the leading foxnews.com
urlslist3F = paste("http://www.foxnews.com", urlslist3[grep("^http", urlslist3, invert = T)], sep = "")
urlslist3N = urlslist3[grep("^http", urlslist3)]
urlslist3 = c(urlslist3N, urlslist3F)
urlslist3 = unique(urlslist3)
urlslist3

##start a data frame
FoxDF = matrix(NA, nrow = length(urlslist3), ncol = 3)
colnames(FoxDF) = c("Source", "Url", "Text")
FoxDF = as.data.frame(FoxDF)

##for loops
for (i in 1:length(urlslist3)){
  
  ##read in the URL
  webpage3 <- read_html(urlslist3[i])
  
  ##pull the specific nodes
  headline_data3 = html_nodes(webpage3,'p+ p , .fn-video+ p , .twitter+ p , .speakable') 
  
  ##pull the text
  text_data3 = html_text(headline_data3)
  
  ##save the data
  FoxDF$Source[i] = "Fox News"
  FoxDF$Url[i] = urlslist3[i]
  FoxDF$Text[i] = paste(text_data3, collapse = "")
} ##end for loop

####Breitbart####
url4 = 'http://www.breitbart.com/big-government/'
webpage4 = read_html(url4)
headline_data4 = html_nodes(webpage4, '.title a , #grid-block-0 span , #BBTrendUL a , #disqus-popularUL a , font')
#headline_data4 = html_text(headline_data4)
head(headline_data4) 

attr_data4 = html_attrs(headline_data4) 
attr_data4

urlslist4 = unlist(attr_data4)
##find all that are href
urlslist4 = urlslist4[grep("http|/big-government", urlslist4)]
##fix the ones without the leading bb
urlslist4F = paste("http://www.breitbart.com", urlslist4[grep("^http", urlslist4, invert = T)], sep = "")
urlslist4N = urlslist4[grep("^http", urlslist4)]
urlslist4 = c(urlslist4N, urlslist4F)
urlslist4 = unique(urlslist4)
urlslist4

##start a data frame
BreitbartDF = matrix(NA, nrow = length(urlslist4), ncol = 3)
colnames(BreitbartDF) = c("Source", "Url", "Text")
BreitbartDF = as.data.frame(BreitbartDF)

##for loops
for (i in 1:length(urlslist4)){
  
  ##read in the URL
  webpage4 <- read_html(urlslist4[i])
  
  ##pull the specific nodes
  headline_data4 = html_nodes(webpage4,'.entry-content p , h2') 
  
  ##pull the text
  text_data4 = html_text(headline_data4)
  
  ##save the data
  BreitbartDF$Source[i] = "Breitbart"
  BreitbartDF$Url[i] = urlslist4[i]
  BreitbartDF$Text[i] = paste(text_data4, collapse = "")
} ##end for loop

####NPR Archive####
url5 = 'https://www.npr.org/sections/politics/archive'
webpage5 = read_html(url5)
headline_data5 = html_nodes(webpage5,'.title a')
#headline_data = html_text(headline_data)
#head(headline_data)

#URLs
attr_data5 = html_attrs(headline_data5) 
attr_data5

urlslist5 = unlist(attr_data5)
urlslist5 = urlslist5[grep("http", urlslist5)]
urlslist5

##start a data frame
NPRArchiveDF = matrix(NA, nrow = length(urlslist5), ncol = 3)
colnames(NPRArchiveDF) = c("Source", "Url", "Text")
NPRArchiveDF = as.data.frame(NPRArchiveDF)

##for loops
for (i in 1:length(urlslist5)){
  
  ##read in the URL
  webpage5 <- read_html(urlslist5[i])
  
  ##pull the specific nodes
  headline_data5 = html_nodes(webpage5,'#storytext > p') 
  
  ##pull the text
  text_data5 = html_text(headline_data5)
  
  ##save the data
  NPRArchiveDF$Source[i] = "NPR"
  NPRArchiveDF$Url[i] = urlslist5[i]
  NPRArchiveDF$Text[i] = paste(text_data5, collapse = "")
} ##end for loop

####put together####
##set your working directory
setwd("~/OneDrive - Missouri State University/RESEARCH/2 projects/Will-Pilot")

##import the overalldata 
overalldata = read.csv("exp1_data/overalldata.csv")

##combine with new data (add the other DF names in here...)
newdata = rbind(overalldata, NYtimesDF, NPRDF, FoxDF, BreitbartDF, NPRArchiveDF)

#temp NPR updates
#newdata = newdata[ , -4]
#newdata = rbind(newdata, NPRArchiveDF)

#change politics archive to NPR, so we can eliminate dupes
#newdata$Source[newdata$Source == "NPR Politics Archive"] = "NPR"
#newdata$Source = droplevels(newdata$Source)

##make the newdata unique in case of overlap across days
newdata = unique(newdata)

##write it back out
#write.csv(newdata, "overalldata.csv", row.names = F)

##number of articles
table(newdata$Source)

##number of words
library(ngram)
newdata$Text = as.character(newdata$Text)
for (i in 1:nrow(newdata)) {
  
  #newdata$writing[i] = preprocess(newdata$Text[i], case="lower", remove.punct=TRUE)
  newdata$wordcount[i] = string.summary(newdata$Text[i])$words
  
}

tapply(newdata$wordcount, newdata$Source, mean)
tapply(newdata$wordcount, newdata$Source, sum)

```

## Data analysis

Once data collection ended, the text was scanned using the *ngram* package in *R* [@Schmidt2017]. This package includes a word count function, which was used to remove articles that came through as blank text, as well as to eliminate text picked up from the Disqus commenting system used by certain websites. At this point, duplicate articles were discarded.

```{r data-cleanup, eval=FALSE, include=FALSE}
##pull in the data
master = read.csv("exp1_data/overalldata.csv", stringsAsFactors = F)

##get rid of the blank stuff
library(ngram)

for (i in 1:nrow(master)) {
  master$wordcount[i] = wordcount(master$Text[i])
}

nozero = subset(master, wordcount > 0)
tapply(nozero$wordcount, nozero$Source, mean)
tapply(nozero$wordcount, nozero$Source, sum)

##take out the disqus ones
nodis = nozero[ -c(grep("disqus", nozero$Url)), ]

##figure out the duplicates
nodis$URLS = duplicated(nodis$Url)

##all data
final = subset(nodis, URLS == FALSE)

tapply(final$wordcount, final$Source, mean)
tapply(final$wordcount, final$Source, sum)

write.csv(final, "finaldata.csv")

```

The article text was processed using the *tm* and *ngram* packages in *R* in order to render the text in lowercase, remove punctuation, and fix spacing issues [@Feinerer2017]. The individual words were then reduced to their stems (i.e., *abused* was stemmed to *abus*). The same procedure was applied to the MFD words and the words in the @Warriner2013 dataset. Using the @Warriner2013 dictionary, the words making up each of the five foundations in the MFD were matched to their respective valence value.   

Concurrent research by @Jordan2019 is assessing the validity of both the Moral Foundations Questionnaire and the Moral Foundations Dictionary through a multi-trait multi-method analysis of the two instruments using multiple samples. The instruments and foundation areas are being analyzed against one another, in order to test reliability, as well as against the Congressional Record in order to test predictive validity for political orientation. The researchers were able to identify a number of potential new words that, if added to the MFD, could comprise a dictionary with greater validity, and less likelihood of zero percent texts, as this often occurs with the current MFD. Those results have informed this analysis, and their updated findings may change the underlying dictionary used in this analysis (albeit, we do not expect any changes in the results presented below). 

The source article words were compiled into a dataset where they were matched up with their counterparts in the MFD along with their valence and a percentage of their occurrence. Therefore, for each article, the percentage of the number of *harm/care* words occurring in the articles were calculated, and this process was repeated for each of the foundations. Words' percent occurrence were multiplied by their *z*-scored valence. Valences were *z*-scored in order to eliminate any ambiguity regarding the direction of the valence. Positive values indicate positive valence, and negative values indicate negative valence. Words were categorized in accordance to their MFD affiliation, creating a weighted sum for each moral foundation.

```{r stem_text, eval=TRUE, include=FALSE}

final = read.csv("exp1_data/finaldata.csv", stringsAsFactors = F)
final = na.omit(final) #one weird NA line

##load libraries
library(tm)
library(ngram)

#process the Text column (write a loop) - save processed text as a separate column
for(i in 1:nrow(final)) {
  
  final$edited[i] = preprocess(final$Text[i], #one value at a time
           case = "lower", 
           remove.punct = TRUE,
           remove.numbers = FALSE, 
           fix.spacing = TRUE)

##stem the words - do this second in the loop
  final$stemmed[i] = stemDocument(final$edited[i], language = "english")
} 

```

```{r stem_warriner, include = FALSE}
##open the Warriner data for later
warriner = read.csv("exp1_data/BRM-emot-submit.csv", stringsAsFactors = F) #want v.mean.sum
warriner$V.Mean.Sum = scale(warriner$V.Mean.Sum)

for(i in 1:nrow(warriner)) {
  warriner$Word2[i] = stemDocument(warriner$Word[i], language = "english")
} 
```

```{r create_datasets_harm, echo = F, include=FALSE}
#read in the harm total words from the mtmm project
harm_all = read.csv("exp1_data/harm_words_total.csv", stringsAsFactors = F)

harm_final_data = data.frame("URL" = NA, "Source" = NA, "Var1" = NA, "Freq" = NA, "wordcount" = NA)

for (i in 1:nrow(final)) {
##first make a table of a response, unlisting everything
temp = as.data.frame(table(unlist(strsplit(final$stemmed[i], " "))))

#only pull the words in the dataset for each foundation
temp2 = temp[temp$Var1 %in% unique(harm_all$words[harm_all$words != "" & harm_all$words != "NA" & !is.na(harm_all$words)]), ]

#control here for times when no words
if (nrow(temp2) == 0){
  temp2 = data.frame("Var1" = 0, "Freq" = 0)
}

#add to the dataset
temp2$URL = final$Url[i]
temp2$Source = final$Source[i]
temp2$wordcount = final$wordcount[i]

#bind with original
harm_final_data = rbind(harm_final_data, temp2)
}

#pull in the data from Warriner 
harm_final_data$valence = vlookup(harm_final_data$Var1, #index in original data
                                  warriner, #where's the stuff you want to put in
                                  "V.Mean.Sum", #what column is the stuff you want
                                  "Word2") #what's the matching index

#figure out stupid words
#table(harm_final_data$Var1[is.na(harm_final_data$valence) & harm_final_data$Freq > 0])
#fought, safer, spurn
harm_final_data$valence[harm_final_data$Var1 == "fought"] = warriner$V.Mean.Sum[warriner$Word2 == "fight"]
harm_final_data$valence[harm_final_data$Var1 == "safer"] = warriner$V.Mean.Sum[warriner$Word2 == "safe"]
#no spurn avaliable
```

```{r create_datasets_fair, include=FALSE}
fair_all = read.csv("exp1_data/fair_words_total.csv", stringsAsFactors = F)

fair_final_data = data.frame("URL" = NA, "Source" = NA, "Var1" = NA, "Freq" = NA, "wordcount" = NA)

for (i in 1:nrow(final)) {
##first make a table of a response, unlisting everything
temp = as.data.frame(table(unlist(strsplit(final$stemmed[i], " "))))

#only pull the words in the dataset for each foundation
temp2 = temp[temp$Var1 %in% unique(fair_all$words[fair_all$words != "" & fair_all$words != "NA" & !is.na(fair_all$words)]), ]

#control here for times when no words
if (nrow(temp2) == 0){
  temp2 = data.frame("Var1" = 0, "Freq" = 0)
}

#add to the dataset
temp2$URL = final$Url[i]
temp2$Source = final$Source[i]
temp2$wordcount = final$wordcount[i]

#bind with original
fair_final_data = rbind(fair_final_data, temp2)
}

fair_final_data$valence = vlookup(fair_final_data$Var1, #index in original data
                                  warriner, #where's the stuff you want to put in
                                  "V.Mean.Sum", #what column is the stuff you want
                                  "Word2") #what's the matching index

#figure out stupid words
table(fair_final_data$Var1[is.na(fair_final_data$valence) & fair_final_data$Freq > 0])
#just plagiar safer
fair_final_data$valence[fair_final_data$Var1 == "safer"] = warriner$V.Mean.Sum[warriner$Word2 == "safe"]
#nothing for just or plagiarism
```

```{r create_datasets_ingroup, include=FALSE}
ingroup_all = read.csv("exp1_data/ingroup_words_total.csv", stringsAsFactors = F)

ingroup_final_data = data.frame("URL" = NA, "Source" = NA, "Var1" = NA, "Freq" = NA, "wordcount" = NA)

for (i in 1:nrow(final)) {
##first make a table of a response, unlisting everything
temp = as.data.frame(table(unlist(strsplit(final$stemmed[i], " "))))

#only pull the words in the dataset for each foundation
temp2 = temp[temp$Var1 %in% unique(ingroup_all$words[ingroup_all$words != "" & ingroup_all$words != "NA" & !is.na(ingroup_all$words)]), ]

#control here for times when no words
if (nrow(temp2) == 0){
  temp2 = data.frame("Var1" = 0, "Freq" = 0)
}

#add to the dataset
temp2$URL = final$Url[i]
temp2$Source = final$Source[i]
temp2$wordcount = final$wordcount[i]

#bind with original
ingroup_final_data = rbind(ingroup_final_data, temp2)
}

ingroup_final_data$valence = vlookup(ingroup_final_data$Var1, #index in original data
                                  warriner, #where's the stuff you want to put in
                                  "V.Mean.Sum", #what column is the stuff you want
                                  "Word2") #what's the matching index

#figure out stupid words
table(ingroup_final_data$Var1[is.na(ingroup_final_data$valence) & ingroup_final_data$Freq > 0])
#backstabb  benedict      best      fals   indvidu      juda    knight      true
ingroup_final_data$valence[ingroup_final_data$Var1 == "fals"] = warriner$V.Mean.Sum[warriner$Word2 == "FALSE"]
ingroup_final_data$valence[ingroup_final_data$Var1 == "indvidu"] = mean(warriner$V.Mean.Sum[warriner$Word2 == "individu"])
ingroup_final_data$valence[ingroup_final_data$Var1 == "true"] = mean(warriner$V.Mean.Sum[warriner$Word2 == "TRUE"])
```

```{r create_datasets_authority, include=FALSE}
authority_all = read.csv("exp1_data/authority_words_total.csv", stringsAsFactors = F)

authority_final_data = data.frame("URL" = NA, "Source" = NA, "Var1" = NA, "Freq" = NA, "wordcount" = NA)

for (i in 1:nrow(final)) {
##first make a table of a response, unlisting everything
temp = as.data.frame(table(unlist(strsplit(final$stemmed[i], " "))))

#only pull the words in the dataset for each foundation
temp2 = temp[temp$Var1 %in% unique(authority_all$words[authority_all$words != "" & authority_all$words != "NA" & !is.na(authority_all$words)]), ]

#control here for times when no words
if (nrow(temp2) == 0){
  temp2 = data.frame("Var1" = 0, "Freq" = 0)
}

#add to the dataset
temp2$URL = final$Url[i]
temp2$Source = final$Source[i]
temp2$wordcount = final$wordcount[i]

#bind with original
authority_final_data = rbind(authority_final_data, temp2)
}

authority_final_data$valence = vlookup(authority_final_data$Var1, #index in original data
                                  warriner, #where's the stuff you want to put in
                                  "V.Mean.Sum", #what column is the stuff you want
                                  "Word2") #what's the matching index

#figure out stupid words
table(authority_final_data$Var1[is.na(authority_final_data$valence) & authority_final_data$Freq > 0])
#  abov behind   gase higher  older  taken 
authority_final_data$valence[authority_final_data$Var1 == "gase"] = warriner$V.Mean.Sum[warriner$Word2 == "gas"]
authority_final_data$valence[authority_final_data$Var1 == "higher"] = mean(warriner$V.Mean.Sum[warriner$Word2 == "high"])
authority_final_data$valence[authority_final_data$Var1 == "older"] = mean(warriner$V.Mean.Sum[warriner$Word2 == "old"])
authority_final_data$valence[authority_final_data$Var1 == "taken"] = mean(warriner$V.Mean.Sum[warriner$Word2 == "take"])
```

```{r create_datasets_purity, include=FALSE}
purity_all = read.csv("exp1_data/purity_words_total.csv", stringsAsFactors = F)

purity_final_data = data.frame("URL" = NA, "Source" = NA, "Var1" = NA, "Freq" = NA, "wordcount" = NA)

for (i in 1:nrow(final)) {
##first make a table of a response, unlisting everything
temp = as.data.frame(table(unlist(strsplit(final$stemmed[i], " "))))

#only pull the words in the dataset for each foundation
temp2 = temp[temp$Var1 %in% unique(purity_all$words[purity_all$words != "" & purity_all$words != "NA" & !is.na(purity_all$words)]), ]

#control here for times when no words
if (nrow(temp2) == 0){
  temp2 = data.frame("Var1" = 0, "Freq" = 0)
}

#add to the dataset
temp2$URL = final$Url[i]
temp2$Source = final$Source[i]
temp2$wordcount = final$wordcount[i]

#bind with original
purity_final_data = rbind(purity_final_data, temp2)
}

purity_final_data$valence = vlookup(purity_final_data$Var1, #index in original data
                                  warriner, #where's the stuff you want to put in
                                  "V.Mean.Sum", #what column is the stuff you want
                                  "Word2") #what's the matching index

#figure out stupid words
table(purity_final_data$Var1[is.na(purity_final_data$valence) & purity_final_data$Freq > 0])
#befor christian     jesus      mari      none    sacred    someon      true     women 
purity_final_data$valence[purity_final_data$Var1 == "sacred"] = warriner$V.Mean.Sum[warriner$Word2 == "sacr"]
purity_final_data$valence[purity_final_data$Var1 == "true"] = mean(warriner$V.Mean.Sum[warriner$Word2 == "TRUE"])
purity_final_data$valence[purity_final_data$Var1 == "women"] = mean(warriner$V.Mean.Sum[warriner$Word2 == "woman"])
```

```{r descriptives, echo = F, include = FALSE}
#summarize the dataset so that you have the total and percent for each foundation 
# summary(harm_final_data)
# summary(fair_final_data)
# summary(ingroup_final_data)
# summary(authority_final_data)
# summary(purity_final_data)
# #report descriptive statistics of the foundations
# mean(harm_final_data$valence)
# sd(harm_final_data$valence)
# mean(fair_final_data$valence)
# sd(fair_final_data$valence)
# mean(ingroup_final_data$valence)
# sd(ingroup_final_data$valence)
# mean(authority_final_data$valence)
# sd(authority_final_data$valence)
# mean(purity_final_data$valence)
# sd(purity_final_data$valence)
#note that a big problem is the valence of the words 
```

```{r analysis_harm, include = FALSE}
#final_data clean up
#multiply by frequency and create percentage
harm_final_data$weight = harm_final_data$Freq * harm_final_data$valence / harm_final_data$wordcount * 100

#flatten down to sum by article 
harm_final_temp = aggregate(harm_final_data$weight, 
          by = list(harm_final_data$URL),
          FUN = sum, na.rm = T)
colnames(harm_final_temp) = c("URL", "MFDweight")

harm_final_temp$lean[grepl("nytimes.com", harm_final_temp$URL) |
                       grepl("npr.org", harm_final_temp$URL)] = "Liberal"
harm_final_temp$lean[grepl("breitbart.com", harm_final_temp$URL) |
                       grepl("foxnews.com", harm_final_temp$URL)] = "Conservative"

harm_final_temp$Source[grepl("nytimes.com", harm_final_temp$URL)] = "NY Times"
harm_final_temp$Source[grepl("npr.org", harm_final_temp$URL)] = "NPR"
harm_final_temp$Source[grepl("foxnews.com", harm_final_temp$URL)] = "Fox"
harm_final_temp$Source[grepl("breitbart.com", harm_final_temp$URL)] = "Breitbart"

harm_model = lme(MFDweight ~ lean, 
                 data = harm_final_temp, 
                 method = "ML",
                 na.action = "na.omit",
                 random = list(~1|Source))
summary(harm_model)
```

```{r analysis_fair, include = FALSE}
#final_data clean up
#multiply by frequency and create percentage
fair_final_data$weight = fair_final_data$Freq * fair_final_data$valence / fair_final_data$wordcount * 100

#flatten down to sum by article 
fair_final_temp = aggregate(fair_final_data$weight, 
          by = list(fair_final_data$URL),
          FUN = sum, na.rm = T)
colnames(fair_final_temp) = c("URL", "MFDweight")

fair_final_temp$lean[grepl("nytimes.com", fair_final_temp$URL) |
                       grepl("npr.org", fair_final_temp$URL)] = "Liberal"
fair_final_temp$lean[grepl("breitbart.com", fair_final_temp$URL) |
                       grepl("foxnews.com", fair_final_temp$URL)] = "Conservative"

fair_final_temp$Source[grepl("nytimes.com", fair_final_temp$URL)] = "NY Times"
fair_final_temp$Source[grepl("npr.org", fair_final_temp$URL)] = "NPR"
fair_final_temp$Source[grepl("foxnews.com", fair_final_temp$URL)] = "Fox"
fair_final_temp$Source[grepl("breitbart.com", fair_final_temp$URL)] = "Breitbart"

fair_model = lme(MFDweight ~ lean, 
                 data = fair_final_temp, 
                 method = "ML",
                 na.action = "na.omit",
                 random = list(~1|Source))
summary(fair_model)
```

```{r analysis_ingroup, include = FALSE}
#final_data clean up
#multiply by frequency and create percentage
ingroup_final_data$weight = ingroup_final_data$Freq * ingroup_final_data$valence / ingroup_final_data$wordcount * 100

#flatten down to sum by article 
ingroup_final_temp = aggregate(ingroup_final_data$weight, 
          by = list(ingroup_final_data$URL),
          FUN = sum, na.rm = T)
colnames(ingroup_final_temp) = c("URL", "MFDweight")

ingroup_final_temp$lean[grepl("nytimes.com", ingroup_final_temp$URL) |
                       grepl("npr.org", ingroup_final_temp$URL)] = "Liberal"
ingroup_final_temp$lean[grepl("breitbart.com", ingroup_final_temp$URL) |
                       grepl("foxnews.com", ingroup_final_temp$URL)] = "Conservative"

ingroup_final_temp$Source[grepl("nytimes.com", ingroup_final_temp$URL)] = "NY Times"
ingroup_final_temp$Source[grepl("npr.org", ingroup_final_temp$URL)] = "NPR"
ingroup_final_temp$Source[grepl("foxnews.com", ingroup_final_temp$URL)] = "Fox"
ingroup_final_temp$Source[grepl("breitbart.com", ingroup_final_temp$URL)] = "Breitbart"

ingroup_model = lme(MFDweight ~ lean, 
                 data = ingroup_final_temp, 
                 method = "ML",
                 na.action = "na.omit",
                 random = list(~1|Source))
summary(ingroup_model)
```

```{r analysis_authority, include = FALSE}
#final_data clean up
#multiply by frequency and create percentage
authority_final_data$weight = authority_final_data$Freq * authority_final_data$valence / authority_final_data$wordcount * 100

#flatten down to sum by article 
authority_final_temp = aggregate(authority_final_data$weight, 
          by = list(authority_final_data$URL),
          FUN = sum, na.rm = T)
colnames(authority_final_temp) = c("URL", "MFDweight")

authority_final_temp$lean[grepl("nytimes.com", authority_final_temp$URL) |
                       grepl("npr.org", authority_final_temp$URL)] = "Liberal"
authority_final_temp$lean[grepl("breitbart.com", authority_final_temp$URL) |
                       grepl("foxnews.com", authority_final_temp$URL)] = "Conservative"

authority_final_temp$Source[grepl("nytimes.com", authority_final_temp$URL)] = "NY Times"
authority_final_temp$Source[grepl("npr.org", authority_final_temp$URL)] = "NPR"
authority_final_temp$Source[grepl("foxnews.com", authority_final_temp$URL)] = "Fox"
authority_final_temp$Source[grepl("breitbart.com", authority_final_temp$URL)] = "Breitbart"

authority_model = lme(MFDweight ~ lean, 
                 data = authority_final_temp, 
                 method = "ML",
                 na.action = "na.omit",
                 random = list(~1|Source))
summary(authority_model)
```

```{r analysis_purity, include = FALSE}
#final_data clean up
#multiply by frequency and create percentage
purity_final_data$weight = purity_final_data$Freq * purity_final_data$valence / purity_final_data$wordcount * 100

#flatten down to sum by article 
purity_final_temp = aggregate(purity_final_data$weight, 
          by = list(purity_final_data$URL),
          FUN = sum, na.rm = T)
colnames(purity_final_temp) = c("URL", "MFDweight")

purity_final_temp$lean[grepl("nytimes.com", purity_final_temp$URL) |
                       grepl("npr.org", purity_final_temp$URL)] = "Liberal"
purity_final_temp$lean[grepl("breitbart.com", purity_final_temp$URL) |
                       grepl("foxnews.com", purity_final_temp$URL)] = "Conservative"

purity_final_temp$Source[grepl("nytimes.com", purity_final_temp$URL)] = "NY Times"
purity_final_temp$Source[grepl("npr.org", purity_final_temp$URL)] = "NPR"
purity_final_temp$Source[grepl("foxnews.com", purity_final_temp$URL)] = "Fox"
purity_final_temp$Source[grepl("breitbart.com", purity_final_temp$URL)] = "Breitbart"

purity_model = lme(MFDweight ~ lean, 
                 data = purity_final_temp, 
                 method = "ML",
                 na.action = "na.omit",
                 random = list(~1|Source))
summary(purity_model)
```

# Results

## Descriptive Statistics

```{r exp1-source-descriptives, results = 'asis', echo = FALSE, message = F}

final$uniquewords = NA
final$valenceavg = NA

for (i in 1:nrow(final)){
  
  #add unique
  final$uniquewords[i] = length(unique(unlist(strsplit(final$stemmed[i], " "))))
  
  #add valence
  tempwords = as.data.frame(unlist(strsplit(final$stemmed[i], " ")))
  colnames(tempwords) = "theword"
  tempwords$tempval = vlookup(tempwords$theword, #index in original data
                    warriner, #where's the stuff you want to put in
                    "V.Mean.Sum", #what column is the stuff you want
                    "Word2") #what's the matching index
  
  final$valenceavg[i] = mean(tempwords$tempval, na.rm = T)

}


# tapply(final$valenceavg, final$Source, mean)
# tapply(final$valenceavg, final$Source, sd)
# tapply(final$wordcount, final$Source, mean)
# tapply(final$wordcount, final$Source, sd)
# tapply(final$uniquewords, final$Source, mean)
# tapply(final$uniquewords, final$Source, sd)
# tapply(final$Source, final$Source, length)
# tapply(final$wordcount, final$Source, sum)

library(quanteda)

final$FK = NA

for (i in 1:nrow(final)){
  final$FK[i] = unlist(textstat_readability(final$Text[i], 
                     measure = "Flesch.Kincaid")$Flesch.Kincaid)
}

# tapply(final$FK, final$Source, mean)
# tapply(final$FK, final$Source, sd)

####build the table here####
tableprint = matrix(NA, nrow=4, ncol=11)
colnames(tableprint) = c("Source", "Mean_Valence", "SD_Valence", "Number_of_Articles", "Total_Words", "Mean_Tokens", "SD_Tokens", "Mean_Types", "SD_Types", "Mean_Readability", "SD_Readability")

#tableprint[ , 1] = unique(final$Source)
tableprint[ , 1] = c("Breitbart", "Fox News", "NPR", "New York Times")
tableprint[ , 2] = tapply(final$valenceavg, final$Source, mean)
tableprint[ , 3] = tapply(final$valenceavg, final$Source, sd)
tableprint[ , 4] = tapply(final$valenceavg, final$Source, length)
tableprint[ , 5] = tapply(final$wordcount, final$Source, sum)
tableprint[ , 6] = tapply(final$wordcount, final$Source, mean)
tableprint[ , 7] = tapply(final$wordcount, final$Source, sd)
tableprint[ , 8] = tapply(final$uniquewords, final$Source, mean)
tableprint[ , 9] = tapply(final$uniquewords, final$Source, sd)
tableprint[ , 10] = tapply(final$FK, final$Source, mean)
tableprint[ , 11] = tapply(final$FK, final$Source, sd)

tableprint[ , c(2,3,6,7,8,9,10,11)] = printnum(as.numeric(tableprint[ , c(2,3,6,7,8,9,10,11)]))

tableprint = tableprint[c(3, 4, 1, 2) , ]

#change this to match
apa_table(as.data.frame(tableprint),
          note = "Readability statistics were calculated using the Flesch-Kincaid Grade Level readability formula. V = Valence, T = Tokens or total words, Ty = Types or unique words, FK = Flesch-Kincaid",
          caption = "Experiment 1 - Descriptive Statistics by Source", 
          col.names = c("Source", "$M_V$", "$SD_V$", "$N_{Article}$", "$N_{Words}$", "$M_T$", "$SD_T$", "$M_{Ty}$", "$SD_{Ty}$", "$M_{FK}$", "$SD_{FK}$"),
          format = "latex",
          escape = F,
          align = c("l", rep("c", 6))
          )
```

The researchers calculated descriptive statistics for each news source in order to understand any and all fundamental linguistic differences in the sources' use of English. Statistics calculated included average *z*-scored valence of the unique words per article, number of articles per source, total number of words per source, average number of tokens (words) per article in each source, average number of types (unique words) per article in each source, and mean readability level per source. Readability statistics were calculated using the Flesch-Kincaid Grade Level Readability formula [@Kincaid1975]. Readability is calculated using a formula where the total number of syllables, words, and sentences in a given passage are determinants of its difficulty. The obtained value is intended to match up with the grade level at which one should be able to comfortably read the passage [@Kincaid1975]. For example, a text with a readability score of 11 should be easily read by a high school junior.

As seen in Table \@ref(tab:exp1-source-descriptives), the sources are similar in some aspects yet different in others. Valence appears to be slightly positive across all sources. The large standard deviations seem to indicate little to no presence of a difference in valence across sources. 
*The New York Times* published the greatest number of articles as well as total words. *Breitbart* featured the lowest number of articles, and *NPR* the lowest number of total words from all articles. Per individual article, however, *Breitbart* appears to feature the highest average number of words as well as unique words. Once again the standard deviations call into question any apparent differences between sources. Finally, *Fox News* articles had the lowest reading grade level on average, while *The New York Times* ihad the highest. This result might be attributable to the greater number of tokens in the average *New York Times* article compared to *Fox News*. The standard deviations for readability suggest the presence of a diverse array of articles for each source, ranging from low to high reading level. Large standard deviations suggest the sources feature a lot of overlap between them in their representation of scores. 

## Inferential Statistics

To analyze if news sources adhered to differences in word use based on their target audience, we utilized a multilevel model (MLM) to analyze the data. MLM is a regression technique that allows one to control for the repeated measurement and nested structured of the data, which creates correlated error [@Gelman2006]. Using the *nlme* library in *R* [@Pinheiro2017], each foundation's weighted percentage was predicated here by the political lean of the news source, using the individual news sources as a random intercept to control for the structure of the data.

The multilevel model did not indicate the presence of any significant or practical effect of political lean for any of the five moral foundations. The strongest effect size was observed for the *authority/respect* foundation, but the effect was in the opposite direction from what was originally hypothesized - liberal sources tended to use more *authority/respect* words than did conservative sources. Descriptive and test statistics, *p*-values and effect sizes (Cohen's *d*) can be found in Table \@ref(tab:exp1-table). To interpret the weighted scores, one can examine the mean and standard deviations for each. A zero score for the mean, with a non-zero standard deviation, would indicate a perfect balance of positive and negative words in each category, likely representing a neutral tone when all words are considered. Negative percentages would indicate more representation of the negative words in the MFD area, while positive percentages indicate an endorsement of the positive words in a MFD. Therefore, we suggest using the sign of the mean score to determine the directionality of the endorsement for the MFD (positive, neutral, negative), and the standard deviation to ensure that a zero score is not zero endorsement (i.e., a SD of zero indicates no words were used). Based on the weighted percent values for the five foundations, the researchers observed that MFD words seem to make up a small portion of the article text. Furthermore, the observed percentages and means appear to indicate a generally positive endorsement of all five foundations across both liberal and conservative sources.

```{r exp1-table, results = 'asis', echo = F}
#MFD, mean, sd, mean, sd, t, p, d
tableprint = matrix(NA, ncol = 8, nrow = 5)
colnames(tableprint) = c("Foundation", "M_C", "SD_C", "M_L", "SD_L", "t", "p", "d")

#calculate d
#harm
harm_d = d.ind.t(m1 = mean(harm_final_temp$MFDweight[harm_final_temp$lean == "Conservative"], na.rm = T), 
                 m2 = mean(harm_final_temp$MFDweight[harm_final_temp$lean == "Liberal"], na.rm = T),
                 sd1 = sd(harm_final_temp$MFDweight[harm_final_temp$lean == "Conservative"], na.rm = T),
                 sd2 = sd(harm_final_temp$MFDweight[harm_final_temp$lean == "Liberal"], na.rm = T),
                 n1 = length(harm_final_temp$MFDweight[harm_final_temp$lean == "Conservative"]),
                 n2 = length(harm_final_temp$MFDweight[harm_final_temp$lean == "Liberal"]),
                 a = .05)

#fair
fair_d = d.ind.t(m1 = mean(fair_final_temp$MFDweight[fair_final_temp$lean == "Conservative"], na.rm = T), 
                 m2 = mean(fair_final_temp$MFDweight[fair_final_temp$lean == "Liberal"], na.rm = T),
                 sd1 = sd(fair_final_temp$MFDweight[fair_final_temp$lean == "Conservative"], na.rm = T),
                 sd2 = sd(fair_final_temp$MFDweight[fair_final_temp$lean == "Liberal"], na.rm = T),
                 n1 = length(fair_final_temp$MFDweight[fair_final_temp$lean == "Conservative"]),
                 n2 = length(fair_final_temp$MFDweight[fair_final_temp$lean == "Liberal"]),
                 a = .05)

#ingroup
ingroup_d = d.ind.t(m1 = mean(ingroup_final_temp$MFDweight[ingroup_final_temp$lean == "Conservative"], na.rm = T), 
                 m2 = mean(ingroup_final_temp$MFDweight[ingroup_final_temp$lean == "Liberal"], na.rm = T),
                 sd1 = sd(ingroup_final_temp$MFDweight[ingroup_final_temp$lean == "Conservative"], na.rm = T),
                 sd2 = sd(ingroup_final_temp$MFDweight[ingroup_final_temp$lean == "Liberal"], na.rm = T),
                 n1 = length(ingroup_final_temp$MFDweight[ingroup_final_temp$lean == "Conservative"]),
                 n2 = length(ingroup_final_temp$MFDweight[ingroup_final_temp$lean == "Liberal"]),
                 a = .05)

#authority
authority_d = d.ind.t(m1 = mean(authority_final_temp$MFDweight[authority_final_temp$lean == "Conservative"], na.rm = T), 
                 m2 = mean(authority_final_temp$MFDweight[authority_final_temp$lean == "Liberal"], na.rm = T),
                 sd1 = sd(authority_final_temp$MFDweight[authority_final_temp$lean == "Conservative"], na.rm = T),
                 sd2 = sd(authority_final_temp$MFDweight[authority_final_temp$lean == "Liberal"], na.rm = T),
                 n1 = length(authority_final_temp$MFDweight[authority_final_temp$lean == "Conservative"]),
                 n2 = length(authority_final_temp$MFDweight[authority_final_temp$lean == "Liberal"]),
                 a = .05)

#purity
purity_d = d.ind.t(m1 = mean(purity_final_temp$MFDweight[purity_final_temp$lean == "Conservative"], na.rm = T), 
                 m2 = mean(purity_final_temp$MFDweight[purity_final_temp$lean == "Liberal"], na.rm = T),
                 sd1 = sd(purity_final_temp$MFDweight[purity_final_temp$lean == "Conservative"], na.rm = T),
                 sd2 = sd(purity_final_temp$MFDweight[purity_final_temp$lean == "Liberal"], na.rm = T),
                 n1 = length(purity_final_temp$MFDweight[purity_final_temp$lean == "Conservative"]),
                 n2 = length(purity_final_temp$MFDweight[purity_final_temp$lean == "Liberal"]),
                 a = .05)

#put in the numbers
tableprint[1, ] = c("Harm/Care", harm_d$M1, harm_d$sd1, harm_d$M2, harm_d$sd1, 
                    summary(harm_model)$tTable[2,4], summary(harm_model)$tTable[2,5],
                    harm_d$d)
tableprint[2, ] = c("Fairness/Reciprocity", fair_d$M1, fair_d$sd1, fair_d$M2, fair_d$sd1, 
                    summary(fair_model)$tTable[2,4], summary(fair_model)$tTable[2,5],
                    fair_d$d)
tableprint[3, ] = c("Ingroup/Loyalty", ingroup_d$M1, ingroup_d$sd1, ingroup_d$M2, ingroup_d$sd1, 
                    summary(ingroup_model)$tTable[2,4], summary(ingroup_model)$tTable[2,5],
                    ingroup_d$d)
tableprint[4, ] = c("Authority/Respect", authority_d$M1, authority_d$sd1, authority_d$M2, authority_d$sd1, 
                    summary(authority_model)$tTable[2,4], summary(authority_model)$tTable[2,5],
                    authority_d$d)
tableprint[5, ] = c("Purity/Sanctity", purity_d$M1, purity_d$sd1, purity_d$M2, purity_d$sd1, 
                    summary(purity_model)$tTable[2,4], summary(purity_model)$tTable[2,5],
                    purity_d$d)

tableprint[ , c(2:6, 8)] = printnum(as.numeric(tableprint[ , c(2:6, 8)]))
tableprint[ , 7] = printp(as.numeric(tableprint[ , 7]))

apa_table(tableprint,
          note = "For mean and standard deviation values, 'C' and 'L' refer to 'conservative' and 'liberal,' respectively",
          caption = "Experiment 1 Results - Multilevel Model", 
          col.names = c("Foundation", "$M_C$", "$SD_C$", "$M_L$", "$SD_L$", "$t$", "$p$", "$d$"),
          format = "latex",
          escape = F,
          align = c("l", rep("c", 7))
          )
```

```{r conference-pic, eval = F, include = F}
library(ggplot2)
library(reshape)

cleanup = theme(panel.grid.major = element_blank(), 
                panel.grid.minor = element_blank(), 
                panel.background = element_blank(), 
                axis.line.x = element_line(color = "black"),
                axis.line.y = element_line(color = "black"),
                legend.key = element_rect(fill = "white"),
                text = element_text(size = 10))

graphdata = as.data.frame(tableprint)
graphlong = melt(graphdata[ , c(1,2,4)], 
                 id = "Foundation")
graphlong2 = melt(graphdata[ , c(1, 3,5)],
                  id = "Foundation",
                  measured = c("SD_C", "SD_L"))
graphlong$sd = graphlong2$value
graphlong$lower = as.numeric(as.character(graphlong$value)) - 2*as.numeric(as.character(graphlong$sd))
graphlong$upper = as.numeric(as.character(graphlong$value)) + 2*as.numeric(as.character(graphlong$sd))

graphlong$value = as.numeric(as.character(graphlong$value))

graph_data = ggplot(graphlong, aes(Foundation, value, color = variable)) + 
  geom_point(position = position_dodge(width = 0.90)) +
  geom_errorbar(aes(ymin = lower, ymax = upper), 
                position = position_dodge(width = 0.90),
                width = .2) + 
  cleanup +
  ylab("Mean Weighted Valence") + 
  scale_color_manual(values = c("red", "blue"),
                       labels = c("Conservative", "Liberal"),
                       name = "Party") +
  theme(axis.text.x = element_text(angle = 60, hjust = 1)) + 
  geom_hline(yintercept = 0, linetype = 1, color = "black")
  
tiff(filename = "exp1_graph.tiff", res = 300, width = 4, 
     height = 3, units = 'in', compression = "lzw")
plot(graph_data)
dev.off()

```

# Discussion 

The results obtained in Experiment 1 did not confirm the hypothesis. The researchers found little compelling evidence of an effect of partisan lean on MFD endorsement. The strongest effect found was for the *authority/respect* foundation owing to the fact its Cohen's *d* value was greater than the other four foundations. However, the effect was in the opposite direction of that which was hypothesized. Specifically, the results indicated that liberal leaning sources demonstrated higher positivity regarding that foundation than conservatives. This result is contrary not only to the research hypothesis for Experiment 1 but also to previous findings in Moral Foundations Theory research. It should be noted, however, the effect size was small and the relationship was not found to be statistically significant. 

Upon speculation, the researchers identified one possible reason for why the results were unable to confirm the hypothesis. The selection of the broad and amorphous topic of "political news" may have led to the scraping of large numbers of articles with little to no moral-centric content. Rather, many articles may have been, for example, simple reporting on congressional procedures that would leave little room for the use of moral language here, let alone words from the Moral Foundations Dictionary. In short, the range of topics covered in Experiment 1 was likely too broad. The possibility exists that a tighter focus on one political issue or event, especially one that (on the surface) has a stronger relationship with morality might be more illuminating for research in moral language in news media.

Owing to the exploratory nature of Experiment 1, the researchers were afforded the opportunity to consider changes to the method to be utilized in Experiment 2. Generally speaking, the researchers believe their methodology to be sound. Web scraping methods and text processing remain viable methods for collecting large amounts of text and subsequently rendering that text in a form suitable for data analysis. Experiment 1 also demonstrated a method by which to address inherent problems in the Moral Foundations Dictionary relating to valence. The solution provided in Experiment 1 appears to provide insights into the MFD words where none previously existed. Finally, calculating weighted percentages and sums for each moral foundation provides an easily interpreted summary of MFD word positivity and occurrence. 

While the methodology used in Experiment 1 features many strengths, there are aspects which could be strengthened for future studies. The researchers identified two such changes that were subsequently employed in Experiment 2. First, the researchers elected to include more news sources for web scraping and analysis in addition to the four used in Experiment 1. Second, the researchers chose to focus their data collection efforts exclusively on one event in U.S. politics: the nomination and confirmation of Justice Brett Kavanaugh to the U.S. Supreme Court. In Experiment 2, the researchers sought to confirm the usefulness and validity of the method as well as test a similar hypothesis as Experiment 1.

# Experiment 2

## Kavanaugh Supreme Court Hearing

In the wake of Justice Anthony Kennedy's retirement from the Supreme Court of the United States, President Donald Trump nominated Brett Kavanaugh as the new Associate Justice. Kavanaugh was previously on the U.S. Court of Appeals for the District of Columbia. The Senate Judiciary Committee began his confirmation hearing on September 4, 2018 [@USGovernment2018b]. Following allegations of sexual assault by high school classmate Dr. Christine Blasey Ford, the committee postponed its vote on whether or not to open the confirmation to the entire Senate. 

On September 27, the committee questioned Dr. Ford before commencing a second round of questioning for Judge Kavanaugh [@USGovernment2018]. During the intervening weeks between hearings, two more women came forward with two separate allegations of sexual assault on the part of Kavanaugh. According to Nielsen reports, more than 20 million people watched the September 27 proceedings on television [@OConnell2018]. This figure does not take into account viewers who watched online, nor does it account for viewers outside the United States. On September 28, the Senate Judiciary Committee voted to send the nomination to the Senate floor. Senator Jeff Flake of Arizona, however, lobbied for a week-long FBI investigation on Kavanaugh and the allegations facing him, which the committee, and later the President, approved. The investigation concluded with no significant findings. The Senate voted 50-48 to approve Kavanaugh's appointment on October 6, 2018 [@USGovernment2018a].

The Kavanaugh nomination, confirmation hearing, and eventual swearing-in, as well as the news media's coverage of all three events, feature many dimensions that likely differ depending on one's morals. The issue might be exacerbated given the presence of questionable sexual behaviors at the center of many concerns. On one side of the debate, Kavanaugh's Supreme Court tenure presents a prime opportunity to bring morality back into interpretation of the Constitution. Kavanaugh's confirmation creates a conservative stronghold among the justices on the court. Commentators have noted this might help advance a judicial agenda that backpedals certain rights previously upheld by the Supreme Court, including abortion and gay marriage - social issues challenged by their opponents at least partially on moral grounds. Concerns around abortion might be related either to *harm/care* or *purity/sanctity*. On the other side of the debate, the assault allegations have energized Kavanaugh's opponents to advocate for his rejection from the court owing to misdeeds resulting from Kavanaugh's own alleged lack of morals. Likewise, arguments could be made that relate concerns regarding sexual violence to *harm/care* or *fairness/reciprocity*. Additionally, the moral duty of the Senate as the upper chamber in the U.S. legislature has been scrutinized in public discourse with respect to its handling of the assault allegations vis-a-vis Kavanaugh's confirmation. 

## U.S. Government Shutdown of 2018-2019

The U.S. Federal Government partially shut down on December 22, 2018. The government reopened on January 25, 2019 upon the passage of an appropriations bill by both houses of Congress [@Axelrod2019]. The shutdown stemmed from a disagreement between President Donald Trump and Congress over funding for the president's proposed U.S.-Mexico border wall. President Trump demanded $5.7 billion dollars in the new budget to be appropriated for the wall, which Congress did not provide in its budget [@Peoples2019]. Owing to the lack of funding, the government began a partial shutdown, causing "around 800,000" federal employees to be either furloughed or compelled to work without pay [@Axelrod2019].

The government shutdown ended 35 days later when Congress passed a continuing resolution to fund the government for three weeks while further negotiations regarding the budget would take place. President Trump signed the bill, ending the longest government shutdown in U.S. history [@Axelrod2019]. Many political commentators cite the mounting pressure on Trump and Congress stemming from significant costs both to unpaid American workers and the economy as a whole [@Kheel2019]. Perhaps owing to the financial pressures experienced by thousands of federal employees and their families as a result of a political quarrel, President Trump saw his approval rating fall as the shutdown progressed [@Peoples2019]. While the shutdown and its associated burdens were unpopular, the issues discussed concurrent to the shutdown offer the potential for more divergent opinions.

As mentioned before, the shutdown started as a result of an impasse regarding funding for the border wall. Since the wall was proposed during the 2016 presidential campaign, the issue has taken on a moral dimension. As an example, Shaun Casey, the director of Georgetown University's Berkely Center for Religion, Peace and World Affairs, cites diverging opinions stemming from theological concerns [@Martin2019]. Some supporters, citing certain passages from the Bible, believe construction of the wall is part of a divinely ordained process handed down by God. Opponents, on the other hand, have cited other parts of the Bible extolling the significance of exiled peoples, especially the Hebrew people [@Martin2019]. This example potentially ties into the five moral foundations. Theological concerns regarding the border wall might invoke *authority/respect* as well as *purity/sanctity*. In addition to these two foundations, the involvement of foreign countries and their citizens' migration into the U.S. could also elicit moral concerns related to *ingroup/outgroup*.

Owing to the wealth of moral opinions regarding the border wall and its association with the 2018-2019 government shutdown, the researchers decided to add the government shutdown as a news event to analyze under the auspices of Moral Foundations Theory. In addition to articles related to the Kavanaugh confirmation hearing, the researchers scraped articles related to the government shutdown in order to analyze their content for valence and moral alignment.

# Method

In contrast to Experiment 1, the researchers approached Experiment 2 with the intention to confirm the method employed was valid for the analysis of the scraped text as well as for any inferences drawn from the analyses. For Experiment 2, the researchers hypothesized that news sources perceived as liberal will exhibit positive endorsements of the individualizing moral foundations (*harm/care* and *fairness/reciprocity*) in their articles reporting on both the Kavanaugh confirmation hearing as well as the 2018-2019 government shutdown. News sources perceived as conservative are hypothesized to positively endorse all five foundations equally in their coverage of the Kavanaugh hearing and the government shutdown. The researchers tested the hypothesis by analyzing the content scraped from news sources' web pages spanning the two weeks before (September 13, 2018) and two weeks after (October 11, 2018) Kavanaugh's confirmation hearing, owing to its prominence in the news. Likewise, the researchers analyzed content spanning two weeks before the start of the government shutdown (December 8, 2018) to two weeks following the end of the shutdown (February 8, 2019). The content will be analyzed for valence and moral alignment under Moral Foundations Theory. Once again, no human participants were needed for this study, so no Institutional Review Board approval was necessary.

```{r NYTimes_kav, eval = F, include = F}

library(rvest)
library(RSelenium)

rD <- rsDriver(browser=c("chrome"), chromever="73.0.3683.68")
remDr <- rD[["client"]]

#page for NYTIMES
url <- 'https://www.nytimes.com/search?endDate=20181011&query=kavanaugh&sort=best&startDate=20180913'

#navigate to it
remDr$navigate(url)

#click the button 
#don't change class name
#right click --> inspect element 
webElem <- remDr$findElement(using = 'class name',"css-1t62hi8")

#with 752 results at 10 per page we need
#752 - 10 original = 742 / 10 per click = 75 clicks

for (i in 1:75) { #change this to 75 later
  webElem$clickElement()
  Sys.sleep(runif(1, 1, 5))
}

webpage <-remDr$getPageSource()

#Reading the HTML code from the website - headlines
webpage2 <- read_html(unlist(webpage))

headline_data = html_nodes(webpage2,'a')

attr_data = html_attrs(headline_data) 
attr_data

urlslist = unlist(attr_data)
urlslist = urlslist[grep(".html", urlslist)]
urlslist = urlslist[grep("/2018", urlslist)]
urlslist = unique(urlslist)
urlslist

#paste NY on front
urlslist = paste("https://www.nytimes.com", urlslist, sep = "")

##start a data frame
NYtimesDF = matrix(NA, nrow = length(urlslist), ncol = 3)
colnames(NYtimesDF) = c("Source", "Url", "Text")
NYtimesDF = as.data.frame(NYtimesDF)

##for loops
for (i in 1:length(urlslist)){
  
  ##read in the URL
  webpage <- read_html(urlslist[i])
  
  ##pull the specific nodes
  headline_data = html_nodes(webpage,'.StoryBodyCompanionColumn') 
  
  ##pull the text
  text_data = html_text(headline_data)
  
  ##save the data
  NYtimesDF$Source[i] = "NY Times"
  NYtimesDF$Url[i] = urlslist[i]
  NYtimesDF$Text[i] = paste(text_data, collapse = "")
  Sys.sleep(runif(1, 1, 10)) #don't make them angry
} ##end for loop

write.csv(NYtimesDF, "NYtimes_kav.csv", row.names = F)

remDr$close()
# stop the selenium server
rD[["server"]]$stop()

```

```{r NYTimes_shut, eval = F, include = F}
library(rvest)
library(RSelenium)
library(beepr)

rD <- rsDriver(browser=c("chrome"), chromever="73.0.3683.68")
remDr <- rD[["client"]]

#page for NYTIMES
url <- 'https://www.nytimes.com/search?endDate=20190208&query=government%20shutdown&sort=best&startDate=20181208'

#navigate to it
remDr$navigate(url)

#click the button 
#don't change class name
#right click --> inspect element 
webElem <- remDr$findElement(using = 'class name',"css-1t62hi8")

#with 803 results at 10 per page we need
#803 - 10 original / 10 per click is 80 clicks

for (i in 1:80) {
  webElem$clickElement()
  Sys.sleep(runif(1, 1, 5))
}

webpage <-remDr$getPageSource()

#Reading the HTML code from the website - headlines
webpage2 <- read_html(unlist(webpage))

headline_data = html_nodes(webpage2,'a')

attr_data = html_attrs(headline_data) 
#attr_data

urlslist = unlist(attr_data)
urlslist = urlslist[grep(".html", urlslist)]
urlslist = urlslist[grep("/201", urlslist)]
urlslist = unique(urlslist)
urlslist

#paste NY on front
urlslist = paste("https://www.nytimes.com", urlslist, sep = "")

##start a data frame
NYtimesDF = matrix(NA, nrow = length(urlslist), ncol = 3)
colnames(NYtimesDF) = c("Source", "Url", "Text")
NYtimesDF = as.data.frame(NYtimesDF)

##for loops
for (i in 1:length(urlslist)){
  
  ##read in the URL
  webpage <- read_html(urlslist[i])
  
  ##pull the specific nodes
  headline_data = html_nodes(webpage,'.StoryBodyCompanionColumn') 
  
  ##pull the text
  text_data = html_text(headline_data)
  
  ##save the data
  NYtimesDF$Source[i] = "NY Times"
  NYtimesDF$Url[i] = urlslist[i]
  NYtimesDF$Text[i] = paste(text_data, collapse = "")
  Sys.sleep(runif(1, 1, 10)) #don't make them angry
} ##end for loop

write.csv(NYtimesDF, "NYtimes_gs.csv", row.names = F)
beep(sound = 5)

remDr$close()
# stop the selenium server
rD[["server"]]$stop()
```

````{r NPR_kav, eval = F, include = F}
library(rvest)
library(RSelenium)
library(beepr)

#open the browser
rD <- rsDriver(browser=c("chrome"), chromever="73.0.3683.68")
remDr <- rD[["client"]]

#create a blank space to put the links
urlslist_final = list()

##loop through the page number at the end until done with ~1000 / 20 = 50
for (i in 1:50) { ##change this to 50
  
  url = paste0('https://www.npr.org/search?query=kavanaugh&page=', i)
  
  #navigate to it
  remDr$navigate(url)
  
  #scroll on the page
  webscroll <- remDr$findElement("css", "body")
  webscroll$sendKeysToElement(list(key = "end"))
  
  #get the links
  webElems <- remDr$findElements(using = "css", "[href]")
  urlslist_final[[i]] = unlist(sapply(webElems, function(x) {x$getElementAttribute("href")}))
  
  #don't go too fast
  Sys.sleep(runif(1, 1, 5))
  
} #close the loop

remDr$close()
# stop the selenium server
rD[["server"]]$stop()

beep(sound = 3)

urlslist = unlist(urlslist_final)
#find all the ones with /2018/
urlslist2 = urlslist[grep("/2018", urlslist)]
urlslist2 = urlslist2[grep("npr.org", urlslist2)] #get rid of wbur
urlslist2 = unique(urlslist2)
temp = strsplit(urlslist2, split = "/")
temp2 = plyr::ldply(temp, rbind)
temp2 = as.data.frame(apply(temp2, 2, as.character), stringsAsFactors = F)

#deal with the ones with extra parts
temp2[temp2$`4` == "sections" , 4:8] = temp2[temp2$`4` == "sections" , 6:10]

#pull only the right dates
rows = rownames(temp2[ temp2$`4` == 2018 & #must be 2018 and 
                         (temp2$`5` == "09" & as.numeric(temp2$`6`)>=13 | 
                            temp2$`5` == "10" & as.numeric(temp2$`6`)<=11), #Must be 9 or 10 
                ])

  
urlslist3 = urlslist2[as.numeric(rows)]

##start a data frame
NPRDF = matrix(NA, nrow = length(urlslist3), ncol = 3)
colnames(NPRDF) = c("Source", "Url", "Text")
NPRDF = as.data.frame(NPRDF)


##for loops
for (i in 1:length(urlslist3)){
  
  ##read in the URL
  webpage2 <- read_html(urlslist3[i])
  
  ##pull the specific nodes
  headline_data2 = html_nodes(webpage2,'#storytext > p') 
  
  ##pull the text
  text_data2 = html_text(headline_data2)
  
  ##save the data
  NPRDF$Source[i] = "NPR"
  NPRDF$Url[i] = urlslist3[i]
  NPRDF$Text[i] = paste(text_data2, collapse = "")
  Sys.sleep(runif(1, 1, 10))
} ##end for loop

beep(sound = 3)

write.csv(NPRDF, "NPR_kav.csv", row.names = F)

```

```{r NPR_shut, eval = F, include = F}

library(rvest)
library(RSelenium)
library(beepr)

#open the browser
rD <- rsDriver(browser=c("chrome"), chromever="73.0.3683.68")
remDr <- rD[["client"]]

#create a blank space to put the links
urlslist_final = list()

##loop through the page number at the end until done with ~3316 / 20 = 166
for (i in 1:166) { 
  
  url = paste0('https://www.npr.org/search?query=government%20shutodwn&page=', i)
  
  #navigate to it
  remDr$navigate(url)
  
  #scroll on the page
  webscroll <- remDr$findElement("css", "body")
  webscroll$sendKeysToElement(list(key = "end"))
  
  #get the links
  webElems <- remDr$findElements(using = "css", "[href]")
  urlslist_final[[i]] = unlist(sapply(webElems, function(x) {x$getElementAttribute("href")}))
  
  #don't go too fast
  Sys.sleep(runif(1, 1, 5))
  
} #close the loop

remDr$close()
# stop the selenium server
rD[["server"]]$stop()

beep(sound = 3)

urlslist = unlist(urlslist_final)
#find all the ones with /2018/
urlslist2 = urlslist[grep("/201", urlslist)]
urlslist2 = urlslist2[grep("npr.org", urlslist2)] #get rid of wbur
urlslist2 = unique(urlslist2)
temp = strsplit(urlslist2, split = "/")
temp2 = plyr::ldply(temp, rbind)
temp2 = as.data.frame(apply(temp2, 2, as.character), stringsAsFactors = F)

#deal with the ones with extra parts
temp2[temp2$`4` == "sections" , 4:8] = temp2[temp2$`4` == "sections" , 6:10]

#pull only the right dates
rows = rownames(temp2[ (temp2$`4` == 2018 | temp2$`4` == 2019) & #must be 2018/19 and  
                         (
                           (temp2$`5` == "12" & as.numeric(temp2$`6`)>8 & temp2$`4` == 2018) | #after dec 12
                            (temp2$`5`== "01" & temp2$`4` == 2019) | #all of jan
                            (temp2$`5` == "02" & as.numeric(temp2$`6`)<8 & temp2$`4` == 2019)
                           ), #before feb 8
                ])

  
urlslist3 = urlslist2[as.numeric(rows)]

##start a data frame
NPRDF = matrix(NA, nrow = length(urlslist3), ncol = 3)
colnames(NPRDF) = c("Source", "Url", "Text")
NPRDF = as.data.frame(NPRDF)


##for loops
for (i in 315:length(urlslist3)){
  
  ##read in the URL
  webpage2 <- read_html(urlslist3[i])
  
  ##pull the specific nodes
  headline_data2 = html_nodes(webpage2,'#storytext > p') 
  
  ##pull the text
  text_data2 = html_text(headline_data2)
  
  ##save the data
  NPRDF$Source[i] = "NPR"
  NPRDF$Url[i] = urlslist3[i]
  NPRDF$Text[i] = paste(text_data2, collapse = "")
  Sys.sleep(runif(1, 1, 10))
} ##end for loop

beep(sound = 3)

write.csv(NPRDF, "NPR_gs.csv", row.names = F)
```

````{r Slate_kav, eval = F, include = F}
library(rvest)
library(RSelenium)
library(beepr)

#open the browser
rD <- rsDriver(browser=c("chrome"), chromever="73.0.3683.68")
remDr <- rD[["client"]]

url = "https://slate.com/search?q=kavanaugh&via=homepage_nav_search"

remDr$navigate(url)

#create a blank space to put the links
urlslist_final = list()

#loop over the results and go through them
#appear to be 10 pages
for (i in 1:10){
  
  #find the button to click on 
  webClick <- remDr$findElements(using = "css", ".gsc-cursor-page")
  
  #don't go too fast for the page to load
  Sys.sleep(runif(1, 1, 5))
  
  #click on the page number
  webClick[[i]]$clickElement()
  
  #page reload
  Sys.sleep(runif(1, 1, 5))
  
  #find the urls for page X
  webElems <- remDr$findElements(using = "css", "[href]")
  
  #add the urls to a list
  urlslist_final[[i]] = unlist(sapply(webElems, function(x) {x$getElementAttribute("href")}))
  
  #don't go too fast
  Sys.sleep(runif(1, 1, 5))
}

remDr$close()
# stop the selenium server
rD[["server"]]$stop()

beep(sound = 3)

##deal with the URLS
urlslist = unlist(urlslist_final)

#find all the ones with /2018/
urlslist2 = urlslist[grep("/2018/10|/2018/09", urlslist)]
urlslist2 = unique(urlslist2)

#71 articles seems suspiciously low, so also scraped this page
#https://slate.com/tag/brett-kavanaugh/

#open the browser
rD <- rsDriver(browser=c("chrome"), chromever="73.0.3683.68")
remDr <- rD[["client"]]

#create a blank space to put the links
urlslist_final = list()

#loop over the results and go through them
#337 / 20 = 17 pages
for (i in 1:17){
  
  url = paste0("https://slate.com/tag/brett-kavanaugh/", i)
  
  remDr$navigate(url)
  
  #don't go too fast
  Sys.sleep(runif(1, 1, 5))
  
  #find the urls for page X
  webElems <- remDr$findElements(using = "css", "[href]")
  
  #add the urls to a list
  urlslist_final[[i]] = unlist(sapply(webElems, function(x) {x$getElementAttribute("href")}))
  
  #don't go too fast
  Sys.sleep(runif(1, 1, 5))
}

remDr$close()
# stop the selenium server
rD[["server"]]$stop()

beep(sound = 3)

##deal with the URLS
urlslist = unlist(urlslist_final)

#find all the ones with /2018/
urlslist2.1 = urlslist[grep("/2018/10|/2018/09", urlslist)]
urlslist2.1 = unique(urlslist2.1)

urlslist3 = unique(c(urlslist2, urlslist2.1))

##start a data frame
SLATEDF = matrix(NA, nrow = length(urlslist3), ncol = 3)
colnames(SLATEDF) = c("Source", "Url", "Text")
SLATEDF = as.data.frame(SLATEDF)


##for loops
for (i in 1:length(urlslist3)){
  
  ##read in the URL
  webpage2 <- read_html(urlslist3[i])
  
  ##pull the specific nodes
  headline_data2 = html_nodes(webpage2,'.slate-paragraph') 
  
  ##pull the text
  text_data2 = html_text(headline_data2)
  
  ##save the data
  SLATEDF$Source[i] = "SLATE"
  SLATEDF$Url[i] = urlslist3[i]
  SLATEDF$Text[i] = paste(text_data2, collapse = "")
  Sys.sleep(runif(1, 1, 10))
} ##end for loop

beep(sound = 3)

write.csv(SLATEDF, "SLATE_kav.csv", row.names = F)

```

```{r SLATE_shut, eval = F, include = F}
library(rvest)
library(RSelenium)
library(beepr)

#open the browser
rD <- rsDriver(browser=c("chrome"), chromever="73.0.3683.68")
remDr <- rD[["client"]]

url = "https://slate.com/search?q=government+shutdown"

remDr$navigate(url)

#create a blank space to put the links
urlslist_final = list()

#loop over the results and go through them
#appear to be 10 pages
for (i in 1:10){
  
  #find the button to click on 
  webClick <- remDr$findElements(using = "css", ".gsc-cursor-page")
  
  #don't go too fast for the page to load
  Sys.sleep(runif(1, 1, 5))
  
  #click on the page number
  webClick[[i]]$clickElement()
  
  #page reload
  Sys.sleep(runif(1, 1, 5))
  
  #find the urls for page X
  webElems <- remDr$findElements(using = "css", "[href]")
  
  #add the urls to a list
  urlslist_final[[i]] = unlist(sapply(webElems, function(x) {x$getElementAttribute("href")}))
  
  #don't go too fast
  Sys.sleep(runif(1, 1, 5))
}

remDr$close()
# stop the selenium server
rD[["server"]]$stop()

beep(sound = 3)

##deal with the URLS
urlslist = unlist(urlslist_final)

#find all the ones from 12 2018 to 02 2019
urlslist2 = urlslist[grep("/2018/12|/2019/01|/2019/02", urlslist)]
urlslist2 = unique(urlslist2)

#60 something articles seems suspiciously low, so also scraped this page
#https://slate.com/tag/governtment-shutdown

#open the browser
rD <- rsDriver(browser=c("chrome"), chromever="73.0.3683.68")
remDr <- rD[["client"]]

#create a blank space to put the links
urlslist_final = list()

#loop over the results and go through them
#354 / 20 = 18 pages
for (i in 1:18){
  
  url = paste0("https://slate.com/tag/government-shutdown/", i)
  
  remDr$navigate(url)
  
  #don't go too fast
  Sys.sleep(runif(1, 1, 5))
  
  #find the urls for page X
  webElems <- remDr$findElements(using = "css", "[href]")
  
  #add the urls to a list
  urlslist_final[[i]] = unlist(sapply(webElems, function(x) {x$getElementAttribute("href")}))
  
  #don't go too fast
  Sys.sleep(runif(1, 1, 5))
}

remDr$close()
# stop the selenium server
rD[["server"]]$stop()

beep(sound = 3)

##deal with the URLS
urlslist = unlist(urlslist_final)

#find all the ones with /2018/
urlslist2.1 = urlslist[grep("/2018/12|/2019/01|/2019/02", urlslist)]
urlslist2.1 = unique(urlslist2.1)

urlslist3 = unique(c(urlslist2, urlslist2.1))

##start a data frame
SLATEDF = matrix(NA, nrow = length(urlslist3), ncol = 3)
colnames(SLATEDF) = c("Source", "Url", "Text")
SLATEDF = as.data.frame(SLATEDF)


##for loops
for (i in 1:length(urlslist3)){
  
  ##read in the URL
  webpage2 <- read_html(urlslist3[i])
  
  ##pull the specific nodes
  headline_data2 = html_nodes(webpage2,'.slate-paragraph') 
  
  ##pull the text
  text_data2 = html_text(headline_data2)
  
  ##save the data
  SLATEDF$Source[i] = "SLATE"
  SLATEDF$Url[i] = urlslist3[i]
  SLATEDF$Text[i] = paste(text_data2, collapse = "")
  Sys.sleep(runif(1, 1, 10))
} ##end for loop

#beep(sound = 3)

write.csv(SLATEDF, "SLATE_gs.csv", row.names = F)
```

````{r HuffPo_kav, eval = F, include = F}
library(rvest)
library(RSelenium)
library(beepr)
library(lubridate)

#open the browser
rD <- rsDriver(browser=c("chrome"), chromever="73.0.3683.68")
remDr <- rD[["client"]]

url = "https://search.huffpost.com/search?utf8=%E2%9C%93&p=kavanaugh&fr=huffpost_desktop-s"

remDr$navigate(url)

#create a blank space to put the links
urlslist_final = list()

#loop over the results and go through them
#943 results with 10 per page 94 clicks
for (i in 1:94){
  
  #find the urls for page X
  webElems <- remDr$findElements(using = "class name", "fz-20")
  
  #add the urls to a list
  urlslist_final[[i]] = unlist(sapply(webElems, function(x) {x$getElementAttribute("href")}))
  
  #find the button to click on 
  webClick <- remDr$findElements(using = "class name", "next")
  
  #click on the page number
  webClick[[1]]$clickElement()
  
  #page reload
  Sys.sleep(runif(1, 1, 5))
  
}

remDr$close()
# stop the selenium server
rD[["server"]]$stop()

beep(sound = 3)

##deal with the URLS
urlslist = unlist(urlslist_final)
urlslist = unique(urlslist)

##start a data frame
HUFFPODF = matrix(NA, nrow = length(urlslist), ncol = 3)
colnames(HUFFPODF) = c("Source", "Url", "Text")
HUFFPODF = as.data.frame(HUFFPODF)
int <- interval(ymd("2018-09-13"), ymd("2018-10-11"))


for (i in 937:length(urlslist)){
  
  #convert the url
  temp = paste("https", unlist(strsplit(unlist(strsplit(urlslist[i], "https"))[3], "/RK=0"))[1], sep = "")
  temp = gsub("%3a", ":", temp)
  temp = gsub("%2f", "/", temp)
  
  webpage = read_html(temp)
  #figure out the date
  headline_data = html_nodes(webpage, '.timestamp__date--published')
  text_data = html_text(headline_data)
  time = unlist(strsplit(text_data, split = "\n| "))[2]
  time = as.Date(time, format = "%m/%d/%Y")
  
  if (time %within% int) {
    ##pull out the text
    headline_data2 = html_nodes(webpage,'p') 
    text_data2 = html_text(headline_data2)
    
    ##save the data
    HUFFPODF$Source[i] = "HUFFPO"
    HUFFPODF$Url[i] = urlslist[i]
    HUFFPODF$Text[i] = paste(text_data2, collapse = "")
  }
  
  Sys.sleep(runif(1,1,10))
  
  }

beep(sound = 3)

write.csv(HUFFPODF, "HUFFPO_kav.csv", row.names = F)
```

```{r HuffPo_shut, eval = F, include = F}
library(rvest)
library(RSelenium)
library(beepr)
library(lubridate)

#open the browser
rD <- rsDriver(browser=c("chrome"), chromever="73.0.3683.68")
remDr <- rD[["client"]]

url = "https://search.huffpost.com/search?utf8=%E2%9C%93&p=government+shutdown&fr=huffpost_desktop"

remDr$navigate(url)

#create a blank space to put the links
urlslist_final = list()

#loop over the results and go through them
#942 results with 10 per page 94 clicks
for (i in 1:94){
  
  #find the urls for page X
  webElems <- remDr$findElements(using = "class name", "fz-20")
  
  #add the urls to a list
  urlslist_final[[i]] = unlist(sapply(webElems, function(x) {x$getElementAttribute("href")}))
  
  #find the button to click on 
  webClick <- remDr$findElements(using = "class name", "next")
  
  #click on the page number
  webClick[[1]]$clickElement()
  
  #page reload
  Sys.sleep(runif(1, 1, 5))
  
}

remDr$close()
# stop the selenium server
rD[["server"]]$stop()

beep(sound = 2)

##deal with the URLS
urlslist = unlist(urlslist_final)
urlslist = unique(urlslist)

##start a data frame
HUFFPODF = matrix(NA, nrow = length(urlslist), ncol = 3)
colnames(HUFFPODF) = c("Source", "Url", "Text")
HUFFPODF = as.data.frame(HUFFPODF)
int <- interval(ymd("2018-12-08"), ymd("2019-02-08"))


for (i in 1:length(urlslist)){
  
  #convert the url
  temp = paste("https", unlist(strsplit(unlist(strsplit(urlslist[i], "https"))[3], "/RK=0"))[1], sep = "")
  temp = gsub("%3a", ":", temp)
  temp = gsub("%2f", "/", temp)
  
  webpage = read_html(temp)
  #figure out the date
  headline_data = html_nodes(webpage, '.timestamp__date--published')
  text_data = html_text(headline_data)
  time = unlist(strsplit(text_data, split = "\n| "))[2]
  time = as.Date(time, format = "%m/%d/%Y")
  
  if (time %within% int) {
    ##pull out the text
    headline_data2 = html_nodes(webpage,'p') 
    text_data2 = html_text(headline_data2)
    
    ##save the data
    HUFFPODF$Source[i] = "HUFFPO"
    HUFFPODF$Url[i] = urlslist[i]
    HUFFPODF$Text[i] = paste(text_data2, collapse = "")
  }
  
  Sys.sleep(runif(1,1,10))
  
  }

beep(sound = 2)

write.csv(HUFFPODF, "HUFFPO_gs.csv", row.names = F)
```

````{r Politico_kav, eval = F, include = F}
library(rvest)
library(RSelenium)
library(beepr)
library(lubridate)

#open the browser
rD <- rsDriver(browser=c("chrome"), chromever="73.0.3683.68")
remDr <- rD[["client"]]

url = "https://www.politico.com/search?adv=true&userInitiated=true&s=newest&q=kavanaugh&pv=&c=&r=&start=09%2F13%2F2018&start_submit=09%2F13%2F2018&end=10%2F11%2F2018&end_submit=10%2F11%2F2018"

remDr$navigate(url)

#create a blank space to put the links
urlslist_final = list()

#loop over the results and go through them
#749 with 20 per page with 37 clicks

for (i in 1:37){
  
  if (i > 1){
    #navigate to the next page
    url = paste0("https://www.politico.com/search/", i, 
                 "?s=newest&q=kavanaugh&adv=true&start=09/13/2018&end=10/11/2018")
    remDr$navigate(url)
  }
  
  #find the urls for page X
  webElems <- remDr$findElements(using = "css", "[href]")
  
  #add the urls to a list
  urlslist_final[[i]] = unlist(sapply(webElems, function(x) {x$getElementAttribute("href")}))

  #page reload
  Sys.sleep(runif(1, 1, 5))
  
}

remDr$close()
# stop the selenium server
rD[["server"]]$stop()

beep(sound = 7)

##deal with the URLS
urlslist = unlist(urlslist_final)
urlslist = unique(urlslist)
urlslist = urlslist[grep("/2018/", urlslist)]
urlslist = urlslist[-c(grep("subscriber.", urlslist))] #take out the stuff we can't see

##start a data frame
POLITICODF = matrix(NA, nrow = length(urlslist), ncol = 3)
colnames(POLITICODF) = c("Source", "Url", "Text")
POLITICODF = as.data.frame(POLITICODF)


for (i in 1:length(urlslist)){
  
  webpage = read_html(urlslist[i])
  
  headline_data2 = html_nodes(webpage,'p') 
  text_data2 = html_text(headline_data2)
  
  ##save the data
  POLITICODF$Source[i] = "POLITICO"
  POLITICODF$Url[i] = urlslist[i]
  POLITICODF$Text[i] = paste(text_data2, collapse = "")
  
  }
  
  Sys.sleep(runif(1,1,10))
  
beep(sound = 7)

write.csv(POLITICODF, "POLITICO_kav.csv", row.names = F)
#easiest one yet
```

```{r Politico_shut, eval = F, include = F}
library(rvest)
library(RSelenium)
library(beepr)
library(lubridate)

#open the browser
rD <- rsDriver(browser=c("chrome"), chromever="73.0.3683.68")
remDr <- rD[["client"]]

url = "https://www.politico.com/search?adv=true&userInitiated=true&s=newest&q=government+shutdown&pv=&c=&r=&start=12%2F08%2F2018&start_submit=12%2F08%2F2018&end=02%2F08%2F2019&end_submit=02%2F08%2F2019"

remDr$navigate(url)

#create a blank space to put the links
urlslist_final = list()

#loop over the results and go through them
#1329 with 20 per page with 66 clicks

for (i in 1:66){
  
  if (i > 1){
    #navigate to the next page
    url = paste0("https://www.politico.com/search/", i, 
                 "?s=newest&q=government shutdown&adv=true&start=12/08/2018&end=02/08/2019")
    remDr$navigate(url)
  }
  
  #find the urls for page X
  webElems <- remDr$findElements(using = "css", "[href]")
  
  #add the urls to a list
  urlslist_final[[i]] = unlist(sapply(webElems, function(x) {x$getElementAttribute("href")}))

  #page reload
  Sys.sleep(runif(1, 1, 5))
  
}

remDr$close()
# stop the selenium server
rD[["server"]]$stop()

beep(sound = 7)

##deal with the URLS
urlslist = unlist(urlslist_final)
urlslist = unique(urlslist)
urlslist = urlslist[grep("/2018/", urlslist)]
urlslist = urlslist[-c(grep("subscriber.", urlslist))] #take out the stuff we can't see

##start a data frame
POLITICODF = matrix(NA, nrow = length(urlslist), ncol = 3)
colnames(POLITICODF) = c("Source", "Url", "Text")
POLITICODF = as.data.frame(POLITICODF)


for (i in 1:length(urlslist)){
  
  webpage = read_html(urlslist[i])
  
  headline_data2 = html_nodes(webpage,'p') 
  text_data2 = html_text(headline_data2)
  
  ##save the data
  POLITICODF$Source[i] = "POLITICO"
  POLITICODF$Url[i] = urlslist[i]
  POLITICODF$Text[i] = paste(text_data2, collapse = "")
  
  
  Sys.sleep(runif(1,1,10))
  
  }
  
  
beep(sound = 7)

write.csv(POLITICODF, "POLITICO_gs.csv", row.names = F)

```

````{r Fox_kav, eval = F, include = F}

##start a data frame
library(rvest)
library(RSelenium)
library(beepr)
library(lubridate)

#open the browser
rD <- rsDriver(browser=c("chrome"), chromever="73.0.3683.68")
remDr <- rD[["client"]]

#create a blank space to put the links
urlslist_final = list()

#loop over the results and go through them
#1651 with 10 per page with 165 clicks

for (i in 1:165){
  
    #navigate to the page
    url = paste0("https://www.foxnews.com/search-results/search?q=kavanaugh&ss=fn&min_date=2018-09-13&max_date=2018-10-11&start=",
                 (i-1)*10)
    remDr$navigate(url)
  
  #find the urls for page X
  webElems <- remDr$findElements(using = "css", ".ng-binding")
  
  #add the urls to a list
  urlslist_final[[i]] = unlist(sapply(webElems, function(x) {x$getElementAttribute("href")}))

  #page reload
  Sys.sleep(runif(1, 1, 5))
  
}

remDr$close()
# stop the selenium server
rD[["server"]]$stop()

beep(sound = 6)

##deal with the URLS
urlslist = unlist(urlslist_final)
urlslist = unique(urlslist)
urlslist = urlslist[-c(grep("video.", urlslist))] #take out videos
urlslist = urlslist[-c(grep("/search-results", urlslist))] #take out search result fake links


##start a data frame
FOXDF = matrix(NA, nrow = length(urlslist), ncol = 3)
colnames(FOXDF) = c("Source", "Url", "Text")
FOXDF = as.data.frame(FOXDF)


for (i in 1:length(urlslist)){
  
  webpage = read_html(urlslist[i])
  
  headline_data2 = html_nodes(webpage,'p') 
  text_data2 = html_text(headline_data2)
  
  ##save the data
  FOXDF$Source[i] = "FOX"
  FOXDF$Url[i] = urlslist[i]
  FOXDF$Text[i] = paste(text_data2, collapse = "")
  
  
  Sys.sleep(runif(1,1,10))
  
  }
  
  
beep(sound = 7)

write.csv(FOXDF, "FOX_kav.csv", row.names = F)
```

````{r Breitbart_kav, eval = F, include = F}
library(rvest)
library(RSelenium)
library(beepr)
library(lubridate)

#open the browser
rD <- rsDriver(browser=c("chrome"), chromever="73.0.3683.68")
remDr <- rD[["client"]]

#create a blank space to put the links
urlslist_final = list()

url = "https://www.breitbart.com/tag/brett-kavanaugh/"

remDr$navigate(url)

#loop over the results and go through them
for (i in 1:42){
  
  #find the urls for page X
  webElems <- remDr$findElements(using = "css", "h2 a")
  
  #add the urls to a list
  urlslist_final[[i]] = unlist(sapply(webElems, function(x) {x$getElementAttribute("href")}))

  if (i > 1)
    {
  url = paste0("https://www.breitbart.com/tag/brett-kavanaugh/page/",
               i+1, "/")
  remDr$navigate(url)
  }
  
  #page reload
  Sys.sleep(runif(1, 1, 5))
  
}

remDr$close()
# stop the selenium server
rD[["server"]]$stop()

beep(sound = 6)

##deal with the URLS
urlslist = unlist(urlslist_final)
urlslist = unique(urlslist)
urlslist = urlslist[-c(grep("store", urlslist))] #take out store

##find the right dates
temp = strsplit(urlslist, split = "/")
temp2 = plyr::ldply(temp, rbind)
temp2 = as.data.frame(apply(temp2, 2, as.character), stringsAsFactors = F)

#pull only the right dates
rows = rownames(temp2[ temp2$`5` == 2018 & #must be 2018 and 
                         (temp2$`6` == "09" & as.numeric(temp2$`7`)>=13 | 
                            temp2$`6` == "10" & as.numeric(temp2$`7`)<=11), #Must be 9 or 10 
                       ])

urlslist3 = urlslist[as.numeric(rows)]

##start a data frame
BREITBARTDF = matrix(NA, nrow = length(urlslist3), ncol = 3)
colnames(BREITBARTDF) = c("Source", "Url", "Text")
BREITBARTDF = as.data.frame(BREITBARTDF)


for (i in 1:length(urlslist3)){
  
  webpage = read_html(urlslist3[i])
  
  headline_data2 = html_nodes(webpage,'h2, p') 
  text_data2 = html_text(headline_data2)
  
  ##save the data
  BREITBARTDF$Source[i] = "BREITBART"
  BREITBARTDF$Url[i] = urlslist3[i]
  BREITBARTDF$Text[i] = paste(text_data2, collapse = "")
  
  
  Sys.sleep(runif(1,1,10))
  
  }
  
  
beep(sound = 7)

write.csv(BREITBARTDF, "BREITBART_kav.csv", row.names = F)
```

```{r Breitbart_gs, eval = F, include = F}
library(rvest)
library(RSelenium)
library(beepr)
library(lubridate)

#open the browser
rD <- rsDriver(browser=c("chrome"), chromever="73.0.3683.68")
remDr <- rD[["client"]]

#create a blank space to put the links
urlslist_final = list()

url = "https://www.breitbart.com/tag/government-shutdown/"

remDr$navigate(url)

#loop over the results and go through them
for (i in 1:16){
  
  #find the urls for page X
  webElems <- remDr$findElements(using = "css", "h2 a")
  
  #add the urls to a list
  urlslist_final[[i]] = unlist(sapply(webElems, function(x) {x$getElementAttribute("href")}))

  if (i > 1)
    {
  url = paste0("https://www.breitbart.com/tag/government-shutdown/page/",
               i+1, "/")
  remDr$navigate(url)
  }
  
  #page reload
  Sys.sleep(runif(1, 1, 5))
  
}

remDr$close()
# stop the selenium server
rD[["server"]]$stop()

beep(sound = 6)

##deal with the URLS
urlslist = unlist(urlslist_final)
urlslist = unique(urlslist)
urlslist = urlslist[-c(grep("store", urlslist))] #take out store

##find the right dates
temp = strsplit(urlslist, split = "/")
temp2 = plyr::ldply(temp, rbind)
temp2 = as.data.frame(apply(temp2, 2, as.character), stringsAsFactors = F)

#pull only the right dates
rows = rownames(temp2[ (temp2$`5` == 2018 | temp2$`5` == 2019) & #must be 2018/19 and  
                         (
                           (temp2$`6` == "12" & as.numeric(temp2$`7`)>8 & temp2$`5` == 2018) | #after dec 12
                             (temp2$`6`== "01" & temp2$`5` == 2019) | #all of jan
                             (temp2$`6` == "02" & as.numeric(temp2$`7`)<8 & temp2$`5` == 2019)
                         ), #before feb 8
                       ])

urlslist3 = urlslist[as.numeric(rows)]

##start a data frame
BREITBARTDF = matrix(NA, nrow = length(urlslist3), ncol = 3)
colnames(BREITBARTDF) = c("Source", "Url", "Text")
BREITBARTDF = as.data.frame(BREITBARTDF)


for (i in 1:length(urlslist3)){
  
  webpage = read_html(urlslist3[i])
  
  headline_data2 = html_nodes(webpage,'h2, p') 
  text_data2 = html_text(headline_data2)
  
  ##save the data
  BREITBARTDF$Source[i] = "BREITBART"
  BREITBARTDF$Url[i] = urlslist3[i]
  BREITBARTDF$Text[i] = paste(text_data2, collapse = "")
  
  
  Sys.sleep(runif(1,1,10))
  
  }
  
  
beep(sound = 7)

write.csv(BREITBARTDF, "BREITBART_gs.csv", row.names = F)
```

````{r Rush_kav, eval = F, include = F}
library(rvest)
library(RSelenium)
library(beepr)
library(lubridate)

#open the browser
rD <- rsDriver(browser=c("chrome"), chromever="73.0.3683.68")
remDr <- rD[["client"]]

#create a blank space to put the links
urlslist_final = list()

url = "https://www.rushlimbaugh.com/?s=kavanaugh&is_v=1"

remDr$navigate(url)

#loop over the results and go through them
##18 pages
for (i in 1:18){
  
  #find the urls for page X
  webElems <- remDr$findElements(using = "css", "a")
  
  #add the urls to a list
  urlslist_final[[i]] = unlist(sapply(webElems, function(x) {x$getElementAttribute("href")}))

  #find the thing to click on 
  webClick <- remDr$findElements(using = "css", ".ais-pagination--link")
  
  #click on the page number
  webClick[[18]]$clickElement()
  
  #page reload
  Sys.sleep(runif(1, 1, 5))
  
}

remDr$close()
# stop the selenium server
rD[["server"]]$stop()

beep(sound = 6)

##deal with the URLS
urlslist = unlist(urlslist_final)
urlslist = unique(urlslist)
urlslist = urlslist[-c(grep("store", urlslist))] #take out store
urlslist = urlslist[grep("/daily/[0-9]*/[0-9]*/[0-9]*/[a-zA-Z]", urlslist)]

##find the right dates
temp = strsplit(urlslist, split = "/")
temp2 = plyr::ldply(temp, rbind)
temp2 = as.data.frame(apply(temp2, 2, as.character), stringsAsFactors = F)

#pull only the right dates
rows = rownames(temp2[ temp2$`5` == 2018 & #must be 2018 and 
                         (temp2$`6` == "09" & as.numeric(temp2$`7`)>=13 | 
                            temp2$`6` == "10" & as.numeric(temp2$`7`)<=11), #Must be 9 or 10 
                       ])

urlslist3 = urlslist[as.numeric(rows)]

##start a data frame
RUSHDF = matrix(NA, nrow = length(urlslist3), ncol = 3)
colnames(RUSHDF) = c("Source", "Url", "Text")
RUSHDF = as.data.frame(RUSHDF)


for (i in 1:length(urlslist3)){
  
  webpage = read_html(urlslist3[i])
  
  headline_data2 = html_nodes(webpage,'p') 
  text_data2 = html_text(headline_data2)
  
  ##save the data
  RUSHDF$Source[i] = "RUSH"
  RUSHDF$Url[i] = urlslist3[i]
  RUSHDF$Text[i] = paste(text_data2, collapse = "")
  
  
  Sys.sleep(runif(1,1,10))
  
  }
  
  
beep(sound = 7)

write.csv(RUSHDF, "RUSH_kav.csv", row.names = F)
```

```{r Rush_shut, eval = F, include = F}
library(rvest)
library(RSelenium)
library(beepr)
library(lubridate)

#open the browser
rD <- rsDriver(browser=c("chrome"), chromever="73.0.3683.68")
remDr <- rD[["client"]]

#create a blank space to put the links
urlslist_final = list()

url = "https://www.rushlimbaugh.com/?s=government%20shutdown&is_v=1"

remDr$navigate(url)

#loop over the results and go through them
##47 pages
for (i in 1:47){
  
  #find the urls for page X
  webElems <- remDr$findElements(using = "css", "a")
  
  #add the urls to a list
  urlslist_final[[i]] = unlist(sapply(webElems, function(x) {x$getElementAttribute("href")}))

  #find the thing to click on 
  webClick <- remDr$findElements(using = "css", ".ais-pagination--link")
  
  #click on the page number
  webClick[[18]]$clickElement()
  
  #page reload
  Sys.sleep(runif(1, 1, 5))
  
}

remDr$close()
# stop the selenium server
rD[["server"]]$stop()

beep(sound = 6)

##deal with the URLS
urlslist = unlist(urlslist_final)
urlslist = unique(urlslist)
urlslist = urlslist[-c(grep("store", urlslist))] #take out store
urlslist = urlslist[grep("/daily/[0-9]*/[0-9]*/[0-9]*/[a-zA-Z]", urlslist)]

##find the right dates
temp = strsplit(urlslist, split = "/")
temp2 = plyr::ldply(temp, rbind)
temp2 = as.data.frame(apply(temp2, 2, as.character), stringsAsFactors = F)

#pull only the right dates
rows = rownames(temp2[ (temp2$`5` == 2018 | temp2$`5` == 2019) & #must be 2018/19 and  
                         (
                           (temp2$`6` == "12" & as.numeric(temp2$`7`)>8 & temp2$`5` == 2018) | #after dec 12
                             (temp2$`6`== "01" & temp2$`5` == 2019) | #all of jan
                             (temp2$`6` == "02" & as.numeric(temp2$`7`)<8 & temp2$`5` == 2019)
                         ), #before feb 8
                       ])
urlslist3 = urlslist[as.numeric(rows)]

##start a data frame
RUSHDF = matrix(NA, nrow = length(urlslist3), ncol = 3)
colnames(RUSHDF) = c("Source", "Url", "Text")
RUSHDF = as.data.frame(RUSHDF)


for (i in 1:length(urlslist3)){
  
  webpage = read_html(urlslist3[i])
  
  headline_data2 = html_nodes(webpage,'p') 
  text_data2 = html_text(headline_data2)
  
  ##save the data
  RUSHDF$Source[i] = "RUSH"
  RUSHDF$Url[i] = urlslist3[i]
  RUSHDF$Text[i] = paste(text_data2, collapse = "")
  
  
  Sys.sleep(runif(1,1,10))
  
  }
  
  
beep(sound = 7)

write.csv(RUSHDF, "RUSH_gs.csv", row.names = F)
 
```

````{r Blaze_kav, eval = F, include = F}
url9 = 'https://www.theblaze.com/search/?q=kavanaugh'
webpage9 = read_html(url9)
headline_data9 = html_nodes(webpage9, '.widget__headline-text')

attr_data9 = html_attrs(headline_data9) 
attr_data9

urlslist9 = unlist(attr_data9)
urlslist9 = urlslist9[grep(urlslist9)]

##start a data frame
BlazeDF = matrix(NA, nrow = length(urlslist9), ncol = 3)
colnames(BlazeDF) = c("Source", "Url", "Text")
BlazeDF = as.data.frame(BlazeDF)

##for loops
for (i in 1:length(urlslist9)){
  
  ##read in the URL
  webpage9 <- read_html(urlslist9[i])
  
  ##pull the specific nodes
  headline_data9 = html_nodes(webpage9,'p') 
  
  ##pull the text
  text_data9 = html_text(headline_data9)
  
  ##save the data
  BlazeDF$Source[i] = "The Blaze"
  BlazeDF$Url[i] = urlslist9[i]
  BlazeDF$Text[i] = paste(text_data9, collapse = "")
} ##end for loop
```

```{r Blaze_shut, eval = F, include = F}
library(rvest)
library(RSelenium)
library(beepr)
library(lubridate)

#open the browser
rD <- rsDriver(browser=c("chrome"), chromever="73.0.3683.68")
remDr <- rD[["client"]]

url = "https://www.theblaze.com/search/?q=government+shutdown"

remDr$navigate(url)

webClick <- remDr$findElements(using = "css", ".action-btn")

#click next until done
while (length(webClick) > 0 ) { 
  
  #find the urls for page X
  webClick <- remDr$findElements(using = "css", ".action-btn")
  
  webClick[[1]]$clickElement()
  
  Sys.sleep(runif(1, 1, 5))
}

#find the urls for page X
webElems <- remDr$findElements(using = "css", "a")

#add the urls to a list
urlslist_final = unlist(sapply(webElems, function(x) {x$getElementAttribute("href")}))

remDr$close()
# stop the selenium server
rD[["server"]]$stop()


##deal with the URLS
urlslist = urlslist_final
urlslist = unique(urlslist)
urlslist = urlslist[-c(grep("video/", urlslist))]
urlslist = urlslist[grep("/news/", urlslist)]
urlslist = urlslist[-c(grep("/201[3-7]", urlslist))]

##start a data frame
BLAZEDF = matrix(NA, nrow = length(urlslist), ncol = 3)
colnames(BLAZEDF) = c("Source", "Url", "Text")
BLAZEDF = as.data.frame(BLAZEDF)
int <- interval(ymd("2018-12-08"), ymd("2019-02-08"))


for (i in 1:length(urlslist)){
  
  webpage = read_html(urlslist[i])
  
  headline_data = html_nodes(webpage, '.post-date')
  text_data = html_text(headline_data)
  date = as.Date(text_data, "%B %d, %Y")
  
  if (date %within% int){
    headline_data2 = html_nodes(webpage,'h1, h3, p') 
    text_data2 = html_text(headline_data2)
    
    ##save the data
    BLAZEDF$Source[i] = "BLAZE"
    BLAZEDF$Url[i] = urlslist[i]
    BLAZEDF$Text[i] = paste(text_data2, collapse = "")
  }
  
  Sys.sleep(runif(1,1,10))
  
  }
  
  
beep(sound = 7)

write.csv(BLAZEDF, "BLAZE_gs.csv", row.names = F)

```

```{r Hannity_kav, eval = F, include = F}
library(rvest)
library(RSelenium)
library(beepr)
library(lubridate)

#open the browser
rD <- rsDriver(browser=c("chrome"), chromever="73.0.3683.68")
remDr <- rD[["client"]]

url = "https://www.hannity.com/trending/brett-kavanaugh/"

remDr$navigate(url)

urlslist_final = list()

for (i in 1:4){
  
  #find the urls for page X
  webElems <- remDr$findElements(using = "css", "a")
  
  #add the urls to a list
  urlslist_final[[i]] = unlist(sapply(webElems, function(x) {x$getElementAttribute("href")}))
  
  #move on to the next page
  url = paste0("https://www.hannity.com/trending/brett-kavanaugh/?pg=", i+1)
  
  remDr$navigate(url)
  
  Sys.sleep(runif(1,1,5))
  
}

remDr$close()
# stop the selenium server
rD[["server"]]$stop()


##deal with the URLS
urlslist = unlist(urlslist_final)
urlslist = unique(urlslist)

#also use google search to see if we can find more

#open the browser
rD <- rsDriver(browser=c("chrome"), chromever="73.0.3683.68")
remDr <- rD[["client"]]

url = "https://www.google.com/search?q=kavanaugh+site%3Ahannity.com&source=lnt&tbs=cdr%3A1%2Ccd_min%3A9%2F13%2F2018%2Ccd_max%3A10%2F11%2F2018&tbm="

remDr$navigate(url)

urlslist_final2 = list()

for (i in 1:3){
  
  #find the urls for page X
  webElems <- remDr$findElements(using = "css", "a")
  
  #add the urls to a list
  urlslist_final2[[i]] = unlist(sapply(webElems, function(x) {x$getElementAttribute("href")}))
  
  #find the thing to click on 
  webClick <- remDr$findElements(using = "link text", "Next")
  
  #click on the page number
  webClick[[1]]$clickElement()
  
  Sys.sleep(runif(1,1,5))
  
}

remDr$close()
# stop the selenium server
rD[["server"]]$stop()

urlslist2 = unlist(urlslist_final2)
urlslist2 = c(urlslist, urlslist2)
urlslist2 = unique(urlslist2)
urlslist3 = urlslist2[grep("/media-room/[a-zA-Z]", urlslist2)]
urlslist3 = urlslist3[-c(grep("webcache", urlslist3))]
urlslist3 = urlslist3[-c(grep("radio-show", urlslist3))]


##start a data frame
HANNITYDF = matrix(NA, nrow = length(urlslist), ncol = 3)
colnames(HANNITYDF) = c("Source", "Url", "Text")
HANNITYDF = as.data.frame(HANNITYDF)
int <- interval(ymd("2018-09-13"), ymd("2018-10-11"))

for (i in 1:length(urlslist3)){
  
  webpage = read_html(urlslist3[i])
  
  headline_data = html_nodes(webpage, '.author-info')
  text_data = html_text(headline_data)
  text_data = strsplit(text_data, " ")[[1]][7]
  date = as.Date(text_data, "%m.%d.%y")
  
  if (date %within% int){
    headline_data2 = html_nodes(webpage,'h1, p') 
    text_data2 = html_text(headline_data2)
    
    ##save the data
    HANNITYDF$Source[i] = "HANNITY"
    HANNITYDF$Url[i] = urlslist3[i]
    HANNITYDF$Text[i] = paste(text_data2, collapse = "")
  }
  
  Sys.sleep(runif(1,1,10))
  
  }
  
  
beep(sound = 7)

write.csv(HANNITYDF, "HANNITY_kav.csv", row.names = F)
```

```{r Hannity_shut, eval = F, include = F}
library(rvest)
library(RSelenium)
library(beepr)
library(lubridate)

#open the browser
rD <- rsDriver(browser=c("chrome"), chromever="73.0.3683.68")
remDr <- rD[["client"]]

url = "http://www.hannity.com/trending/shutdown/"

remDr$navigate(url)

urlslist_final = list()

for (i in 1:4){
  
  #find the urls for page X
  webElems <- remDr$findElements(using = "css", "a")
  
  #add the urls to a list
  urlslist_final[[i]] = unlist(sapply(webElems, function(x) {x$getElementAttribute("href")}))
  
  #move on to the next page
  url = paste0("http://www.hannity.com/trending/shutdown/?pg=", i+1)
  
  remDr$navigate(url)
  
  Sys.sleep(runif(1,1,5))
  
}

remDr$close()
# stop the selenium server
rD[["server"]]$stop()


##deal with the URLS
urlslist = unlist(urlslist_final)
urlslist = unique(urlslist)

#also use google search to see if we can find more

#open the browser
rD <- rsDriver(browser=c("chrome"), chromever="73.0.3683.68")
remDr <- rD[["client"]]

url = "https://www.google.com/search?biw=1366&bih=632&tbs=cdr%3A1%2Ccd_min%3A12%2F8%2F2018%2Ccd_max%3A2%2F8%2F2019&ei=gECaXNe8Aaft_Qb9w7G4BA&q=government+shutdown+site%3Ahannity.com&oq=government+shutdown+site%3Ahannity.com&gs_l=psy-ab.3...0.0..1915...0.0..0.0.0.......0......gws-wiz.aFps0nA33mY"

remDr$navigate(url)

urlslist_final2 = list()

for (i in 1:9){
  
  #find the urls for page X
  webElems <- remDr$findElements(using = "css", "a")
  
  #add the urls to a list
  urlslist_final2[[i]] = unlist(sapply(webElems, function(x) {x$getElementAttribute("href")}))
  
  #find the thing to click on 
  webClick <- remDr$findElements(using = "link text", "Next")
  
  #click on the page number
  webClick[[1]]$clickElement()
  
  Sys.sleep(runif(1,1,5))
  
}

remDr$close()
# stop the selenium server
rD[["server"]]$stop()

urlslist2 = unlist(urlslist_final2)
urlslist2 = c(urlslist, urlslist2)
urlslist2 = unique(urlslist2)
urlslist3 = urlslist2[grep("/media-room/[a-zA-Z]", urlslist2)]
urlslist3 = urlslist3[-c(grep("webcache", urlslist3))]
urlslist3 = urlslist3[-c(grep("radio-show", urlslist3))]


##start a data frame
HANNITYDF = matrix(NA, nrow = length(urlslist), ncol = 3)
colnames(HANNITYDF) = c("Source", "Url", "Text")
HANNITYDF = as.data.frame(HANNITYDF)
int <- interval(ymd("2018-12-08"), ymd("2019-02-08"))

for (i in 1:length(urlslist3)){
  
  webpage = read_html(urlslist3[i])
  
  headline_data = html_nodes(webpage, '.author-info')
  text_data = html_text(headline_data)
  text_data = strsplit(text_data, " ")[[1]][7]
  date = as.Date(text_data, "%m.%d.%y")
  
  if (date %within% int){
    headline_data2 = html_nodes(webpage,'h1, p') 
    text_data2 = html_text(headline_data2)
    
    ##save the data
    HANNITYDF$Source[i] = "HANNITY"
    HANNITYDF$Url[i] = urlslist3[i]
    HANNITYDF$Text[i] = paste(text_data2, collapse = "")
  }
  
  Sys.sleep(runif(1,1,10))
  
  }
  
  
beep(sound = 7)

write.csv(HANNITYDF, "HANNITY_gs.csv", row.names = F)

```

## Sources

Articles pertaining to the Brett Kavanaugh Supreme Court confirmation hearing and the 2018-2019 U.S. Government shutdown were scraped from the websites of 10 U.S. news sources. As in Experiment 1, these sources were selected owing to their favorability among political partisans according to @Mitchell2014. The sources favored by the highest proportion of consistent liberals were *The New York Times*, *National Public Radio (NPR)*, *Slate*, *Huffington Post*, and *Politico* [@Mitchell2014]. The sources favored by the highest proportion of consistent conservatives included *Fox News*, *Breitbart*, *The Rush Limbaugh Show*, *The Blaze*, and *Sean Hannity*. Political articles referencing Brett Kavanaugh's nomination process were identified and subsequently scraped by including the URL for each source's coverage of the nomination in the *R* script. All code for this manuscript can be found at https://osf.io/5kpj7/, and the scripts, again written with the *papaja* library in *R*, are provided inline with this manuscript [@Aust2017].

## Materials

Using the *rvest* library in the statistical package *R*, we pulled body text for individual articles from each of the aforementioned 10 news sources (identified using CSS language). We compiled the articles into a dataset [@Wickham2016]. Using this dataset, we identified word count and average word count per source. This process was run for articles pertaining to Kavanaugh's nomination that were published between September 13, 2018 and October 11, 2018 inclusive. This date range was selected in reference to the widely-publicized and viewed nomination hearing on September 27, 2018. We set the start date at September 13 (two weeks before the hearing) and the end date at October 11 (two weeks after the hearing) so that we could capture a large amount of data (roughly one month) during which Kavanaugh's nomination was at its peak saturation in news coverage. 

The same process was followed for scraping articles related to the partial U.S. Government shutdown of 2018-2019. The articles scraped were published between December 8, 2018 and February 8, 2019 inclusive. Once again, the researchers elected to scrape articles published two weeks before and after the event in question in order to capitalize on the shutdown's saturation in American news media.

```{r data_clean_kav, eval = T, echo = F}
##combine all kav data into one dataset
#read it in
NYtimes_kav = read.csv("exp2_data/NYtimes_kav.csv", stringsAsFactors = F, fileEncoding="UTF-8") #ok
NPR_kav = read.csv("exp2_data/NPR_kav.csv", stringsAsFactors = F, fileEncoding="UTF-8") #ok
SLATE_kav = read.csv("exp2_data/SLATE_kav.csv", stringsAsFactors = F, fileEncoding="UTF-8") #ok
HUFFPO_kav = read.csv("exp2_data/HUFFPO_kav.csv", stringsAsFactors = F, fileEncoding="UTF-8") #ok
POLITICO_kav = read.csv("exp2_data/POLITICO_kav.csv", stringsAsFactors = F, fileEncoding="UTF-8") #ok 
FOX_kav = read.csv("exp2_data/FOX_kav.csv", stringsAsFactors = F) #ok 
BREITBART_kav = read.csv("exp2_data/BREITBART_kav.csv", stringsAsFactors = F) #ok
RUSH_kav = read.csv("exp2_data/RUSH_kav.csv", stringsAsFactors = F, fileEncoding="UTF-8") #ok 
BLAZE_kav = read.csv("exp2_data/BLAZE_kav.csv", stringsAsFactors = F, fileEncoding="UTF-8") #ok 
HANNITY_kav = read.csv("exp2_data/HANNITY_kav.csv", stringsAsFactors = F, fileEncoding="UTF-8") #ok 


#merge it together
master_kav = rbind(NYtimes_kav, NPR_kav, SLATE_kav, 
                        HUFFPO_kav, POLITICO_kav, FOX_kav, 
                        BREITBART_kav, RUSH_kav, BLAZE_kav, 
                        HANNITY_kav)

#fix this issue
master_kav$Text = gsub("\\.", "\\. ", master_kav$Text)

##get rid of the blank stuff
library(ngram)

for (i in 1:nrow(master_kav)) {
  master_kav$wordcount[i] = wordcount(master_kav$Text[i])
}

nozero = subset(master_kav, wordcount > 0)
nozero = subset(nozero, Source == "NY Times" | Source == "BLAZE" | Source == "NPR" | Source == "HUFFPO" | Source == "FOX" | Source == "HANNITY" | Source == "SLATE" | Source == "POLITICO" | Source == "RUSH" | Source == "BREITBART")
# tapply(nozero$wordcount, nozero$Source, mean)
# tapply(nozero$wordcount, nozero$Source, sum)

##take out the disqus ones - there are none
#nodis = nozero[ -c(grep("disqus", nozero$Url)), ]

##all data
final_kav = nozero
```

```{r data_clean_gs, eval = T, echo = F}
##combine all gs data into one dataset
#read it in
NYtimes_gs = read.csv("exp2_data/NYtimes_gs.csv", stringsAsFactors = F, fileEncoding = "UTF-8") #ok
NPR_gs = read.csv("exp2_data/NPR_gs.csv", stringsAsFactors = F) #ok
SLATE_gs = read.csv("exp2_data/SLATE_gs.csv", stringsAsFactors = F) #ok 
HUFFPO_gs = read.csv("exp2_data/HUFFPO_gs.csv", stringsAsFactors = F) #ok 
POLITICO_gs = read.csv("exp2_data/POLITICO_gs.csv", stringsAsFactors = F, fileEncoding = "UTF-8") #not ok 
FOX_gs = read.csv("exp2_data/FOX_gs.csv", stringsAsFactors = F) #ok 
BREITBART_gs = read.csv("exp2_data/BREITBART_gs.csv", stringsAsFactors = F) #ok 
RUSH_gs = read.csv("exp2_data/RUSH_gs.csv", stringsAsFactors = F, fileEncoding = "UTF-8") #not ok 
BLAZE_gs = read.csv("exp2_data/BLAZE_gs.csv", stringsAsFactors = F) #ok 
HANNITY_gs = read.csv("exp2_data/HANNITY_gs.csv", stringsAsFactors = F) #ok 

#merge it together
master_gs = rbind(NYtimes_gs, NPR_gs, SLATE_gs, 
                        HUFFPO_gs, POLITICO_gs, FOX_gs, 
                        BREITBART_gs, RUSH_gs, BLAZE_gs, 
                        HANNITY_gs)

#fix this issue
master_gs$Text = gsub("\\.", "\\. ", master_gs$Text)

for (i in 1:nrow(master_gs)) {
  master_gs$wordcount[i] = wordcount(master_gs$Text[i])
}

nozero = subset(master_gs, wordcount > 0)
nozero = subset(nozero, Source == "NY Times" | Source == "BLAZE" | Source == "NPR" | Source == "HUFFPO" | Source == "FOX" | Source == "HANNITY" | Source == "SLATE" | Source == "POLITICO" | Source == "RUSH" | Source == "BREITBART")

# tapply(nozero$wordcount, nozero$Source, mean)
# tapply(nozero$wordcount, nozero$Source, sum)

##take out the disqus ones
#nodis = nozero[ -c(grep("disqus", nozero$Url)), ]

##all data
final_gs = nozero
```

```{r stem_text_both, eval = T, echo = F}
#process the Text column (write a loop) - save processed text as a separate column
for(i in 1:nrow(final_kav)) {
  
  final_kav$edited[i] = preprocess(final_kav$Text[i], #one value at a time
           case = "lower", 
           remove.punct = TRUE,
           remove.numbers = FALSE, 
           fix.spacing = TRUE)

##stem the words - do this second in the loop
  final_kav$stemmed[i] = stemDocument(final_kav$edited[i], language = "english")
}

#process the Text column (write a loop) - save processed text as a separate column
for(i in 1:nrow(final_gs)) {
  
  final_gs$edited[i] = preprocess(final_gs$Text[i], #one value at a time
           case = "lower", 
           remove.punct = TRUE,
           remove.numbers = FALSE, 
           fix.spacing = TRUE)

##stem the words - do this second in the loop
  final_gs$stemmed[i] = stemDocument(final_gs$edited[i], language = "english")
}

```

## Data analysis

As in Experiment 1, the text was scanned with *ngram*. Again, blank articles, text from the Disqus system, and duplicate articles were removed [@Schmidt2017]. The text was processed and stemmed in order to convert to a usable form for further analysis [@Feinerer2017]. Words were subsequently matched with their valences from @Warriner2013. Depending on the results of @Jordan2019's multi-trait multi-method analysis of the MFD and the Moral Foundations Questionnaire, alternative forms of the Moral Foundations Dictionary with additional words may be imported instead of the original dictionary.

Using the *tm* and *ngram* packages in *R*, the researchers processed the text in order to convert it to lowercase, fix spacing anomalies, and remove punctuation [@Feinerer2017]. Each individual word was reduced to its stem (i.e., *diseased* was stemmed to *diseas*). Once again, the same procedure was applied to the MFD words and the words in the @Warriner2013 dataset. Using the @Warriner2013 dictionary, the words in the MFD were assigned their respective valence. The researchers obtained the words' percent occurrence in the text. Percents were multiplied by *z*-scored valence and categorized into their proper MFD category.

```{r create_datasets_harm_kav, eval = T, echo = F, include = F}
#read in the harm total words from the mtmm project
harm_all_kav = read.csv("exp1_data/harm_words_total.csv", stringsAsFactors = F)

harm_final_data_kav = data.frame("URL" = NA, "Source" = NA, "Var1" = NA, "Freq" = NA, "wordcount" = NA)

for (i in 1:nrow(final_kav)) {
##first make a table of a response, unlisting everything
temp = as.data.frame(table(unlist(strsplit(final_kav$stemmed[i], " "))))

#only pull the words in the dataset for each foundation
temp2 = temp[temp$Var1 %in% unique(harm_all_kav$words[harm_all_kav$words != "" & harm_all_kav$words != "NA" & !is.na(harm_all_kav$words)]), ]

#control here for times when no words
if (nrow(temp2) == 0){
  temp2 = data.frame("Var1" = 0, "Freq" = 0)
}

#add to the dataset
temp2$URL = final_kav$Url[i]
temp2$Source = final_kav$Source[i]
temp2$wordcount = final_kav$wordcount[i]

#bind with original
harm_final_data_kav = rbind(harm_final_data_kav, temp2)
}

#pull in the data from Warriner 
harm_final_data_kav$valence = vlookup(harm_final_data_kav$Var1, #index in original data
                                  warriner, #where's the stuff you want to put in
                                  "V.Mean.Sum", #what column is the stuff you want
                                  "Word2") #what's the matching index

#figure out stupid words
#table(harm_final_data_kav$Var1[is.na(harm_final_data_kav$valence) & harm_final_data_kav$Freq > 0])
#fought, safer, spurn
harm_final_data_kav$valence[harm_final_data_kav$Var1 == "fought"] = warriner$V.Mean.Sum[warriner$Word2 == "fight"]
harm_final_data_kav$valence[harm_final_data_kav$Var1 == "safer"] = warriner$V.Mean.Sum[warriner$Word2 == "safe"]
#no spurn avaliable
```

```{r create_datasets_fair_kav, include=FALSE}
fair_all_kav = read.csv("exp1_data/fair_words_total.csv", stringsAsFactors = F)

fair_final_data_kav = data.frame("URL" = NA, "Source" = NA, "Var1" = NA, "Freq" = NA, "wordcount" = NA)

for (i in 1:nrow(final_kav)) {
##first make a table of a response, unlisting everything
temp = as.data.frame(table(unlist(strsplit(final_kav$stemmed[i], " "))))

#only pull the words in the dataset for each foundation
temp2 = temp[temp$Var1 %in% unique(fair_all_kav$words[fair_all_kav$words != "" & fair_all_kav$words != "NA" & !is.na(fair_all_kav$words)]), ]

#control here for times when no words
if (nrow(temp2) == 0){
  temp2 = data.frame("Var1" = 0, "Freq" = 0)
}

#add to the dataset
temp2$URL = final_kav$Url[i]
temp2$Source = final_kav$Source[i]
temp2$wordcount = final_kav$wordcount[i]

#bind with original
fair_final_data_kav = rbind(fair_final_data_kav, temp2)
}

fair_final_data_kav$valence = vlookup(fair_final_data_kav$Var1, #index in original data
                                  warriner, #where's the stuff you want to put in
                                  "V.Mean.Sum", #what column is the stuff you want
                                  "Word2") #what's the matching index

#figure out stupid words
table(fair_final_data_kav$Var1[is.na(fair_final_data_kav$valence) & fair_final_data_kav$Freq > 0])
#just plagiar safer
fair_final_data_kav$valence[fair_final_data_kav$Var1 == "safer"] = warriner$V.Mean.Sum[warriner$Word2 == "safe"]
#nothing for just or plagiarism
```

```{r create_datasets_ingroup_kav, include=FALSE}
ingroup_all_kav = read.csv("exp1_data/ingroup_words_total.csv", stringsAsFactors = F)

ingroup_final_data_kav = data.frame("URL" = NA, "Source" = NA, "Var1" = NA, "Freq" = NA, "wordcount" = NA)

for (i in 1:nrow(final_kav)) {
##first make a table of a response, unlisting everything
temp = as.data.frame(table(unlist(strsplit(final_kav$stemmed[i], " "))))

#only pull the words in the dataset for each foundation
temp2 = temp[temp$Var1 %in% unique(ingroup_all_kav$words[ingroup_all_kav$words != "" & ingroup_all_kav$words != "NA" & !is.na(ingroup_all_kav$words)]), ]

#control here for times when no words
if (nrow(temp2) == 0){
  temp2 = data.frame("Var1" = 0, "Freq" = 0)
}

#add to the dataset
temp2$URL = final_kav$Url[i]
temp2$Source = final_kav$Source[i]
temp2$wordcount = final_kav$wordcount[i]

#bind with original
ingroup_final_data_kav = rbind(ingroup_final_data_kav, temp2)
}

ingroup_final_data_kav$valence = vlookup(ingroup_final_data_kav$Var1, #index in original data
                                  warriner, #where's the stuff you want to put in
                                  "V.Mean.Sum", #what column is the stuff you want
                                  "Word2") #what's the matching index

#figure out stupid words
table(ingroup_final_data_kav$Var1[is.na(ingroup_final_data_kav$valence) & ingroup_final_data_kav$Freq > 0])
#backstabb  benedict      best      fals   indvidu      juda    knight      true
ingroup_final_data_kav$valence[ingroup_final_data_kav$Var1 == "fals"] = warriner$V.Mean.Sum[warriner$Word2 == "FALSE"]
ingroup_final_data_kav$valence[ingroup_final_data_kav$Var1 == "indvidu"] = mean(warriner$V.Mean.Sum[warriner$Word2 == "individu"])
ingroup_final_data_kav$valence[ingroup_final_data_kav$Var1 == "true"] = mean(warriner$V.Mean.Sum[warriner$Word2 == "TRUE"])
```

```{r create_datasets_authority_kav, include=FALSE}
authority_all_kav = read.csv("exp1_data/authority_words_total.csv", stringsAsFactors = F)

authority_final_data_kav = data.frame("URL" = NA, "Source" = NA, "Var1" = NA, "Freq" = NA, "wordcount" = NA)

for (i in 1:nrow(final_kav)) {
##first make a table of a response, unlisting everything
temp = as.data.frame(table(unlist(strsplit(final_kav$stemmed[i], " "))))

#only pull the words in the dataset for each foundation
temp2 = temp[temp$Var1 %in% unique(authority_all_kav$words[authority_all_kav$words != "" & authority_all_kav$words != "NA" & !is.na(authority_all_kav$words)]), ]

#control here for times when no words
if (nrow(temp2) == 0){
  temp2 = data.frame("Var1" = 0, "Freq" = 0)
}

#add to the dataset
temp2$URL = final_kav$Url[i]
temp2$Source = final_kav$Source[i]
temp2$wordcount = final_kav$wordcount[i]

#bind with original
authority_final_data_kav = rbind(authority_final_data_kav, temp2)
}

authority_final_data_kav$valence = vlookup(authority_final_data_kav$Var1, #index in original data
                                  warriner, #where's the stuff you want to put in
                                  "V.Mean.Sum", #what column is the stuff you want
                                  "Word2") #what's the matching index

#figure out stupid words
table(authority_final_data_kav$Var1[is.na(authority_final_data_kav$valence) & authority_final_data_kav$Freq > 0])
#  abov behind   gase higher  older  taken 
authority_final_data_kav$valence[authority_final_data_kav$Var1 == "gase"] = warriner$V.Mean.Sum[warriner$Word2 == "gas"]
authority_final_data_kav$valence[authority_final_data_kav$Var1 == "higher"] = mean(warriner$V.Mean.Sum[warriner$Word2 == "high"])
authority_final_data_kav$valence[authority_final_data_kav$Var1 == "older"] = mean(warriner$V.Mean.Sum[warriner$Word2 == "old"])
authority_final_data_kav$valence[authority_final_data_kav$Var1 == "taken"] = mean(warriner$V.Mean.Sum[warriner$Word2 == "take"])
```

```{r create_datasets_purity_kav, include=FALSE}
purity_all_kav = read.csv("exp1_data/purity_words_total.csv", stringsAsFactors = F)

purity_final_data_kav = data.frame("URL" = NA, "Source" = NA, "Var1" = NA, "Freq" = NA, "wordcount" = NA)

for (i in 1:nrow(final_kav)) {
##first make a table of a response, unlisting everything
temp = as.data.frame(table(unlist(strsplit(final_kav$stemmed[i], " "))))

#only pull the words in the dataset for each foundation
temp2 = temp[temp$Var1 %in% unique(purity_all_kav$words[purity_all_kav$words != "" & purity_all_kav$words != "NA" & !is.na(purity_all_kav$words)]), ]

#control here for times when no words
if (nrow(temp2) == 0){
  temp2 = data.frame("Var1" = 0, "Freq" = 0)
}

#add to the dataset
temp2$URL = final_kav$Url[i]
temp2$Source = final_kav$Source[i]
temp2$wordcount = final_kav$wordcount[i]

#bind with original
purity_final_data_kav = rbind(purity_final_data_kav, temp2)
}

purity_final_data_kav$valence = vlookup(purity_final_data_kav$Var1, #index in original data
                                  warriner, #where's the stuff you want to put in
                                  "V.Mean.Sum", #what column is the stuff you want
                                  "Word2") #what's the matching index

#figure out stupid words
table(purity_final_data_kav$Var1[is.na(purity_final_data_kav$valence) & purity_final_data_kav$Freq > 0])
#befor christian     jesus      mari      none    sacred    someon      true     women 
purity_final_data_kav$valence[purity_final_data_kav$Var1 == "sacred"] = warriner$V.Mean.Sum[warriner$Word2 == "sacr"]
purity_final_data_kav$valence[purity_final_data_kav$Var1 == "true"] = mean(warriner$V.Mean.Sum[warriner$Word2 == "TRUE"])
purity_final_data_kav$valence[purity_final_data_kav$Var1 == "women"] = mean(warriner$V.Mean.Sum[warriner$Word2 == "woman"])
```

```{r create_datasets_harm_gs, eval = T, echo = F, include = F}
#read in the harm total words from the mtmm project
harm_all_gs = read.csv("exp1_data/harm_words_total.csv", stringsAsFactors = F)

harm_final_data_gs = data.frame("URL" = NA, "Source" = NA, "Var1" = NA, "Freq" = NA, "wordcount" = NA)

for (i in 1:nrow(final_gs)) {
##first make a table of a response, unlisting everything
temp = as.data.frame(table(unlist(strsplit(final_gs$stemmed[i], " "))))

#only pull the words in the dataset for each foundation
temp2 = temp[temp$Var1 %in% unique(harm_all_gs$words[harm_all_gs$words != "" & harm_all_gs$words != "NA" & !is.na(harm_all_gs$words)]), ]

#control here for times when no words
if (nrow(temp2) == 0){
  temp2 = data.frame("Var1" = 0, "Freq" = 0)
}

#add to the dataset
temp2$URL = final_gs$Url[i]
temp2$Source = final_gs$Source[i]
temp2$wordcount = final_gs$wordcount[i]

#bind with original
harm_final_data_gs = rbind(harm_final_data_gs, temp2)
}

#pull in the data from Warriner 
harm_final_data_gs$valence = vlookup(harm_final_data_gs$Var1, #index in original data
                                  warriner, #where's the stuff you want to put in
                                  "V.Mean.Sum", #what column is the stuff you want
                                  "Word2") #what's the matching index

#figure out stupid words
#table(harm_final_data_gs$Var1[is.na(harm_final_data_gs$valence) & harm_final_data_gs$Freq > 0])
#fought, safer, spurn
harm_final_data_gs$valence[harm_final_data_gs$Var1 == "fought"] = warriner$V.Mean.Sum[warriner$Word2 == "fight"]
harm_final_data_gs$valence[harm_final_data_gs$Var1 == "safer"] = warriner$V.Mean.Sum[warriner$Word2 == "safe"]
#no spurn avaliable
```

```{r create_datasets_fair_gs, echo = F, include=FALSE}
fair_all_gs = read.csv("exp1_data/fair_words_total.csv", stringsAsFactors = F)

fair_final_data_gs = data.frame("URL" = NA, "Source" = NA, "Var1" = NA, "Freq" = NA, "wordcount" = NA)

for (i in 1:nrow(final_gs)) {
##first make a table of a response, unlisting everything
temp = as.data.frame(table(unlist(strsplit(final_gs$stemmed[i], " "))))

#only pull the words in the dataset for each foundation
temp2 = temp[temp$Var1 %in% unique(fair_all_gs$words[fair_all_gs$words != "" & fair_all_gs$words != "NA" & !is.na(fair_all_gs$words)]), ]

#control here for times when no words
if (nrow(temp2) == 0){
  temp2 = data.frame("Var1" = 0, "Freq" = 0)
}

#add to the dataset
temp2$URL = final_gs$Url[i]
temp2$Source = final_gs$Source[i]
temp2$wordcount = final_gs$wordcount[i]

#bind with original
fair_final_data_gs = rbind(fair_final_data_gs, temp2)
}

fair_final_data_gs$valence = vlookup(fair_final_data_gs$Var1, #index in original data
                                  warriner, #where's the stuff you want to put in
                                  "V.Mean.Sum", #what column is the stuff you want
                                  "Word2") #what's the matching index

#figure out stupid words
table(fair_final_data_gs$Var1[is.na(fair_final_data_gs$valence) & fair_final_data_gs$Freq > 0])
#just plagiar safer
fair_final_data_gs$valence[fair_final_data_gs$Var1 == "safer"] = warriner$V.Mean.Sum[warriner$Word2 == "safe"]
#nothing for just or plagiarism
```

```{r create_datasets_ingroup_gs, echo = F, include=FALSE}
ingroup_all_gs = read.csv("exp1_data/ingroup_words_total.csv", stringsAsFactors = F)

ingroup_final_data_gs = data.frame("URL" = NA, "Source" = NA, "Var1" = NA, "Freq" = NA, "wordcount" = NA)

for (i in 1:nrow(final_gs)) {
##first make a table of a response, unlisting everything
temp = as.data.frame(table(unlist(strsplit(final_gs$stemmed[i], " "))))

#only pull the words in the dataset for each foundation
temp2 = temp[temp$Var1 %in% unique(ingroup_all_gs$words[ingroup_all_gs$words != "" & ingroup_all_gs$words != "NA" & !is.na(ingroup_all_gs$words)]), ]

#control here for times when no words
if (nrow(temp2) == 0){
  temp2 = data.frame("Var1" = 0, "Freq" = 0)
}

#add to the dataset
temp2$URL = final_gs$Url[i]
temp2$Source = final_gs$Source[i]
temp2$wordcount = final_gs$wordcount[i]

#bind with original
ingroup_final_data_gs = rbind(ingroup_final_data_gs, temp2)
}

ingroup_final_data_gs$valence = vlookup(ingroup_final_data_gs$Var1, #index in original data
                                  warriner, #where's the stuff you want to put in
                                  "V.Mean.Sum", #what column is the stuff you want
                                  "Word2") #what's the matching index

#figure out stupid words
table(ingroup_final_data_gs$Var1[is.na(ingroup_final_data_gs$valence) & ingroup_final_data_gs$Freq > 0])
#backstabb  benedict      best      fals   indvidu      juda    knight      true
ingroup_final_data_gs$valence[ingroup_final_data_gs$Var1 == "fals"] = warriner$V.Mean.Sum[warriner$Word2 == "FALSE"]
ingroup_final_data_gs$valence[ingroup_final_data_gs$Var1 == "indvidu"] = mean(warriner$V.Mean.Sum[warriner$Word2 == "individu"])
ingroup_final_data_gs$valence[ingroup_final_data_gs$Var1 == "true"] = mean(warriner$V.Mean.Sum[warriner$Word2 == "TRUE"])
```

```{r create_datasets_authority_gs, echo = F, include=FALSE}
authority_all_gs = read.csv("exp1_data/authority_words_total.csv", stringsAsFactors = F)

authority_final_data_gs = data.frame("URL" = NA, "Source" = NA, "Var1" = NA, "Freq" = NA, "wordcount" = NA)

for (i in 1:nrow(final_gs)) {
##first make a table of a response, unlisting everything
temp = as.data.frame(table(unlist(strsplit(final_gs$stemmed[i], " "))))

#only pull the words in the dataset for each foundation
temp2 = temp[temp$Var1 %in% unique(authority_all_gs$words[authority_all_gs$words != "" & authority_all_gs$words != "NA" & !is.na(authority_all_gs$words)]), ]

#control here for times when no words
if (nrow(temp2) == 0){
  temp2 = data.frame("Var1" = 0, "Freq" = 0)
}

#add to the dataset
temp2$URL = final_gs$Url[i]
temp2$Source = final_gs$Source[i]
temp2$wordcount = final_gs$wordcount[i]

#bind with original
authority_final_data_gs = rbind(authority_final_data_gs, temp2)
}

authority_final_data_gs$valence = vlookup(authority_final_data_gs$Var1, #index in original data
                                  warriner, #where's the stuff you want to put in
                                  "V.Mean.Sum", #what column is the stuff you want
                                  "Word2") #what's the matching index

#figure out stupid words
table(authority_final_data_gs$Var1[is.na(authority_final_data_gs$valence) & authority_final_data_gs$Freq > 0])
#  abov behind   gase higher  older  taken 
authority_final_data_gs$valence[authority_final_data_gs$Var1 == "gase"] = warriner$V.Mean.Sum[warriner$Word2 == "gas"]
authority_final_data_gs$valence[authority_final_data_gs$Var1 == "higher"] = mean(warriner$V.Mean.Sum[warriner$Word2 == "high"])
authority_final_data_gs$valence[authority_final_data_gs$Var1 == "older"] = mean(warriner$V.Mean.Sum[warriner$Word2 == "old"])
authority_final_data_gs$valence[authority_final_data_gs$Var1 == "taken"] = mean(warriner$V.Mean.Sum[warriner$Word2 == "take"])
```

```{r create_datasets_purity_gs, echo = F, include=FALSE}
purity_all_gs = read.csv("exp1_data/purity_words_total.csv", stringsAsFactors = F)

purity_final_data_gs = data.frame("URL" = NA, "Source" = NA, "Var1" = NA, "Freq" = NA, "wordcount" = NA)

for (i in 1:nrow(final_gs)) {
##first make a table of a response, unlisting everything
temp = as.data.frame(table(unlist(strsplit(final_gs$stemmed[i], " "))))

#only pull the words in the dataset for each foundation
temp2 = temp[temp$Var1 %in% unique(purity_all_gs$words[purity_all_gs$words != "" & purity_all_gs$words != "NA" & !is.na(purity_all_gs$words)]), ]

#control here for times when no words
if (nrow(temp2) == 0){
  temp2 = data.frame("Var1" = 0, "Freq" = 0)
}

#add to the dataset
temp2$URL = final_gs$Url[i]
temp2$Source = final_gs$Source[i]
temp2$wordcount = final_gs$wordcount[i]

#bind with original
purity_final_data_gs = rbind(purity_final_data_gs, temp2)
}

purity_final_data_gs$valence = vlookup(purity_final_data_gs$Var1, #index in original data
                                  warriner, #where's the stuff you want to put in
                                  "V.Mean.Sum", #what column is the stuff you want
                                  "Word2") #what's the matching index

#figure out stupid words
table(purity_final_data_gs$Var1[is.na(purity_final_data_gs$valence) & purity_final_data_gs$Freq > 0])
#befor christian     jesus      mari      none    sacred    someon      true     women 
purity_final_data_gs$valence[purity_final_data_gs$Var1 == "sacred"] = warriner$V.Mean.Sum[warriner$Word2 == "sacr"]
purity_final_data_gs$valence[purity_final_data_gs$Var1 == "true"] = mean(warriner$V.Mean.Sum[warriner$Word2 == "TRUE"])
purity_final_data_gs$valence[purity_final_data_gs$Var1 == "women"] = mean(warriner$V.Mean.Sum[warriner$Word2 == "woman"])
```

```{r descriptives_both, echo = F, include = FALSE}
##summarize the dataset so that you have the total and percent for each foundation
#kav
summary(harm_final_data_kav)
summary(fair_final_data_kav)
summary(ingroup_final_data_kav)
summary(authority_final_data_kav)
summary(purity_final_data_kav)
#gs
summary(harm_final_data_gs)
summary(fair_final_data_gs)
summary(ingroup_final_data_gs)
summary(authority_final_data_gs)
summary(purity_final_data_gs)
##report descriptive statistics of the foundations
#kav
mean(harm_final_data_kav$valence, na.rm = T)
sd(harm_final_data_kav$valence, na.rm = T)
mean(fair_final_data_kav$valence, na.rm = T)
sd(fair_final_data_kav$valence, na.rm = T)
mean(ingroup_final_data_kav$valence, na.rm = T)
sd(ingroup_final_data_kav$valence, na.rm = T)
mean(authority_final_data_kav$valence, na.rm = T)
sd(authority_final_data_kav$valence, na.rm = T)
mean(purity_final_data_kav$valence, na.rm = T)
sd(purity_final_data_kav$valence, na.rm = T)
#gs
mean(harm_final_data_gs$valence, na.rm = T)
sd(harm_final_data_gs$valence, na.rm = T)
mean(fair_final_data_gs$valence, na.rm = T)
sd(fair_final_data_gs$valence, na.rm = T)
mean(ingroup_final_data_gs$valence, na.rm = T)
sd(ingroup_final_data_gs$valence, na.rm = T)
mean(authority_final_data_gs$valence, na.rm = T)
sd(authority_final_data_gs$valence, na.rm = T)
mean(purity_final_data_gs$valence, na.rm = T)
sd(purity_final_data_gs$valence, na.rm = T)
#note that a big problem is the valence of the words 
```

```{r analysis_harm_kav, include = FALSE}
#final_data clean up
#multiply by frequency and create percentage
harm_final_data_kav$weight = harm_final_data_kav$Freq * harm_final_data_kav$valence / harm_final_data_kav$wordcount * 100

#flatten down to sum by article 
harm_final_temp_kav = aggregate(harm_final_data_kav$weight, 
          by = list(harm_final_data_kav$URL),
          FUN = sum, na.rm = T)
colnames(harm_final_temp_kav) = c("URL", "MFDweight")

harm_final_temp_kav$lean[grepl("nytimes.com", harm_final_temp_kav$URL) |
                       grepl("npr.org", harm_final_temp_kav$URL) |
                        grepl("slate.com", harm_final_temp_kav$URL) |
                        grepl("huffingtonpost.com", harm_final_temp_kav$URL) |
                        grepl("politico.com", harm_final_temp_kav$URL)] = "Liberal"
                              
harm_final_temp_kav$lean[grepl("breitbart.com", harm_final_temp_kav$URL) |
                       grepl("foxnews.com", harm_final_temp_kav$URL) |
                       grepl("rushlimbaugh.com", harm_final_temp_kav$URL) |
                       grepl("theblaze.com", harm_final_temp_kav$URL) |
                       grepl("hannity.com", harm_final_temp_kav$URL)] = "Conservative"

harm_final_temp_kav$Source[grepl("nytimes.com", harm_final_temp_kav$URL)] = "NY Times"
harm_final_temp_kav$Source[grepl("npr.org", harm_final_temp_kav$URL)] = "NPR"
harm_final_temp_kav$Source[grepl("slate.com", harm_final_temp_kav$URL)] = "Slate"
harm_final_temp_kav$Source[grepl("huffingtonpost.com", harm_final_temp_kav$URL)] = "Huffington Post"
harm_final_temp_kav$Source[grepl("politico.com", harm_final_temp_kav$URL)] = "Politico"
harm_final_temp_kav$Source[grepl("foxnews.com", harm_final_temp_kav$URL)] = "Fox"
harm_final_temp_kav$Source[grepl("breitbart.com", harm_final_temp_kav$URL)] = "Breitbart"
harm_final_temp_kav$Source[grepl("rushlimbaugh.com", harm_final_temp_kav$URL)] = "Rush Limbaugh"
harm_final_temp_kav$Source[grepl("theblaze.com", harm_final_temp_kav$URL)] = "The Blaze"
harm_final_temp_kav$Source[grepl("hannity.com", harm_final_temp_kav$URL)] = "Sean Hannity"
harm_model_kav = lme(MFDweight ~ lean, 
                 data = harm_final_temp_kav, 
                 method = "ML",
                 na.action = "na.omit",
                 random = list(~1|Source))
summary(harm_model_kav)
r.squaredGLMM(harm_model_kav)
```

```{r analysis_fair_kav, include = FALSE}
#final_data clean up
#multiply by frequency and create percentage
fair_final_data_kav$weight = fair_final_data_kav$Freq * fair_final_data_kav$valence / fair_final_data_kav$wordcount * 100

#flatten down to sum by article 
fair_final_temp_kav = aggregate(fair_final_data_kav$weight, 
          by = list(fair_final_data_kav$URL),
          FUN = sum, na.rm = T)
colnames(fair_final_temp_kav) = c("URL", "MFDweight")

fair_final_temp_kav$lean[grepl("nytimes.com", fair_final_temp_kav$URL) |
                       grepl("npr.org", fair_final_temp_kav$URL) |
                        grepl("slate.com", fair_final_temp_kav$URL) |
                        grepl("huffingtonpost.com", fair_final_temp_kav$URL) |
                        grepl("politico.com", fair_final_temp_kav$URL)] = "Liberal"
                              
fair_final_temp_kav$lean[grepl("breitbart.com", fair_final_temp_kav$URL) |
                       grepl("foxnews.com", fair_final_temp_kav$URL) |
                       grepl("rushlimbaugh.com", fair_final_temp_kav$URL) |
                       grepl("theblaze.com", fair_final_temp_kav$URL) |
                       grepl("hannity.com", fair_final_temp_kav$URL)] = "Conservative"

fair_final_temp_kav$Source[grepl("nytimes.com", fair_final_temp_kav$URL)] = "NY Times"
fair_final_temp_kav$Source[grepl("npr.org", fair_final_temp_kav$URL)] = "NPR"
fair_final_temp_kav$Source[grepl("slate.com", fair_final_temp_kav$URL)] = "Slate"
fair_final_temp_kav$Source[grepl("huffingtonpost.com", fair_final_temp_kav$URL)] = "Huffington Post"
fair_final_temp_kav$Source[grepl("politico.com", fair_final_temp_kav$URL)] = "Politico"
fair_final_temp_kav$Source[grepl("foxnews.com", fair_final_temp_kav$URL)] = "Fox"
fair_final_temp_kav$Source[grepl("breitbart.com", fair_final_temp_kav$URL)] = "Breitbart"
fair_final_temp_kav$Source[grepl("rushlimbaugh.com", fair_final_temp_kav$URL)] = "Rush Limbaugh"
fair_final_temp_kav$Source[grepl("theblaze.com", fair_final_temp_kav$URL)] = "The Blaze"
fair_final_temp_kav$Source[grepl("hannity.com", fair_final_temp_kav$URL)] = "Sean Hannity"
fair_model_kav = lme(MFDweight ~ lean, 
                 data = fair_final_temp_kav, 
                 method = "ML",
                 na.action = "na.omit",
                 random = list(~1|Source))
summary(fair_model_kav)
```

```{r analysis_ingroup_kav, include = FALSE}
#final_data clean up
#multiply by frequency and create percentage
ingroup_final_data_kav$weight = ingroup_final_data_kav$Freq * ingroup_final_data_kav$valence / ingroup_final_data_kav$wordcount * 100

#flatten down to sum by article 
ingroup_final_temp_kav = aggregate(ingroup_final_data_kav$weight, 
          by = list(ingroup_final_data_kav$URL),
          FUN = sum, na.rm = T)
colnames(ingroup_final_temp_kav) = c("URL", "MFDweight")

ingroup_final_temp_kav$lean[grepl("nytimes.com", ingroup_final_temp_kav$URL) |
                       grepl("npr.org", ingroup_final_temp_kav$URL) |
                        grepl("slate.com", ingroup_final_temp_kav$URL) |
                        grepl("huffingtonpost.com", ingroup_final_temp_kav$URL) |
                        grepl("politico.com", ingroup_final_temp_kav$URL)] = "Liberal"
                              
ingroup_final_temp_kav$lean[grepl("breitbart.com", ingroup_final_temp_kav$URL) |
                       grepl("foxnews.com", ingroup_final_temp_kav$URL) |
                       grepl("rushlimbaugh.com", ingroup_final_temp_kav$URL) |
                       grepl("theblaze.com", ingroup_final_temp_kav$URL) |
                       grepl("hannity.com", ingroup_final_temp_kav$URL)] = "Conservative"

ingroup_final_temp_kav$Source[grepl("nytimes.com", ingroup_final_temp_kav$URL)] = "NY Times"
ingroup_final_temp_kav$Source[grepl("npr.org", ingroup_final_temp_kav$URL)] = "NPR"
ingroup_final_temp_kav$Source[grepl("slate.com", ingroup_final_temp_kav$URL)] = "Slate"
ingroup_final_temp_kav$Source[grepl("huffingtonpost.com", ingroup_final_temp_kav$URL)] = "Huffington Post"
ingroup_final_temp_kav$Source[grepl("politico.com", ingroup_final_temp_kav$URL)] = "Politico"
ingroup_final_temp_kav$Source[grepl("foxnews.com", ingroup_final_temp_kav$URL)] = "Fox"
ingroup_final_temp_kav$Source[grepl("breitbart.com", ingroup_final_temp_kav$URL)] = "Breitbart"
ingroup_final_temp_kav$Source[grepl("rushlimbaugh.com", ingroup_final_temp_kav$URL)] = "Rush Limbaugh"
ingroup_final_temp_kav$Source[grepl("theblaze.com", ingroup_final_temp_kav$URL)] = "The Blaze"
ingroup_final_temp_kav$Source[grepl("hannity.com", ingroup_final_temp_kav$URL)] = "Sean Hannity"
ingroup_model_kav = lme(MFDweight ~ lean, 
                 data = ingroup_final_temp_kav, 
                 method = "ML",
                 na.action = "na.omit",
                 random = list(~1|Source))
summary(ingroup_model_kav)
```

```{r analysis_authority_kav, include = FALSE}
#final_data clean up
#multiply by frequency and create percentage
authority_final_data_kav$weight = authority_final_data_kav$Freq * authority_final_data_kav$valence / authority_final_data_kav$wordcount * 100

#flatten down to sum by article 
authority_final_temp_kav = aggregate(authority_final_data_kav$weight, 
          by = list(authority_final_data_kav$URL),
          FUN = sum, na.rm = T)
colnames(authority_final_temp_kav) = c("URL", "MFDweight")

authority_final_temp_kav$lean[grepl("nytimes.com", authority_final_temp_kav$URL) |
                       grepl("npr.org", authority_final_temp_kav$URL) |
                        grepl("slate.com", authority_final_temp_kav$URL) |
                        grepl("huffingtonpost.com", authority_final_temp_kav$URL) |
                        grepl("politico.com", authority_final_temp_kav$URL)] = "Liberal"
                              
authority_final_temp_kav$lean[grepl("breitbart.com", authority_final_temp_kav$URL) |
                       grepl("foxnews.com", authority_final_temp_kav$URL) |
                       grepl("rushlimbaugh.com", authority_final_temp_kav$URL) |
                       grepl("theblaze.com", authority_final_temp_kav$URL) |
                       grepl("hannity.com", authority_final_temp_kav$URL)] = "Conservative"

authority_final_temp_kav$Source[grepl("nytimes.com", authority_final_temp_kav$URL)] = "NY Times"
authority_final_temp_kav$Source[grepl("npr.org", authority_final_temp_kav$URL)] = "NPR"
authority_final_temp_kav$Source[grepl("slate.com", authority_final_temp_kav$URL)] = "Slate"
authority_final_temp_kav$Source[grepl("huffingtonpost.com", authority_final_temp_kav$URL)] = "Huffington Post"
authority_final_temp_kav$Source[grepl("politico.com", authority_final_temp_kav$URL)] = "Politico"
authority_final_temp_kav$Source[grepl("foxnews.com", authority_final_temp_kav$URL)] = "Fox"
authority_final_temp_kav$Source[grepl("breitbart.com", authority_final_temp_kav$URL)] = "Breitbart"
authority_final_temp_kav$Source[grepl("rushlimbaugh.com", authority_final_temp_kav$URL)] = "Rush Limbaugh"
authority_final_temp_kav$Source[grepl("theblaze.com", authority_final_temp_kav$URL)] = "The Blaze"
authority_final_temp_kav$Source[grepl("hannity.com", authority_final_temp_kav$URL)] = "Sean Hannity"
authority_model_kav = lme(MFDweight ~ lean, 
                 data = authority_final_temp_kav, 
                 method = "ML",
                 na.action = "na.omit",
                 random = list(~1|Source))
summary(authority_model_kav)
```

```{r analysis_purity_kav, include = FALSE}
#final_data clean up
#multiply by frequency and create percentage
purity_final_data_kav$weight = purity_final_data_kav$Freq * purity_final_data_kav$valence / purity_final_data_kav$wordcount * 100

#flatten down to sum by article 
purity_final_temp_kav = aggregate(purity_final_data_kav$weight, 
          by = list(purity_final_data_kav$URL),
          FUN = sum, na.rm = T)
colnames(purity_final_temp_kav) = c("URL", "MFDweight")

purity_final_temp_kav$lean[grepl("nytimes.com", purity_final_temp_kav$URL) |
                       grepl("npr.org", purity_final_temp_kav$URL) |
                        grepl("slate.com", purity_final_temp_kav$URL) |
                        grepl("huffingtonpost.com", purity_final_temp_kav$URL) |
                        grepl("politico.com", purity_final_temp_kav$URL)] = "Liberal"
                              
purity_final_temp_kav$lean[grepl("breitbart.com", purity_final_temp_kav$URL) |
                       grepl("foxnews.com", purity_final_temp_kav$URL) |
                       grepl("rushlimbaugh.com", purity_final_temp_kav$URL) |
                       grepl("theblaze.com", purity_final_temp_kav$URL) |
                       grepl("hannity.com", purity_final_temp_kav$URL)] = "Conservative"

purity_final_temp_kav$Source[grepl("nytimes.com", purity_final_temp_kav$URL)] = "NY Times"
purity_final_temp_kav$Source[grepl("npr.org", purity_final_temp_kav$URL)] = "NPR"
purity_final_temp_kav$Source[grepl("slate.com", purity_final_temp_kav$URL)] = "Slate"
purity_final_temp_kav$Source[grepl("huffingtonpost.com", purity_final_temp_kav$URL)] = "Huffington Post"
purity_final_temp_kav$Source[grepl("politico.com", purity_final_temp_kav$URL)] = "Politico"
purity_final_temp_kav$Source[grepl("foxnews.com", purity_final_temp_kav$URL)] = "Fox"
purity_final_temp_kav$Source[grepl("breitbart.com", purity_final_temp_kav$URL)] = "Breitbart"
purity_final_temp_kav$Source[grepl("rushlimbaugh.com", purity_final_temp_kav$URL)] = "Rush Limbaugh"
purity_final_temp_kav$Source[grepl("theblaze.com", purity_final_temp_kav$URL)] = "The Blaze"
purity_final_temp_kav$Source[grepl("hannity.com", purity_final_temp_kav$URL)] = "Sean Hannity"
purity_model_kav = lme(MFDweight ~ lean, 
                 data = purity_final_temp_kav, 
                 method = "ML",
                 na.action = "na.omit",
                 random = list(~1|Source))
summary(purity_model_kav)
```

```{r analysis_harm_gs, include = FALSE}
#final_data clean up
#multiply by frequency and create percentage
harm_final_data_gs$weight = harm_final_data_gs$Freq * harm_final_data_gs$valence / harm_final_data_gs$wordcount * 100

#flatten down to sum by article 
harm_final_temp_gs = aggregate(harm_final_data_gs$weight, 
          by = list(harm_final_data_gs$URL),
          FUN = sum, na.rm = T)
colnames(harm_final_temp_gs) = c("URL", "MFDweight")

harm_final_temp_gs$lean[grepl("nytimes.com", harm_final_temp_gs$URL) |
                       grepl("npr.org", harm_final_temp_gs$URL) |
                        grepl("slate.com", harm_final_temp_gs$URL) |
                        grepl("huffingtonpost.com", harm_final_temp_gs$URL) |
                        grepl("politico.com", harm_final_temp_gs$URL)] = "Liberal"
                              
harm_final_temp_gs$lean[grepl("breitbart.com", harm_final_temp_gs$URL) |
                       grepl("foxnews.com", harm_final_temp_gs$URL) |
                       grepl("rushlimbaugh.com", harm_final_temp_gs$URL) |
                       grepl("theblaze.com", harm_final_temp_gs$URL) |
                       grepl("hannity.com", harm_final_temp_gs$URL)] = "Conservative"

harm_final_temp_gs$Source[grepl("nytimes.com", harm_final_temp_gs$URL)] = "NY Times"
harm_final_temp_gs$Source[grepl("npr.org", harm_final_temp_gs$URL)] = "NPR"
harm_final_temp_gs$Source[grepl("slate.com", harm_final_temp_gs$URL)] = "Slate"
harm_final_temp_gs$Source[grepl("huffingtonpost.com", harm_final_temp_gs$URL)] = "Huffington Post"
harm_final_temp_gs$Source[grepl("politico.com", harm_final_temp_gs$URL)] = "Politico"
harm_final_temp_gs$Source[grepl("foxnews.com", harm_final_temp_gs$URL)] = "Fox"
harm_final_temp_gs$Source[grepl("breitbart.com", harm_final_temp_gs$URL)] = "Breitbart"
harm_final_temp_gs$Source[grepl("rushlimbaugh.com", harm_final_temp_gs$URL)] = "Rush Limbaugh"
harm_final_temp_gs$Source[grepl("theblaze.com", harm_final_temp_gs$URL)] = "The Blaze"
harm_final_temp_gs$Source[grepl("hannity.com", harm_final_temp_gs$URL)] = "Sean Hannity"
harm_model_gs = lme(MFDweight ~ lean, 
                 data = harm_final_temp_gs, 
                 method = "ML",
                 na.action = "na.omit",
                 random = list(~1|Source))
summary(harm_model_gs)
```

```{r analysis_fair_gs, include = FALSE}
#final_data clean up
#multiply by frequency and create percentage
fair_final_data_gs$weight = fair_final_data_gs$Freq * fair_final_data_gs$valence / fair_final_data_gs$wordcount * 100

#flatten down to sum by article 
fair_final_temp_gs = aggregate(fair_final_data_gs$weight, 
          by = list(fair_final_data_gs$URL),
          FUN = sum, na.rm = T)
colnames(fair_final_temp_gs) = c("URL", "MFDweight")

fair_final_temp_gs$lean[grepl("nytimes.com", fair_final_temp_gs$URL) |
                       grepl("npr.org", fair_final_temp_gs$URL) |
                        grepl("slate.com", fair_final_temp_gs$URL) |
                        grepl("huffingtonpost.com", fair_final_temp_gs$URL) |
                        grepl("politico.com", fair_final_temp_gs$URL)] = "Liberal"
                              
fair_final_temp_gs$lean[grepl("breitbart.com", fair_final_temp_gs$URL) |
                       grepl("foxnews.com", fair_final_temp_gs$URL) |
                       grepl("rushlimbaugh.com", fair_final_temp_gs$URL) |
                       grepl("theblaze.com", fair_final_temp_gs$URL) |
                       grepl("hannity.com", fair_final_temp_gs$URL)] = "Conservative"

fair_final_temp_gs$Source[grepl("nytimes.com", fair_final_temp_gs$URL)] = "NY Times"
fair_final_temp_gs$Source[grepl("npr.org", fair_final_temp_gs$URL)] = "NPR"
fair_final_temp_gs$Source[grepl("slate.com", fair_final_temp_gs$URL)] = "Slate"
fair_final_temp_gs$Source[grepl("huffingtonpost.com", fair_final_temp_gs$URL)] = "Huffington Post"
fair_final_temp_gs$Source[grepl("politico.com", fair_final_temp_gs$URL)] = "Politico"
fair_final_temp_gs$Source[grepl("foxnews.com", fair_final_temp_gs$URL)] = "Fox"
fair_final_temp_gs$Source[grepl("breitbart.com", fair_final_temp_gs$URL)] = "Breitbart"
fair_final_temp_gs$Source[grepl("rushlimbaugh.com", fair_final_temp_gs$URL)] = "Rush Limbaugh"
fair_final_temp_gs$Source[grepl("theblaze.com", fair_final_temp_gs$URL)] = "The Blaze"
fair_final_temp_gs$Source[grepl("hannity.com", fair_final_temp_gs$URL)] = "Sean Hannity"
fair_model_gs = lme(MFDweight ~ lean, 
                 data = fair_final_temp_gs, 
                 method = "ML",
                 na.action = "na.omit",
                 random = list(~1|Source))
summary(fair_model_gs)
```

```{r analysis_ingroup_gs, include = FALSE}
#final_data clean up
#multiply by frequency and create percentage
ingroup_final_data_gs$weight = ingroup_final_data_gs$Freq * ingroup_final_data_gs$valence / ingroup_final_data_gs$wordcount * 100

#flatten down to sum by article 
ingroup_final_temp_gs = aggregate(ingroup_final_data_gs$weight, 
          by = list(ingroup_final_data_gs$URL),
          FUN = sum, na.rm = T)
colnames(ingroup_final_temp_gs) = c("URL", "MFDweight")

ingroup_final_temp_gs$lean[grepl("nytimes.com", ingroup_final_temp_gs$URL) |
                       grepl("npr.org", ingroup_final_temp_gs$URL) |
                        grepl("slate.com", ingroup_final_temp_gs$URL) |
                        grepl("huffingtonpost.com", ingroup_final_temp_gs$URL) |
                        grepl("politico.com", ingroup_final_temp_gs$URL)] = "Liberal"
                              
ingroup_final_temp_gs$lean[grepl("breitbart.com", ingroup_final_temp_gs$URL) |
                       grepl("foxnews.com", ingroup_final_temp_gs$URL) |
                       grepl("rushlimbaugh.com", ingroup_final_temp_gs$URL) |
                       grepl("theblaze.com", ingroup_final_temp_gs$URL) |
                       grepl("hannity.com", ingroup_final_temp_gs$URL)] = "Conservative"

ingroup_final_temp_gs$Source[grepl("nytimes.com", ingroup_final_temp_gs$URL)] = "NY Times"
ingroup_final_temp_gs$Source[grepl("npr.org", ingroup_final_temp_gs$URL)] = "NPR"
ingroup_final_temp_gs$Source[grepl("slate.com", ingroup_final_temp_gs$URL)] = "Slate"
ingroup_final_temp_gs$Source[grepl("huffingtonpost.com", ingroup_final_temp_gs$URL)] = "Huffington Post"
ingroup_final_temp_gs$Source[grepl("politico.com", ingroup_final_temp_gs$URL)] = "Politico"
ingroup_final_temp_gs$Source[grepl("foxnews.com", ingroup_final_temp_gs$URL)] = "Fox"
ingroup_final_temp_gs$Source[grepl("breitbart.com", ingroup_final_temp_gs$URL)] = "Breitbart"
ingroup_final_temp_gs$Source[grepl("rushlimbaugh.com", ingroup_final_temp_gs$URL)] = "Rush Limbaugh"
ingroup_final_temp_gs$Source[grepl("theblaze.com", ingroup_final_temp_gs$URL)] = "The Blaze"
ingroup_final_temp_gs$Source[grepl("hannity.com", ingroup_final_temp_gs$URL)] = "Sean Hannity"
ingroup_model_gs = lme(MFDweight ~ lean, 
                 data = ingroup_final_temp_gs, 
                 method = "ML",
                 na.action = "na.omit",
                 random = list(~1|Source))
summary(ingroup_model_gs)
```

```{r analysis_authority_gs, include = FALSE}
#final_data clean up
#multiply by frequency and create percentage
authority_final_data_gs$weight = authority_final_data_gs$Freq * authority_final_data_gs$valence / authority_final_data_gs$wordcount * 100

#flatten down to sum by article 
authority_final_temp_gs = aggregate(authority_final_data_gs$weight, 
          by = list(authority_final_data_gs$URL),
          FUN = sum, na.rm = T)
colnames(authority_final_temp_gs) = c("URL", "MFDweight")

authority_final_temp_gs$lean[grepl("nytimes.com", authority_final_temp_gs$URL) |
                       grepl("npr.org", authority_final_temp_gs$URL) |
                        grepl("slate.com", authority_final_temp_gs$URL) |
                        grepl("huffingtonpost.com", authority_final_temp_gs$URL) |
                        grepl("politico.com", authority_final_temp_gs$URL)] = "Liberal"
                              
authority_final_temp_gs$lean[grepl("breitbart.com", authority_final_temp_gs$URL) |
                       grepl("foxnews.com", authority_final_temp_gs$URL) |
                       grepl("rushlimbaugh.com", authority_final_temp_gs$URL) |
                       grepl("theblaze.com", authority_final_temp_gs$URL) |
                       grepl("hannity.com", authority_final_temp_gs$URL)] = "Conservative"

authority_final_temp_gs$Source[grepl("nytimes.com", authority_final_temp_gs$URL)] = "NY Times"
authority_final_temp_gs$Source[grepl("npr.org", authority_final_temp_gs$URL)] = "NPR"
authority_final_temp_gs$Source[grepl("slate.com", authority_final_temp_gs$URL)] = "Slate"
authority_final_temp_gs$Source[grepl("huffingtonpost.com", authority_final_temp_gs$URL)] = "Huffington Post"
authority_final_temp_gs$Source[grepl("politico.com", authority_final_temp_gs$URL)] = "Politico"
authority_final_temp_gs$Source[grepl("foxnews.com", authority_final_temp_gs$URL)] = "Fox"
authority_final_temp_gs$Source[grepl("breitbart.com", authority_final_temp_gs$URL)] = "Breitbart"
authority_final_temp_gs$Source[grepl("rushlimbaugh.com", authority_final_temp_gs$URL)] = "Rush Limbaugh"
authority_final_temp_gs$Source[grepl("theblaze.com", authority_final_temp_gs$URL)] = "The Blaze"
authority_final_temp_gs$Source[grepl("hannity.com", authority_final_temp_gs$URL)] = "Sean Hannity"
authority_model_gs = lme(MFDweight ~ lean, 
                 data = authority_final_temp_gs, 
                 method = "ML",
                 na.action = "na.omit",
                 random = list(~1|Source))
summary(authority_model_gs)
```

```{r analysis_purity_gs, include = FALSE}
#final_data clean up
#multiply by frequency and create percentage
purity_final_data_gs$weight = purity_final_data_gs$Freq * purity_final_data_gs$valence / purity_final_data_gs$wordcount * 100

#flatten down to sum by article 
purity_final_temp_gs = aggregate(purity_final_data_gs$weight, 
          by = list(purity_final_data_gs$URL),
          FUN = sum, na.rm = T)
colnames(purity_final_temp_gs) = c("URL", "MFDweight")

purity_final_temp_gs$lean[grepl("nytimes.com", purity_final_temp_gs$URL) |
                       grepl("npr.org", purity_final_temp_gs$URL) |
                        grepl("slate.com", purity_final_temp_gs$URL) |
                        grepl("huffingtonpost.com", purity_final_temp_gs$URL) |
                        grepl("politico.com", purity_final_temp_gs$URL)] = "Liberal"
                              
purity_final_temp_gs$lean[grepl("breitbart.com", purity_final_temp_gs$URL) |
                       grepl("foxnews.com", purity_final_temp_gs$URL) |
                       grepl("rushlimbaugh.com", purity_final_temp_gs$URL) |
                       grepl("theblaze.com", purity_final_temp_gs$URL) |
                       grepl("hannity.com", purity_final_temp_gs$URL)] = "Conservative"

purity_final_temp_gs$Source[grepl("nytimes.com", purity_final_temp_gs$URL)] = "NY Times"
purity_final_temp_gs$Source[grepl("npr.org", purity_final_temp_gs$URL)] = "NPR"
purity_final_temp_gs$Source[grepl("slate.com", purity_final_temp_gs$URL)] = "Slate"
purity_final_temp_gs$Source[grepl("huffingtonpost.com", purity_final_temp_gs$URL)] = "Huffington Post"
purity_final_temp_gs$Source[grepl("politico.com", purity_final_temp_gs$URL)] = "Politico"
purity_final_temp_gs$Source[grepl("foxnews.com", purity_final_temp_gs$URL)] = "Fox"
purity_final_temp_gs$Source[grepl("breitbart.com", purity_final_temp_gs$URL)] = "Breitbart"
purity_final_temp_gs$Source[grepl("rushlimbaugh.com", purity_final_temp_gs$URL)] = "Rush Limbaugh"
purity_final_temp_gs$Source[grepl("theblaze.com", purity_final_temp_gs$URL)] = "The Blaze"
purity_final_temp_gs$Source[grepl("hannity.com", purity_final_temp_gs$URL)] = "Sean Hannity"
purity_model_gs = lme(MFDweight ~ lean, 
                 data = purity_final_temp_gs, 
                 method = "ML",
                 na.action = "na.omit",
                 random = list(~1|Source))
summary(purity_model_gs)
```

# Results

## Descriptive Statistics

The researchers calculated descriptive statistics for each news source per topic in order to reveal the presence (if any) of linguistic differences in the sources' use of language. As in Experiment 1, statistics calculated include *z*-scored valence, number of articles per source, total words per source, mean tokens per article in each source, mean types per article in each source, and mean readability level (using the Flesch-Kincaid Grade Level Readability formula) per source [@Kincaid1975].

Table \@ref(tab:exp2-source-descriptives-kav) displays the descriptive statistics for sources' writing on the Kavanaugh confirmation hearing. The sources were similar in most basic linguistic aspects, except for number of articles. For example, *Sean Hannity* appears to have published only 27 articles while *Breitbart* published 757 articles on this topic. Valence was found to be slightly positive across all sources. *Fox News* produced the most total words with the most tokens on average. This is likely due to the fact *Fox News* transcribes many of their videos and publishes them in article form. *Politico* featured the highest number of types on average. *Rush Limbaugh* featured the lowest readability score on average by grade level while *Slate* featured the highest grade-level readability score. The large standard deviations for these statistics, however, preclude conclusions regarding differences in the sources' use of language, as there is likely a lot of overlap between sources' use of language. 

Table \@ref(tab:exp2-source-descriptives-gs) displays descriptive statistics for articles about the partial government shutdown. Like the Kavanaugh hearing, the sources were similar in average valence (slightly positive). Once again, there was variation in the number of articles published by each source on this topic. *Sean Hannity*, *Rush Limbaugh*, and *The Blaze* published fewer than 100 articles while *Fox News* published 1,013 articles. *Fox News* again featured the most total words and mean tokens, but this is likely due to to the presence of a large amount of video transcriptions that the organization published as articles. *Politico* had the most types on average. For this topic, *Fox News* featured the lowest reading grade level while *Slate* featured the highest reading grade level. For each statistic, the excessively high standard deviations render any assertions regarding linguistic differences inconclusive on a descriptive level due to the aforementioned overlap in sources' language use. 


```{r exp2-source-descriptives-kav, results = 'asis', echo = FALSE, message = F}
#Kav
final_kav$uniquewords = NA
final_kav$valenceavg = NA

for (i in 1:nrow(final_kav)){
  
  #add unique
  final_kav$uniquewords[i] = length(unique(unlist(strsplit(final_kav$stemmed[i], " "))))
  
  #add valence
  tempwords_kav = as.data.frame(unlist(strsplit(final_kav$stemmed[i], " ")))
  colnames(tempwords_kav) = "theword"
  tempwords_kav$tempval = vlookup(tempwords_kav$theword, #index in original data
                    warriner, #where's the stuff you want to put in
                    "V.Mean.Sum", #what column is the stuff you want
                    "Word2") #what's the matching index
  
  final_kav$valenceavg[i] = mean(tempwords_kav$tempval, na.rm = T)

}


# tapply(final_kav$valenceavg, final_kav$Source, mean)
# tapply(final_kav$valenceavg, final_kav$Source, sd)
# tapply(final_kav$wordcount, final_kav$Source, mean)
# tapply(final_kav$wordcount, final_kav$Source, sd)
# tapply(final_kav$uniquewords, final_kav$Source, mean)
# tapply(final_kav$uniquewords, final_kav$Source, sd)
# tapply(final_kav$Source, final_kav$Source, length)
# tapply(final_kav$wordcount, final_kav$Source, sum)

library(quanteda)

final_kav$FK = NA

for (i in 1:nrow(final_kav)){
  final_kav$FK[i] = unlist(textstat_readability(final_kav$Text[i], 
                     measure = "Flesch.Kincaid")$Flesch.Kincaid)
}

# tapply(final_kav$FK, final_kav$Source, mean)
# tapply(final_kav$FK, final_kav$Source, sd)

#Kav Table
####build the table here####
tableprint = matrix(NA, nrow=10, ncol=11)
colnames(tableprint) = c("Source", "Mean_Valence", "SD_Valence", "Number_of_Articles", "Total_Words", "Mean_Tokens", "SD_Tokens", "Mean_Types", "SD_Types", "Mean_Readability", "SD_Readability")

#tableprint[ , 1] = unique(final_kav$Source)
tableprint[ , 1] = c("The Blaze", "Breitbart", "Fox News", "Sean Hannity", 
                     "Huffington Post", "NPR", "New York Times", "Politico",
                     "Rush Limbaugh", "Slate")
tableprint[ , 2] = tapply(final_kav$valenceavg, final_kav$Source, mean, na.rm = T)
tableprint[ , 3] = tapply(final_kav$valenceavg, final_kav$Source, sd, na.rm = T)
tableprint[ , 4] = tapply(final_kav$valenceavg, final_kav$Source, length)
tableprint[ , 5] = tapply(final_kav$wordcount, final_kav$Source, sum, na.rm = T)
tableprint[ , 6] = tapply(final_kav$wordcount, final_kav$Source, mean, na.rm = T)
tableprint[ , 7] = tapply(final_kav$wordcount, final_kav$Source, sd)
tableprint[ , 8] = tapply(final_kav$uniquewords, final_kav$Source, mean, na.rm = T)
tableprint[ , 9] = tapply(final_kav$uniquewords, final_kav$Source, sd, na.rm = T)
tableprint[ , 10] = tapply(final_kav$FK, final_kav$Source, mean, na.rm = T)
tableprint[ , 11] = tapply(final_kav$FK, final_kav$Source, sd, na.rm = T)

tableprint[ , c(2,3,6,7,8,9,10,11)] = printnum(as.numeric(tableprint[ , c(2,3,6,7,8,9,10,11)]))

tableprint = tableprint[c(5, 6, 7, 8, 10, 1, 2, 3, 4, 9), ]

#change this to match
apa_table(as.data.frame(tableprint),
          note = "Readability statistics were calculated using the Flesch-Kincaid Grade Level readability formula. V = Valence, T = Tokens or total words, Ty = Types or unique words, FK = Flesch-Kincaid",
          caption = "Kavanaugh - Descriptive Statistics by Source", 
          col.names = c("Source", "$M_V$", "$SD_V$", "$N_{Article}$", "$N_{Words}$", "$M_T$", "$SD_T$", "$M_{Ty}$", "$SD_{Ty}$", "$M_{FK}$", "$SD_{FK}$"),
          format = "latex",
          escape = F,
          small = T,
          align = c("l", rep("c", 6))
          )
```

```{r exp2-source-descriptives-gs, results = 'asis', echo = FALSE, message = F}
#GS
final_gs$uniquewords = NA
final_gs$valenceavg = NA

for (i in 1:nrow(final_gs)){
  
  #add unique
  final_gs$uniquewords[i] = length(unique(unlist(strsplit(final_gs$stemmed[i], " "))))
  
  #add valence
  tempwords_gs = as.data.frame(unlist(strsplit(final_gs$stemmed[i], " ")))
  colnames(tempwords_gs) = "theword"
  tempwords_gs$tempval = vlookup(tempwords_gs$theword, #index in original data
                    warriner, #where's the stuff you want to put in
                    "V.Mean.Sum", #what column is the stuff you want
                    "Word2") #what's the matching index
  
  final_gs$valenceavg[i] = mean(tempwords_gs$tempval, na.rm = T)

}


# tapply(final_gs$valenceavg, final_gs$Source, mean)
# tapply(final_gs$valenceavg, final_gs$Source, sd)
# tapply(final_gs$wordcount, final_gs$Source, mean)
# tapply(final_gs$wordcount, final_gs$Source, sd)
# tapply(final_gs$uniquewords, final_gs$Source, mean)
# tapply(final_gs$uniquewords, final_gs$Source, sd)
# tapply(final_gs$Source, final_gs$Source, length)
# tapply(final_gs$wordcount, final_gs$Source, sum)

library(quanteda)

final_gs$FK = NA

for (i in 1:nrow(final_gs)){
  final_gs$FK[i] = unlist(textstat_readability(final_gs$Text[i], 
                     measure = "Flesch.Kincaid")$Flesch.Kincaid)
}

# tapply(final_gs$FK, final_gs$Source, mean)
# tapply(final_gs$FK, final_gs$Source, sd)

#GS Table
####build the table here####
tableprint = matrix(NA, nrow=10, ncol=11)
colnames(tableprint) = c("Source", "Mean_Valence", "SD_Valence", "Number_of_Articles", "Total_Words", "Mean_Tokens", "SD_Tokens", "Mean_Types", "SD_Types", "Mean_Readability", "SD_Readability")

#tableprint[ , 1] = unique(final_gs$Source)
tableprint[ , 1] = c("The Blaze", "Breitbart", "Fox News", "Sean Hannity", 
                     "Huffington Post", "NPR", "New York Times", "Politico",
                     "Rush Limbaugh", "Slate")
tableprint[ , 2] = tapply(final_gs$valenceavg, final_gs$Source, mean)
tableprint[ , 3] = tapply(final_gs$valenceavg, final_gs$Source, sd)
tableprint[ , 4] = tapply(final_gs$valenceavg, final_gs$Source, length)
tableprint[ , 5] = tapply(final_gs$wordcount, final_gs$Source, sum)
tableprint[ , 6] = tapply(final_gs$wordcount, final_gs$Source, mean)
tableprint[ , 7] = tapply(final_gs$wordcount, final_gs$Source, sd)
tableprint[ , 8] = tapply(final_gs$uniquewords, final_gs$Source, mean)
tableprint[ , 9] = tapply(final_gs$uniquewords, final_gs$Source, sd)
tableprint[ , 10] = tapply(final_gs$FK, final_gs$Source, mean)
tableprint[ , 11] = tapply(final_gs$FK, final_gs$Source, sd)

tableprint[ , c(2,3,6,7,8,9,10,11)] = printnum(as.numeric(tableprint[ , c(2,3,6,7,8,9,10,11)]))

tableprint = tableprint[c(5, 6, 7, 8, 10, 1, 2, 3, 4, 9), ]

#change this to match
apa_table(as.data.frame(tableprint),
          note = "Readability statistics were calculated using the Flesch-Kincaid Grade Level readability formula. V = Valence, T = Tokens or total words, Ty = Types or unique words, FK = Flesch-Kincaid",
          caption = "Government Shutdown - Descriptive Statistics by Source", 
          col.names = c("Source", "$M_V$", "$SD_V$", "$N_{Article}$", "$N_{Words}$", "$M_T$", "$SD_T$", "$M_{Ty}$", "$SD_{Ty}$", "$M_{FK}$", "$SD_{FK}$"),
          format = "latex",
          escape = F,
          small = T, 
          align = c("l", rep("c", 6))
          )
```

## Inferential Statistics

To analyze if news sources adhered to differences in word use based on their target audience, the researchers utilized a multilevel model (MLM) to analyze if news sources leveraged different vocabularies based on target audience. The researchers used the *nlme* library in *R* [@Pinheiro2017]. Each foundation's weighted percentage was predicted by the source's political lean, using the individual source as a random intercept to control for the nested data structure. Two separate MLMs were constructed from datasets compiled for each topic of interest: the Kavanaugh hearing and the partial government shutdown of 2018-2019.

For the Kavanaugh topic, the multilevel model indicated the presence of a statistically significant effect for *harm/care*, but the practical effect denoted by Cohen's *d* was found to be small. There were no other significant or practical effects of political lean for any of the other four moral foundations. The effect for *harm/care* was in the hypothesized direction with liberal sources tending to use more positively *harm/care* words than conservative sources. Descriptive and test statistics, *p*-values and effect sizes (Cohen's *d*) can be found in Table \@ref(tab:exp2-tablekav). 

For news articles about the partial U.S. Federal Government Shutdown of 2018-2019, there were no significant or practical effects of political lean for the moral foundations. A small-to-medium effect size was observed for *authority/respect*. The effect was in the predicted direction as well, as conservative sources tended to offer more positive endorsements of the foundation than liberal sources. This effect was similar to Experiment 1 in which the largest effect size was observed for *authority/respect*. As noted before, the effect found in Experiment 1 was in the opposite direction as what was hypothesized. Thus, it is difficult to draw comparisons between the two studies despite the similar pattern for effect size. Owing to a lack of similar effects for either Experiment 1 or the Kavanaugh topic, there is doubt as to whether or not a practical or generalized effect exists for *authority/respect*. 

Based on the weighted percent values for the five foundations applied to both topics, MFD words seem to make up little of the article text. A similar pattern was observed for the results in Experiment 1. As in Experiment 1, the percentages and means seem to indicate a generally positive endorsement of all five moral foundations across both political leanings.

```{r exp2-tablekav, results = 'asis', echo = F}
#MFD, mean, sd, mean, sd, t, p, d
tableprint_kav = matrix(NA, ncol = 8, nrow = 5)
colnames(tableprint_kav) = c("Foundation", "M_C", "SD_C", "M_L", "SD_L", "t", "p", "d")

#calculate d
#harm
harm_d_kav = d.ind.t(m1 = mean(harm_final_temp_kav$MFDweight[harm_final_temp_kav$lean == "Conservative"], na.rm = T), 
                 m2 = mean(harm_final_temp_kav$MFDweight[harm_final_temp_kav$lean == "Liberal"], na.rm = T),
                 sd1 = sd(harm_final_temp_kav$MFDweight[harm_final_temp_kav$lean == "Conservative"], na.rm = T),
                 sd2 = sd(harm_final_temp_kav$MFDweight[harm_final_temp_kav$lean == "Liberal"], na.rm = T),
                 n1 = length(harm_final_temp_kav$MFDweight[harm_final_temp_kav$lean == "Conservative"]),
                 n2 = length(harm_final_temp_kav$MFDweight[harm_final_temp_kav$lean == "Liberal"]),
                 a = .05)

#fair
fair_d_kav = d.ind.t(m1 = mean(fair_final_temp_kav$MFDweight[fair_final_temp_kav$lean == "Conservative"], na.rm = T), 
                 m2 = mean(fair_final_temp_kav$MFDweight[fair_final_temp_kav$lean == "Liberal"], na.rm = T),
                 sd1 = sd(fair_final_temp_kav$MFDweight[fair_final_temp_kav$lean == "Conservative"], na.rm = T),
                 sd2 = sd(fair_final_temp_kav$MFDweight[fair_final_temp_kav$lean == "Liberal"], na.rm = T),
                 n1 = length(fair_final_temp_kav$MFDweight[fair_final_temp_kav$lean == "Conservative"]),
                 n2 = length(fair_final_temp_kav$MFDweight[fair_final_temp_kav$lean == "Liberal"]),
                 a = .05)

#ingroup
ingroup_d_kav = d.ind.t(m1 = mean(ingroup_final_temp_kav$MFDweight[ingroup_final_temp_kav$lean == "Conservative"], na.rm = T), 
                 m2 = mean(ingroup_final_temp_kav$MFDweight[ingroup_final_temp_kav$lean == "Liberal"], na.rm = T),
                 sd1 = sd(ingroup_final_temp_kav$MFDweight[ingroup_final_temp_kav$lean == "Conservative"], na.rm = T),
                 sd2 = sd(ingroup_final_temp_kav$MFDweight[ingroup_final_temp_kav$lean == "Liberal"], na.rm = T),
                 n1 = length(ingroup_final_temp_kav$MFDweight[ingroup_final_temp_kav$lean == "Conservative"]),
                 n2 = length(ingroup_final_temp_kav$MFDweight[ingroup_final_temp_kav$lean == "Liberal"]),
                 a = .05)

#authority
authority_d_kav = d.ind.t(m1 = mean(authority_final_temp_kav$MFDweight[authority_final_temp_kav$lean == "Conservative"], na.rm = T), 
                 m2 = mean(authority_final_temp_kav$MFDweight[authority_final_temp_kav$lean == "Liberal"], na.rm = T),
                 sd1 = sd(authority_final_temp_kav$MFDweight[authority_final_temp_kav$lean == "Conservative"], na.rm = T),
                 sd2 = sd(authority_final_temp_kav$MFDweight[authority_final_temp_kav$lean == "Liberal"], na.rm = T),
                 n1 = length(authority_final_temp_kav$MFDweight[authority_final_temp_kav$lean == "Conservative"]),
                 n2 = length(authority_final_temp_kav$MFDweight[authority_final_temp_kav$lean == "Liberal"]),
                 a = .05)

#purity
purity_d_kav = d.ind.t(m1 = mean(purity_final_temp_kav$MFDweight[purity_final_temp_kav$lean == "Conservative"], na.rm = T), 
                 m2 = mean(purity_final_temp_kav$MFDweight[purity_final_temp_kav$lean == "Liberal"], na.rm = T),
                 sd1 = sd(purity_final_temp_kav$MFDweight[purity_final_temp_kav$lean == "Conservative"], na.rm = T),
                 sd2 = sd(purity_final_temp_kav$MFDweight[purity_final_temp_kav$lean == "Liberal"], na.rm = T),
                 n1 = length(purity_final_temp_kav$MFDweight[purity_final_temp_kav$lean == "Conservative"]),
                 n2 = length(purity_final_temp_kav$MFDweight[purity_final_temp_kav$lean == "Liberal"]),
                 a = .05)

#put in the numbers
tableprint_kav[1, ] = c("Harm/Care", harm_d_kav$M1, harm_d_kav$sd1, harm_d_kav$M2, harm_d_kav$sd1, 
                    summary(harm_model_kav)$tTable[2,4], summary(harm_model_kav)$tTable[2,5],
                    harm_d_kav$d)
tableprint_kav[2, ] = c("Fairness/Reciprocity", fair_d_kav$M1, fair_d_kav$sd1, fair_d_kav$M2, fair_d_kav$sd1, 
                    summary(fair_model_kav)$tTable[2,4], summary(fair_model_kav)$tTable[2,5],
                    fair_d_kav$d)
tableprint_kav[3, ] = c("Ingroup/Loyalty", ingroup_d_kav$M1, ingroup_d_kav$sd1, ingroup_d_kav$M2, ingroup_d_kav$sd1, 
                    summary(ingroup_model_kav)$tTable[2,4], summary(ingroup_model_kav)$tTable[2,5],
                    ingroup_d_kav$d)
tableprint_kav[4, ] = c("Authority/Respect", authority_d_kav$M1, authority_d_kav$sd1, authority_d_kav$M2, authority_d_kav$sd1, 
                    summary(authority_model_kav)$tTable[2,4], summary(authority_model_kav)$tTable[2,5],
                    authority_d_kav$d)
tableprint_kav[5, ] = c("Purity/Sanctity", purity_d_kav$M1, purity_d_kav$sd1, purity_d_kav$M2, purity_d_kav$sd1, 
                    summary(purity_model_kav)$tTable[2,4], summary(purity_model_kav)$tTable[2,5],
                    purity_d_kav$d)

tableprint_kav[ , c(2:6, 8)] = printnum(as.numeric(tableprint_kav[ , c(2:6, 8)]))
tableprint_kav[ , 7] = printp(as.numeric(tableprint_kav[ , 7]))

apa_table(tableprint_kav,
          note = "For mean and standard deviation values, 'C' and 'L' refer to 'conservative' and 'liberal,' respectively",
          caption = "Kavanaugh Results - Multilevel Model", 
          col.names = c("Foundation", "$M_C$", "$SD_C$", "$M_L$", "$SD_L$", "$t$", "$p$", "$d$"),
          format = "latex",
          escape = F,
          align = c("l", rep("c", 7))
          )
```

```{r exp2-tablegs, results = 'asis', echo = F}
#MFD, mean, sd, mean, sd, t, p, d
tableprint_gs = matrix(NA, ncol = 8, nrow = 5)
colnames(tableprint_gs) = c("Foundation", "M_C", "SD_C", "M_L", "SD_L", "t", "p", "d")

#calculate d
#harm
harm_d_gs = d.ind.t(m1 = mean(harm_final_temp_gs$MFDweight[harm_final_temp_gs$lean == "Conservative"], na.rm = T), 
                 m2 = mean(harm_final_temp_gs$MFDweight[harm_final_temp_gs$lean == "Liberal"], na.rm = T),
                 sd1 = sd(harm_final_temp_gs$MFDweight[harm_final_temp_gs$lean == "Conservative"], na.rm = T),
                 sd2 = sd(harm_final_temp_gs$MFDweight[harm_final_temp_gs$lean == "Liberal"], na.rm = T),
                 n1 = length(harm_final_temp_gs$MFDweight[harm_final_temp_gs$lean == "Conservative"]),
                 n2 = length(harm_final_temp_gs$MFDweight[harm_final_temp_gs$lean == "Liberal"]),
                 a = .05)

#fair
fair_d_gs = d.ind.t(m1 = mean(fair_final_temp_gs$MFDweight[fair_final_temp_gs$lean == "Conservative"], na.rm = T), 
                 m2 = mean(fair_final_temp_gs$MFDweight[fair_final_temp_gs$lean == "Liberal"], na.rm = T),
                 sd1 = sd(fair_final_temp_gs$MFDweight[fair_final_temp_gs$lean == "Conservative"], na.rm = T),
                 sd2 = sd(fair_final_temp_gs$MFDweight[fair_final_temp_gs$lean == "Liberal"], na.rm = T),
                 n1 = length(fair_final_temp_gs$MFDweight[fair_final_temp_gs$lean == "Conservative"]),
                 n2 = length(fair_final_temp_gs$MFDweight[fair_final_temp_gs$lean == "Liberal"]),
                 a = .05)

#ingroup
ingroup_d_gs = d.ind.t(m1 = mean(ingroup_final_temp_gs$MFDweight[ingroup_final_temp_gs$lean == "Conservative"], na.rm = T), 
                 m2 = mean(ingroup_final_temp_gs$MFDweight[ingroup_final_temp_gs$lean == "Liberal"], na.rm = T),
                 sd1 = sd(ingroup_final_temp_gs$MFDweight[ingroup_final_temp_gs$lean == "Conservative"], na.rm = T),
                 sd2 = sd(ingroup_final_temp_gs$MFDweight[ingroup_final_temp_gs$lean == "Liberal"], na.rm = T),
                 n1 = length(ingroup_final_temp_gs$MFDweight[ingroup_final_temp_gs$lean == "Conservative"]),
                 n2 = length(ingroup_final_temp_gs$MFDweight[ingroup_final_temp_gs$lean == "Liberal"]),
                 a = .05)

#authority
authority_d_gs = d.ind.t(m1 = mean(authority_final_temp_gs$MFDweight[authority_final_temp_gs$lean == "Conservative"], na.rm = T), 
                 m2 = mean(authority_final_temp_gs$MFDweight[authority_final_temp_gs$lean == "Liberal"], na.rm = T),
                 sd1 = sd(authority_final_temp_gs$MFDweight[authority_final_temp_gs$lean == "Conservative"], na.rm = T),
                 sd2 = sd(authority_final_temp_gs$MFDweight[authority_final_temp_gs$lean == "Liberal"], na.rm = T),
                 n1 = length(authority_final_temp_gs$MFDweight[authority_final_temp_gs$lean == "Conservative"]),
                 n2 = length(authority_final_temp_gs$MFDweight[authority_final_temp_gs$lean == "Liberal"]),
                 a = .05)

#purity
purity_d_gs = d.ind.t(m1 = mean(purity_final_temp_gs$MFDweight[purity_final_temp_gs$lean == "Conservative"], na.rm = T), 
                 m2 = mean(purity_final_temp_gs$MFDweight[purity_final_temp_gs$lean == "Liberal"], na.rm = T),
                 sd1 = sd(purity_final_temp_gs$MFDweight[purity_final_temp_gs$lean == "Conservative"], na.rm = T),
                 sd2 = sd(purity_final_temp_gs$MFDweight[purity_final_temp_gs$lean == "Liberal"], na.rm = T),
                 n1 = length(purity_final_temp_gs$MFDweight[purity_final_temp_gs$lean == "Conservative"]),
                 n2 = length(purity_final_temp_gs$MFDweight[purity_final_temp_gs$lean == "Liberal"]),
                 a = .05)

#put in the numbers
tableprint_gs[1, ] = c("Harm/Care", harm_d_gs$M1, harm_d_gs$sd1, harm_d_gs$M2, harm_d_gs$sd1, 
                    summary(harm_model_gs)$tTable[2,4], summary(harm_model_gs)$tTable[2,5],
                    harm_d_gs$d)
tableprint_gs[2, ] = c("Fairness/Reciprocity", fair_d_gs$M1, fair_d_gs$sd1, fair_d_gs$M2, fair_d_gs$sd1, 
                    summary(fair_model_gs)$tTable[2,4], summary(fair_model_gs)$tTable[2,5],
                    fair_d_gs$d)
tableprint_gs[3, ] = c("Ingroup/Loyalty", ingroup_d_gs$M1, ingroup_d_gs$sd1, ingroup_d_gs$M2, ingroup_d_gs$sd1, 
                    summary(ingroup_model_gs)$tTable[2,4], summary(ingroup_model_gs)$tTable[2,5],
                    ingroup_d_gs$d)
tableprint_gs[4, ] = c("Authority/Respect", authority_d_gs$M1, authority_d_gs$sd1, authority_d_gs$M2, authority_d_gs$sd1, 
                    summary(authority_model_gs)$tTable[2,4], summary(authority_model_gs)$tTable[2,5],
                    authority_d_gs$d)
tableprint_gs[5, ] = c("Purity/Sanctity", purity_d_gs$M1, purity_d_gs$sd1, purity_d_gs$M2, purity_d_gs$sd1, 
                    summary(purity_model_gs)$tTable[2,4], summary(purity_model_gs)$tTable[2,5],
                    purity_d_gs$d)

tableprint_gs[ , c(2:6, 8)] = printnum(as.numeric(tableprint_gs[ , c(2:6, 8)]))
tableprint_gs[ , 7] = printp(as.numeric(tableprint_gs[ , 7]))

apa_table(tableprint_gs,
          note = "For mean and standard deviation values, 'C' and 'L' refer to 'conservative' and 'liberal,' respectively",
          caption = "Government Shutdown Results - Multilevel Model", 
          col.names = c("Foundation", "$M_C$", "$SD_C$", "$M_L$", "$SD_L$", "$t$", "$p$", "$d$"),
          format = "latex",
          escape = F,
          align = c("l", rep("c", 7))
          )
```

```{r conference-pic_kav, eval = F, include = F}
library(ggplot2)
library(reshape)

cleanup = theme(panel.grid.major = element_blank(), 
                panel.grid.minor = element_blank(), 
                panel.background = element_blank(), 
                axis.line.x = element_line(color = "black"),
                axis.line.y = element_line(color = "black"),
                legend.key = element_rect(fill = "white"),
                text = element_text(size = 10))

graphdata_kav = as.data.frame(tableprint_kav)
graphlong_kav = melt(graphdata_kav[ , c(1,2,4)], 
                 id = "Foundation")
graphlong2_kav = melt(graphdata_kav[ , c(1, 3,5)],
                  id = "Foundation",
                  measured = c("SD_C", "SD_L"))
graphlong_kav$sd = graphlong2_kav$value
graphlong_kav$lower = as.numeric(as.character(graphlong_kav$value)) - 2*as.numeric(as.character(graphlong_kav$sd))
graphlong_kav$upper = as.numeric(as.character(graphlong_kav$value)) + 2*as.numeric(as.character(graphlong_kav$sd))

graphlong_kav$value = as.numeric(as.character(graphlong_kav$value))

graph_data_kav = ggplot(graphlong_kav, aes(Foundation, value, color = variable)) + 
  geom_point(position = position_dodge(width = 0.90)) +
  geom_errorbar(aes(ymin = lower, ymax = upper), 
                position = position_dodge(width = 0.90),
                width = .2) + 
  cleanup +
  ylab("Mean Weighted Valence") + 
  scale_color_manual(values = c("red", "blue"),
                       labels = c("Conservative", "Liberal"),
                       name = "Party") +
  theme(axis.text.x = element_text(angle = 60, hjust = 1)) + 
  geom_hline(yintercept = 0, linetype = 1, color = "black")
  
#tiff(filename = "exp2_graph_kav.tiff", res = 300, width = 4, 
#     height = 3, units = 'in', compression = "lzw")
#plot(graph_data_kav)
#dev.off()
```

```{r conference-pic_gs, eval = F, include = F}
graphdata_gs = as.data.frame(tableprint_gs)
graphlong_gs = melt(graphdata_gs[ , c(1,2,4)], 
                 id = "Foundation")
graphlong2_gs = melt(graphdata_gs[ , c(1, 3,5)],
                  id = "Foundation",
                  measured = c("SD_C", "SD_L"))
graphlong_gs$sd = graphlong2_gs$value
graphlong_gs$lower = as.numeric(as.character(graphlong_gs$value)) - 2*as.numeric(as.character(graphlong_gs$sd))
graphlong_gs$upper = as.numeric(as.character(graphlong_gs$value)) + 2*as.numeric(as.character(graphlong_gs$sd))

graphlong_gs$value = as.numeric(as.character(graphlong_gs$value))

graph_data_gs = ggplot(graphlong_gs, aes(Foundation, value, color = variable)) + 
  geom_point(position = position_dodge(width = 0.90)) +
  geom_errorbar(aes(ymin = lower, ymax = upper), 
                position = position_dodge(width = 0.90),
                width = .2) + 
  cleanup +
  ylab("Mean Weighted Valence") + 
  scale_color_manual(values = c("red", "blue"),
                       labels = c("Conservative", "Liberal"),
                       name = "Party") +
  theme(axis.text.x = element_text(angle = 60, hjust = 1)) + 
  geom_hline(yintercept = 0, linetype = 1, color = "black")
  
#tiff(filename = "exp2_graph_gs.tiff", res = 300, width = 4, 
#     height = 3, units = 'in', compression = "lzw")
#plot(graph_data_gs)
#dev.off()
```

# Discussion

The results obtained in Experiment 2 did not confirm either of the hypotheses. The researchers found little compelling evidence of an effect of partisan lean on MFD endorsement. The only significant effect was found for *harm/care* in the Kavanaugh confirmation hearing dataset. While the effect was in the hypothesized direction (higher positive endorsement for liberal sources), the effect size was small. While there were no significant effects found for the government shutdown topic, a small-to-medium effect size was calculated for the *authority/respect* foundation. Once again, the effect was in the hypothesized direction with conservative sources providing higher positive endorsements of the foundation relative to liberal sources. 

Both of these effects are consistent with both the research hypotheses as well as previous findings in Moral Foundations Theory scholarship. It should be noted these effects were found in isolation, and similar patterns were not found across the other studies. These findings undermine any inferences of political bias stemming from the content of news articles. The results obtained for Experiment 2 call into question the Moral Foundations Dictionary's efficacy as a tool for establishing differences between political news sources by partisan lean.

# Conclusions 

Within the theoretical framework of Moral Foundations Theory [@Haidt2007], the researchers attempted to devise a method leveraging the Moral Foundations Dictionary [@Graham2009] in order to quantify political bias stemming from content published by several prominent American news sources. The researchers downloaded news article text from the web, processed the text, matched it with the Moral Foundations Dictionary words, and calculated average valence for words in each article. To analyze the data, the researchers constructed multi-level models for each foundation in which weighted percentages were predicted by each source's political lean. Individual source was held as a random intercept to control for correlated error. This process was undertaken once in Experiment 1 for general political news and twice in Experiment 2 using articles pertaining to two specific topics in American news: Brett Kavanaugh's confirmation to the Supreme Court and the most recent (as of this writing) partial U.S. Government shutdown from December 2018 to January 2019.

In Experiment 1, the researchers analyzed general political news articles from four prominent U.S. news sources: *The New York Times*, *NPR*, *Fox News*, and *Breitbart*. They hypothesized *The New York Times* and *NPR* would endorse moral foundations representative of liberal political views and that *Fox News* and *Breitbart* would endorse foundations indicative of conservative views. The researchers assigned a political lean (liberal or conservative) to the sources based upon consumer preferences established by @Mitchell2014. After collecting the articles through web scraping, processing the article text, calculating percent occurrence of MFD words, and weighting that value by valence, the researchers constructed the MLM. The results obtained were not significant in any statistical or practical sense. There was a small effect found for the *authority/respect* foundation according to Cohen's *d*, but the effect suggested endorsement for that foundation opposite to what was hypothesized. The researchers viewed these results as an opportunity to alter their method for use in Experiment 2. 

In Experiment 2, the researchers adjusted their method in order to increase their potential ability to quantify political leanings in U.S. news sources. First, the researchers expanded the number of news sources from which articles were collected. They added *Slate*, *Politico*, and *Huffington Post* to the list of liberal sources and *The Blaze*, *Rush Limbaugh*, and *Sean Hannity* to the conservative sources. Once again, political bias groups were assigned based on @Mitchell2014's findings. Second, they elected to collect articles and compile datasets pertaining to two specific topics in U.S. news: the Kavanaugh hearing and the government shutdown. These topics were chosen due to their relevance to multiple moral foundations. Other than those two alterations, the same method (scraping, processing, weighting, and multi-level modelling) was employed for Experiment 2.

For articles pertaining to the Kavanaugh hearing, there was a statistically significant effect for *harm/care*, but the effect size was modest. No other statistically or practically significant effects were found for the Kavanaugh topic. Likewise, there were no statistically or practically significant effects revealed for the government shutdown topic. However, there was a small-to-medium effect size for *authority/respect*. The fact these patterns do not appear in any of the other studies undertaken herein decreases the researchers' ability to draw any valuable conclusions regarding the MFD's ability to reveal political bias in American news media.

The results obtained in both experiments, while not confirmatory, provide several avenues for future investigation. Twice confronted with the failure to uncover differences in news media's political content, the researchers were unable to confirm the existence of an effect of political lean on American news writing under the auspices of Moral Foundations Theory. This does not, however, rule out the possibility that news media is biased. As @Mitchell2014 has demonstrated, self-reported political partisans are drawn to separate sources, but the present research was unable to offer any content-based explanations of this phenomenon. Even when attenuating the scraping criteria to collect text concerning (what the researchers believed to be) morally resonant topics, the method failed to detect differences in MFD word use by prominent news sources. Given part of the researchers' goal with this study was to answer a method question, they must contemplate whether or not the method or the construct of interest (political lean) are the primary contributor to null results. Either the method used to detect lean was insufficent for this purpose, or political lean simply does not exist, indicating the presence of a true negative in the results.

The researchers believe these results represent a false negative rather than a true negative. That is, we believe political lean exists, but that we have been unable to detect it at a content level. The researchers again turn to @Mitchell2014's work in this area: political partisans prefer certain news outlets over others. There must be something about the sources' content that facilitates partisans' repeated usage. Otherwise, they might not express such loyalty to any one source (or group of sources) over others. Given this aspect of the news economy, the researchers still believe certain news outlets in the U.S. contatin a political slant. The researchers' focus, therefore, turns toward the limitations of the method, especially the chosen theoretical framework, under which the present research was conducted.

Despite the fact the results regarding political bias were inconclusive, the researchers still retain confidence in the overall structure of the methodology established in the current study. Specifically, the procedure for scraping text from the web, processing, stemming, and weighting the scores with valence seems to represent a solid method for preparing a high quantity of text passages for data analysis. The researchers implemented valence as an indicator of the directionality of endorsement due to the inherent ambiguity of simply calculating MFD word percent occurrence. This method served both to augment the face validity of the MFD by incorporating valence (thus reducing ambiguity) and to generate a score that is easy to understand and analyze.

There remains an aspect of language which this method still cannot address: context. Owing to the presence of millions of words in the analyses presented in the current study, the researchers would be unable to check for context and correct endorsement scores accordingly. @Graham2009 actually performed a process of human error-checking for context when formulating the MFD. While this might have helped that team fix mistakes made by the LIWC program (@Pennebaker2007), manual coding of this nature has the potential to introduce an unnacceptable level of bias in the method, as perceptions of context would be dependent upon raters' interpretations and beleif systems. Additionally, the numerous ways in which context affects meaning make it difficult to interpret MFD word occurrence outside of a narrow operational definition of "endorsement." In fact, political lean may be communicated through methods other than through the use of particular words.

One method through which news organizations might appeal to political partisans is through their content strategy rather than their use of MFD words. The results in the current study demonstrate a similar pattern of endorsement across both conservative and liberal sources. Specifically, the researchers found slightly positive endorsements for all five foundations no matter the political lean of the source. While this may simply be an indication of insufficient MFD word occurrence in news articles, it may also demonstrate truly equal endorsement despite political lean. While news organizations may endorse all moral foundations, they may also adopt strategies for doing so that appeal to the political partisans consuming their product. As an example, a liberal source might endorse *purity/sanctity* through an appeal to reduce waste or pollution (i.e. keeping the earth "clean"). A conservative source, on the other hand, might endorse *purity/sanctity* through an argument in favor of abstinence or religious commitment (i.e. keeping a "clean" soul, mind, or body). Both publications would register as positive endorsement of *purity/sanctity* despite appropriating the concept for distinct purposes. Through these divergent strategies, liberal and conservative news organizations might be able to appeal to their fans while exhibiting similar levels of moral endorsement. This could represent a further weakness of the Moral Foudnations Dictionary as an instrument, as it is unable to identify these potential strategies for communicating certain points of view.   

Thus, the researchers' attention turned to the instrument used to detect differences: the Moral Foundations Dictionary. As mentioned before, MFD is a measure developed from Moral Foundations Theory [@Graham2009]. Based upon the results obtained, it might be necessary to investigate alternative instruments that could better elucidate the differences of interest. Likewise, other theoretical perspectives may be more equipped to explain political differences in discourse. 

Tools such as these could be developed through the Linguistic Inquiry and Word Count program (LIWC) [@Pennebaker2007]. As stated before, the Moral Foundations Dictionary was established through an analysis of Christian sermons performed mostly with LIWC as well as some human-enforced corrections due to context [@Graham2009]. This approach was grounded in Moral Foundations Theory, a clear framework within which the authors developed the MFD. The original authors of the MFD established sound psychometric properties for the tool, implying its validity in several applications. For this reason, the researchers elected to utilize the MFD as their primary instrument. 

Other instruments could theoretically be developed through a LIWC analysis, including those that would be useful for analyzing political discourse. Researchers could obtain percent occurrence scores for words appearing in certain LIWC categories, which could then be weighted with valence scores from @Warriner2013. The researchers will consider other applications of the LIWC in future studies aiming to analyze political content. @Pennebaker2007 has suggested the program could be applied to analyses of cognitive, social, and emotional states. Furthermore, there is some evidence LIWC can be used to detect deception, but the reliability of the program's accuracy in this regard (67%) calls this assertion into question [@Pennebaker2011]. Despite the promising extent of LIWC's abilities, without a firm theoretical foundation for such analyses, any such applications of LIWC would face serious questions as to their relevance, validity, and generalizability. Nevertheless, LIWC analyses may represent a path to further research into political differences in news media as well as theory formation.

There is, however, another theoretical framework that could prove useful when applied to the method established in the present research. Relational Frame Theory originates in applied behavior analysis, but there exists recent scholarship that demonstrates its potential usefulness in the political realm. Relational Frame Theory (RFT) holds that stimuli, including speech and discourse, elicit responses in terms of relationships to other stimuli, even when the stimuli do not otherwise appear to be related [@Blackledge2014]. For example, an individual's equivalent fear response to a venomous snake as well as a wooded area said to "contain snakes" represents "mutual entailment." That is, the fear response is an effect of both the snake and environment stimuli, which cause the fear response. "Combinatorial entailment" can be represented by a fear response elicited by a novel stimulus whose relationship to a well-known stimulus elicits a certain response [@Blackledge2014]. In the snake example, if another stimulus (Stimulus A), were said to be "worse than" the snake, the relationship between A and the snake would elicit a more severe fear response to Stimulus A. RFT utilizes a set of "families" describing the nature of these relationships. Such families include (among others) "coordinative," establishing similarities between stimuli, "distinctive," implying fundamental differences between stimuli, and "hierarchical," implying a relation in terms of differing order or rank [@Belisle2018]. These families can be used to analyze networks of relational frames in discourse. 

@Belisle2018 conducted a text analysis (using a computer algorithm) of several speeches made by four recent U.S. presidents under Relational Frame Theory. These speeches, representing prominent examples of political discourse, were found to differ in their use of relational frames. For example, speeches by Donald Trump were found to contain more distinctive frames than other presidents, while Barack Obama utilized more coordinative frames than his counterparts. According to @Belisle2018, these observations imply President Obama, who empolyed more coordinative frames, utilized language emphasizing unity and similarity between people. Donald Trump, on the other hand emphasized differences and divisions between people through his use of distinctive frames [@Belisle2018]. At first glance, the differences appear to fall along party lines, though other factors may be involved as well. In any case, this research appears to indicate that RFT might be useful for analyzing political news content similar to the method employed in the current study. 

Another apparent strength of RFT is its demonstrated effectiveness in the analysis of political discourse. Unlike the development of the Moral Foundations Dictionary, this application of RFT was performed on public facing political discourse that might be more similar to news articles than the Christian sermons upon which the MFD was compiled. Additionally, there would likely be greater overlap between the speeches analyzed in @Belisle2018's work and political news, as the media often quotes speeches by the president. Furthermore, the types or "families" of relational frames could be used as the individual constructs much like the moral foundations in the present research. In order to conform with the method established for this study, their occurrence would need to be scored, probably as a percent. This would allow for an intuitive approach to weighting by *z*-scored valence if needed. 

While Relational Frame Theory appears to represent a viable option for further research into political news analysis, there are some drawbacks. First, RFT by itself does not purport to elucidate political differences in discourse, and its applications to political research are not yet well established. In other words, RFT represents one method by which to analyze political text by analyzing occurrence of particular frames. However, there exists the caveat that the frame families are not known to represent patterns of political ideology at this time. One of the main strengths of Moral Foundations Theory in the eyes of the researchers was that it is a theory under which endorsement patterns (direction and relationship) could be predicted based upon political affiliation. While the results obtained herein may weaken assumptions regarding the MFD's capabilities with political text, Moral Foundations Theory has been repeatedly established as a strong framework for detecting political differences along moral lines. 

In order to seriously consider RFT as a viable framework for this type of analysis, the researchers would like to be able to draw on further confirmatory evidence of the theory's utility in political discourse analysis. Specifically, analyses of more individuals than the four presidents addressed by @Belisle2018 would be needed to further demonstrate RFT's applicability in this field. Also, though presidential speeches may be more similar to news articles than sermons, they are still a distinct form of discourse with different goals and audiences than news outlets. RFT's reliability across these forms appears to remain untested as of this writing. These potential shortcomings are testable and might be overcome through more research. As such, RFT's potential as a useful framework for political discourse analysis should not be discounted.

MAYBE DO DATA, THEN THEORIZE

The researchers aimed to utilize the Moral Foundations Dictionary and valence in the analysis of political news articles to determine whether or not bias was present. In Experiment 1, they tested general political news from four U.S. news sources - two liberal and two conservative. They were unable to find any significant evidence of political bias in that analysis. After adjusting the method to include more sources and specific topics for articles, including the Kavanaugh hearing and the government shutdown, the researchers were once again unable to uncover differences in MFD word use between conservative and liberal news sources. The results in the current study seem to demonstrate the MFD's weaknesses in applications to political discourse. MFD words simply might not appear often enough in news articles to register as endorsement for any particular moral foundation over the others. In future research, this weakness could be addressed through further alterations to the method, including alternative analyses using computer programs including LIWC, through a different theoretical perspective such as Relational Frame Theory, or through alternative strategies involving collecting data and drawing conclusions from which future theories may be derived.

Never before has news media and discourse represented such fertile ground for psychological research. The plethora of options for news sources has created not only an abundance of choice but also vast quantities of text data. Along with this recent increase in the amount of text information, there is now an obligation on the part of researchers to devise proper methods for analyzing that text.  Solid methodologies must be constructed and periodically improved to keep pace with evolving technologies. Likewise, a strong theoretical foundation is paramount to making sense of the current and future media ecosystem. Therefore, it is incumbent upon social scientists to continue investigating the information consumed by millions of Americans every day so that insights into the nature and consequences of political discourse can be more completely understood.


\newpage

# References

\begingroup
\setlength{\parindent}{-0.5in}
\setlength{\leftskip}{0.5in}

<div id = "refs"></div>
\endgroup

\newpage

# Appendix

```{r appendix_mfd, eval = T, include = T}
orig_mfd = read.csv("exp1_data/original_mfd.csv")
####build the table here####
tableprint = matrix(NA, nrow=length(orig_mfd), ncol=5)
colnames(tableprint) = c("Harm/Care", "Fairness/Reciprocity", "Ingroup/Loyalty", "Authority/Respect", "Purity/Sanctity")

#tableprint[ , 1] = unique(final_gs$Source)
tableprint[ , 1] = orig_mfd[,1]
tableprint[ , 2] = orig_mfd[,2]
tableprint[ , 3] = orig_mfd[,3]
tableprint[ , 4] = orig_mfd[,4]
tableprint[ , 5] = orig_mfd[,5]

tableprint = tableprint[c(1,2,3,4,5)]
tableprint = orig_mfd
#change this to match
apa_table(as.data.frame(tableprint),
          note = "Original Moral Foundations Dictionary from Graham et al. (2009)",
          caption = "Moral Foundations Dictionary", 
          col.names = c("Harm/Care", "Fairness/Reciprocity", "Ingroup/Loyalty", "Authority/Respect", "Purity/Sanctity"),
          format = "latex",
          escape = F,
          small = T, 
          align = c("l", rep("c", 6))
          )
```

